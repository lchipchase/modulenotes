---
layout: default
title: Operating Systems and Computer Networks
math: true
---

# CS241 - Operating Systems and Computer Networks

1. [CS241 - Operating Systems and Computer Networks](./CS241)
2. [CS255 - Artificial Intelligence](./CS255)
3. [CS258 - Databases](./CS258)
4. [CS260 - Algorithms](./CS260)

## What is an Operating System

No universally accepted definition

- A software acting as an intermediary between the user of a device and the hardware of the device.



Computer system hardware resources:

- CPU
- Memory
- I/O devices
- Storage

The OS is responsible for allocating these resources to user processes and control their execution. A process is a program in execution

- The goal is to avoid failures and errors - two programs attempting to write to the same memory simultaneously

### OS structure

![image-20211012085716795](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211012085716795.png)

### Operating System Services

These can be broken down into two categories:

- Those which are useful to the user (Program execution, IO, File System Management)
- Those which are beneficial to the system (Security, memory allocation)

#### Program Execution

- Load a program into memory
- Execute that program
- Stop that program

#### IO Operations

- While running a program, the computer may be required to perform IO
- For efficiency and protection, users cannot control devices directly
- This OS controls IO devices through device drivers and interrupts
  - More efficient because the users tdo not need to write code to perform IO - call high level routines
  - More secure because multiple programs cannot access the same IO simultaneously

#### File System Management

Programs need to:

- Read/write files and directors
- Perform additional operations - copy/move
- These permissions may be subject to permission management

The OS system provides system calls to achieve these.

#### Communications

Sometimes, one process may need to communicate with another

- communication may  occur between processes that are on the same computer, or a different computer across a network
- This may be implemented by shared memory or message passing

![image-20211012090929711](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211012090929711.png)

#### Error Handling

The OS must constantly be detecting and correcting errors. This may result in:

- The CPU and memory hardware
- The IO devices
- Illegal memory access

The OS handles the errors wither by invoking an error handling routine or shutting down the process.

#### Resource allocation

- When there are multiple users or multiple jobs running simultaneously, resource must be allocated to each
- In scheduling CPU jobs the OS must consider the speed of the processor, the number of available processors, the jobs to be executed etc.

#### Accounting

- Necessary to keep track of which processes are running and how much computing resource they resume 
- This may be used for system admin, or billing users (cloud computing)
- The OS gathers this info through process control blocks

#### Protection and Security

- When separate processes execute concurrently they should not be able ti interfere with the other processes or with the OS itself
- Security of the system from outside threats is also important.
  - Encryption and Authentication

### The Kernel

- The core of an OS
- It is loaded into the main memory at system startup
- Is is the process running at all times
- Certain functions only the kernel can perform
  - Memory management, process scheduling, file handling

#### Kernel Space Vs User Space

**Kernel space** the part of the memory where the kernel executes

**User space** is the section of memory where the user processes run

- Kernel space is kept protected from user space
- Can be accessed via user processes through system calls
  - Perform services like IO operation or process creation

#### System Calls

- When a user process requires a service from the kernel it invokes a system call
- Required since user processes cannot perform certain privileged operations
- Low level functions provided by the OS
- Provide a consistent interface for common operations

#### Dual Mode Operation

- A mechanism to distinguish between OS and user oprtations
- Hardware operates in two modes
  - User mode
  - Kernel mode
- A mode bit indicates user or kernel mode
- Some instructions designates as privileged only executable in kernel mode
  - System calls by a user asking the OS to perform some function changes the hardware from user to kernel mode
  - Return from the system call resets the mode to user mode

 ##### Transition from User to Kernel Mode

![image-20211012093440301](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211012093440301.png)

### Operating System Structures

- Large and complex software projects
- Common to partition the task into small components rather than one huge monolithic system

#### Simple Structures

- Many early were not well defined
- In the era, the hardware did not support dual mode operation

- MS-DOS also limited by hardware constraints; and consisted of two parts:
  - The kernel - this is then divided into a number of interfaces and device drivers
  - System programs.
-  Large amounts of functionality was contained in a single level
- Difficult to maintain, but beneficial due ti the fact where was very little overhead in the system call interface.

#### A Layered Approach

With proper hardware support, a modular design is possible

- this can be achieved through separationof layers
- The bottom layer is the hardware and the top layer is the users

- Layer K, users the service of layer K-1 and provides to K+1

##### Pros and Cons

Pros:

- Simplicity of construction - easy to modify without changing the implementation
- Ease of debugging
- Clear interfaces between layers

Cons:

- Defining layers is difficult - one must be careful that a layer below never requires a process from a layer above
- Efficiency - system calls access multiiple layers to execute which adds overhead

#### Microkernels

- The Mach OS modularised the kernel using a microkernel approach
- Remove all non-essential components from the kernel and implement them as either system or user-level programs
- Results in a smaller kernel
  - Only provide minimal process and memory management and inter-process communication

![image-20211012094447819](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211012094447819.png)



##### Pros and Cons

Pros:

- Extending the OS is easy - only implement the essential services, all new services are added to the user space, the changes tend to be fewer as it has a smaller size
- More security and reliability - more services are running in the user space

Cons:

- Performance suffers due to increased system call overhead
  - Windows NT was much slower than Windows 96

#### Loadable Kernel Modules

- Most modern approach to system design involves this
- Common in modern implementations of UNIX, OS X and windows
- Kernel provides core services, while other services are implemented dynamically as the kernel is running
- Similar to layered, as each section is well defined, however any module can call any other module
- Similar to micro-kernel, as kernel only implements the basic functionality

![image-20211012094833429](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211012094833429.png)

## Processes

### What is a process

-  Program in execution
- A passive entitit stored on disk as an executeable file
- A process is active
  - A program becomes a process when it is loaded into memory. Execution can be started by many methods 
  - One program can lead to many processes

### A Process in Memory

In a 32 bit system or program, the largest memory address that can be indexed is 32 binary 1s

- **Text** - stores the instruction
- **Data** - stores the global variables
- **Heap** - dynamically allocated memory
- **Stack** - stores variables and function parameters such as the return address if a function
- The space between stack and heap allow them to grow or shrink during the program run time.

![image-20211020111141086](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020111141086.png) - The empty space represents the space for the heap and stack to grow - stack down and heap up

#### Process States and State Changes

- New - the process is being created

- Running - the instructions are being executed

- Waiting - The process is waiting for some event to occur

- Ready - The process is waiting to be assigned toa processir

- Terminated - the Process has finished execution

  ![image-20211020111353631](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020111353631.png)

The process may change states multiple time, for example when an interrupt is called, the process will go from running to ready, whilst waiting for the program to be ready.

- This is called context switching

#### Process Control Block - PCB

A data structure to keep track of a process over its entire lifetime, there is a PCB for each process in operation.

- Process state - running, waiting, ready
  - OS maintains a wueue for each process state for each process which is  in that particular state
- Program counter - location o the next execution
  - Needed when the process transitions from the running to ready, to return to the point of where it was previously
- CPU registers - contents of the CPU registers
  - Store important results of CPU computations - these need to be reloaded after interrupt
- CPU scheduling information -  priorities scheduling, queue pointers
  - Handles interrupts?
- Memory management information - memory allocated to the process
  - Helps the process be allocated in memory - page table or segment table of a process
- Accountings information - CPU used, time since start.
  - Useful to manage a system effectvively
- I/O status - list of open files I/O devices etc.
  - Helps to avoid synchronisation errors of processes trying to write to the same file simultaneously.

![image-20211020111901484](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020111901484.png)

##### Keeping Track Of the Process

- In order to keek track of the state of each process, the OS stores info about it
- Info stored in the PCB
- Since the process control block contains critical information on the process, it must be stored in an area of memory protected from normal user access

#### Concurrency and Context Switches

- Done by context switching, one processor cannot execute more than one at a time. therefore context switching is needed
  - First the OS saves the context of the current process inc. program counter and cpu registers into the PCB of the process, when starting the process up again, the context is loaded back into memory

![image-20211020112402541](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020112402541.png)

- Context switch between two processes - P0 is saved, and P1 is loaded in. 
  - Time spend in the context switch is pure overhead as no actual computation is performed here.
  - May be done with proper hardware support

- when the CPU switches to another process
  - the system saves the stae of the current process in the process control block
  - The system loads the save state of the new process
- Called a context switch
- The more complex the OS and the PCB, the more overhead cost there is



#### Process Scheduling

- To maximise CPU use, quickly switch processes onto the CPU for time sharing
- Process Scheduler selects among available processes for next execution on the CPU
- Maintain scheduling queues of processes
  - Job queue - set of all processes iin the new state - long term scheduler - not invoked as often
  - Ready queue - set of all processes in the ready state - short term scheduler - invoked often
  - Device queues - set of all processes waiting for an IO device

##### Ready and device Queues

![image-20211020113055479](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020113055479.png)

Queues formed of the PCB’s of different devices

- to maintain the ready queue, the header holds a pointer of the first and last PCB in the queue, each PCB contains a pointer to the next in the queue

##### Queueing Diagram

During the lifetime, the provess moves from one queue to another. The processes in the ready queue wait to be operated by the CPU.

Whist it is being served, multiple things can happen

- Process can stop execution to perfrom some IO operation
  - Moves the the corresponding IO device queue
  - On completion returns to the ready queu
- Process execution may be interrupted as the time slice has expired
  - Moved back to the ready queue
- Can create a child process
- Parent process after waiting for the child process to execute returns to the ready queue
- Continues moving between during its lifetime
- On execution completion, it is removed from all queues and PCBs are cleared.

![image-20211020113243907](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020113243907.png)

##### Short term Scheduler

- Selects the next process to be execute from memory
  - Invoked v frequently - once every 100 ms

- Must be fast - if it takes 10ms to decide a process a burst for 100 ms - 9% of time is wasted

##### Long term Scheduler

Selects processes in the new state to be brought into the main memory in the ready queue

- Invoked much less frequent - may be minutes between creating once process and the next
- Controls the degree of multiprogamming
  - If this is stable, tha arrival rate of jobs is equal to the completion rate
- Processes can be described as:
  - IO Bound - spends more time doing IO that computation - short CPU bursts
  - CPU bound - spends more time doing computation  - long CPU bursts
  - Long term scheduler strives for good process mix
- Time sharing systems like Linux and Windows dont have a long-term scheduler
  - All processes are dumped on the short term



### Process Creation

A process (child process) may be created by another process (parent process)

- Children processes can in turn create others, forming a process tree
- Processes are usually identified and managed via a process identifier - PID

![image-20211020114742423](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020114742423.png)

#### Process Creation Options

On process creation, the process has a set of options that are selected. 

**Resource sharing options**

- Parent and children share all resource
- Children share subset of parent resources
- Parent and child share no resources

**Execution Options**

- Parent and children execute concurrently
- Parent waits until children terminate

**Address Space Options**

- The child’s address space is a duplicate of that of its parent - has the same code, data and stack as the parent
- The child loads a new program into its address space.

#### Process Creation In Unix

- Th fork() system call creates a new process with a duplicate address space of the parent
- The exec() system call is used after fork() to replace the address space of the child process with a new program

### Process Termination

- Terminates automatically when the last statement has been executed

- A process can be also terminated using the exit() call
- Return stats value (exit code) to parent
- All resources of the process are released by the OS

![image-20211020121912579](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020121912579.png)

- After a child process has terminated, and before its exeit status is colllected by the parent process, the child is said to be a zombie process
- During this, all resources of the child are released but its entry still remains in the process table
- Once the parent receives the exit status, the entry is released from the process table.

#### What happens if the parent exits without Invoking wait

- The children processes are now orphan processes
- In UNIX systems, the init process is assigned to the parent
- The init process preiodically issues the wait() to collect exit status of all orphan processes
  - Allows the exit status to be collcted and releases the orphans pid and process tabe entry

#### Abort System Call

Can terminate the execution of child processes using the abort() system call

- child has exceeded its allocated resources
- Task assigned to child is no longer required
- The parent is exiting and the operating system does not allow a child to continue if its parent terminates - Cascading Termination

### Inter Process Communication - IPC

- Processes running concurrently may want to communicate with each other

  - Two processes may want to communicate to complete a task

  - The data produced by one could be needed by another

Two main mechanisms

#### Shared Memory

- shared memory - a region of memory that is shared by communicating processes is established
  - Two communicate read and write to a shard part of the memory - usually lies within the address space of one of the processes and the other will need access to this space
  - Special permission from the OS is required for one process toa ccess the memory space of the other
    - Need kernel intervention
  - System calls are used only to establish the shared part of the memory 
    - No further intervention from the kernel
    - Less overhead cost
    - harder job for the programmer to make sure that the processes maintain synchronisation - ensure both are not trying to write from simultaneously.
  - Once the kernel has intervened, the processes must ensure they do not overwrite

##### Producer-Consumer Paradigm

- **Producer** -a process which produces information

- **Consumer** - a process which consumes the produced information

Can be both but not simulatneously

- The shard memory/buffer space can be filled by the producer and emptied by the consumer
- Consumer must wait when the buffer is empty
- Producer must wait when the buffer is full
  - When the buffer space is practically unbounded, there is no issue

##### Shared buffer as circular queue

- Still has a beginning and end, in and out, prod writes to in, and consumer reads from out. As items added, in is incremented until buffer is full
- Is full when one space left - or said to be full
- When emptying, as items are emptied, the out end is incremented until empty - when the in and out point to the same location - this is why we cannot distinguish when full or empty if no space is left.

![image-20211020133252220](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020133252220.png)

To calculate, use the modulo of the size of the buffer due to the fact that the data structure is a circle

- To get around this



#### Message Passing

- Message Passing - messages are exchanged between the communicating processes
  - Two communicate by sending messages between each other
  - Kernel provides a medium 
    - Often a buffer
    - They will write or read to the buffer to get the memory

![image-20211020132324537](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020132324537.png)

- should provide two operations
- - send
  - Receive
- Generally implemented using system calls
  - Kernel’s responsibility to control these, hence the larger overhead

##### Implementation of send() and receive()

Depends on:

- Link implementation - direct or indirect
- Synchronisation between send() and receive()
- Buffer size representing the communication link

![image-20211020133750674](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020133750674.png)

##### Direct Communication

Processes must name each other explicitly

- send(P, message)
- receive(Q, message) or receive(id, message) - not required to specify a sender

Properties of communication link

- established automatically
- A link is associated with exactly one pair of processes
- Hardcoding process identifier may not be ideal since every time a process is run its id can change 

##### Indirect communication

Messages are directed from mailboxes - ports

- each mailbox has a unique id: 
  - send(mailboxID, message)
  - receive(mailboxID, message)
- Can only communicate if they share a mailbox

Properties of link

- Link established only if processes share a common mailbox
- A link may be associated with many processes
- Each pair of processes may share several communication links

#### Synchronisation

Blocking is considered synchronous

- blocking send – the sender is blocked until the message is received
- blocking receive – the receive is blocked until a message is available

![image-20211020134741120](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211020134741120.png)

Non-blocking is considered asynchronous

- Non-clocking send  – the sender send the message and continue
- non-clocking receive – the receiver receives a valid message or a null message 



Different combinations of send and receive are possible

- you might use a nonblocking send() in combination with a blocking receive()
  - This allows the sending process to keep working without waiting for the receive to process the message
- The producer/consumer problems become trivial if using the blocking send() and receive() calls



#### Buffering

Communication link is a buffer. Implementation of send() and receive() depends on the capacity of the buffer

- 0 capacity - the queue’s a maximum length of zero, so sender must block until recipient receives the message
- Bounded capacity - the queue has finite length, when full the sender must block
- Unbounded capacity - the queue’s length is potentially infinite - sender never needs to block



#### Examples of IPC systems

- shared memory using mmap
- Pipes
- Named pipes



## Application Layer: client-server model, transport layer services and protocols, socket programming, HTTP

- Network components and functions
- Network delay and loss
- Packet Switching vs Circuit Switching
- Layered Structure off the internet

### What is a computer network

A network of inter-connected computing devices which enable processes running on different devices to communicate

- Processes communicate by sending messages
- Intermediate nodes help forward messages



#### Components of a network

- Network edge consists of end hosts, they run applications - Web, Email, Video streaming etc
  - Applications involve two communicating processes running on two different devices
- Packet Switches - help forward data packets - switch, router
- Communication links - carry data packets between network devices - wireless or wired

- End hosts connect through the internet through access points - routers, mobile towers
- APs are providered by ISPs
  - They are organised in hierarchical structures
- Internet is the network of ISPs

#### Functions of an End Host

1. Run application processes which generate messages
2. Breaks down application messages into smaller chunks called packets
3. Adds addition information - packet headers so that the packets can be carried by the internet to their destinations
   - Ip address: uniquely identifies an end host in the network
   - Port no: uniquely identifies a process running within an end host
4. Send bits over a physical medium
5. If needed, provides reliable and orderly delivery of packets
6. Controls the rate of transmission of packets - If needed. 

#### Functions of Access Points

Serve as an entry to the internet

- Different types using different technologies
  - Residential access nets use Ethernet and WIFI to connect to a home router
  - Institutional access networks use Ethernet and WIFI to connect to institutional router
  - Mobile access networks use 3G, 4G, or 5G to connect to the nearest base station
- Speeds of these networks depend on the technology being used

#### Functions of Network Core

The network core consists of packet switchers

- Find the route that an internet packet will take from source to destination
  - Runs a routing algorithm to construct routing tables
- Once a packet arrives at a router, it first checks the destination and finds the corresponding entry and forwards the packet to the corresponding outgoing link or router

##### Store and Forward Principle

- An entire packet must be received by a router before it is transmitted to the next link 
- Takes L/R seconds to transmit L-bit packets into link at R bits

![image-20211026082223655](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026082223655.png)

- End to end delay = 2L/R seconds
- If L is large, the delay increases
  - Why the long messages are broken into smaller packets

##### Queueing delay, loss

If arrival rate in bits to link exceeds transmission rate of a link for a period of time:

- Packets will queue, wait to be transmitted on link
- Packets can be dropped if the memory buffer fills up

![image-20211026082437594](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026082437594.png)

#### Four sources of Packet Delay

$$d_{nodal}=d_{proc} + d_{queue} + d_{trans} + d_{prop}$$ 

- $$d_{proc}$$ - nodal processing delay
  - Check bit errors
  - Determine output link - reading the packet header
  - typically < msec

- $$d_{trans}$$ - transmission delay
  - $$L$$ - packet length - bits
  - $$R$$ - link bandwidth - bps
  - $$d_{trans}=L/R$$
- $$d_{queue}$$ - Queueing delay
  - Time wasted at output link for transmission
  - Depends on congestion level of the router
- $$d_{prop}$$ - pro0pogation delay
  - $$d$$ - length of physical link
  - $$s$$ - propogation speed in medium - $$\approx 2\cross 10^8ms^{-1}$$
  - $$d_{prop} = d/s$$

#### Throughput 

Rate at which bits are transferred from a source to a destination in a given time window

- Instantaneous - rate at given point of time
- Average - rate over long period of time

Throughput is specific toa  flow or communicating pair

Instantaneous throughput at $$t=\frac{\Delta s}{\Delta t}$$

![image-20211026083316030](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026083316030.png)

When another source begins to transmit to the same destination, the throughput of each flow will be reduced.

##### Scenario

- $$R_s<R_c$$ 
- $$R_s>R_c$$ - average throughput of the smallest in each scenario

The bottleneck link:

- the link on a end-end path which has the slowest throughput of all the other nodes in that link

### Protocols

- Communicating nodes must agree on certain rules
- Defines rules for communication
  - What to do is a message is received
  - Sequence and format of messages

“Protocols define format, order of messages sent and received among network entities, and actions takes on message transmission and receipt”

- These can be implemented with either software or hardware

#### Examples

- Routers run protocols to forwards messages
- NICs implement hardware protocols to send bits through a physical medium

#### **Standards**

- Most protocols defined by the IETF
- Some other bodies specify standards
  - IEEE does Ethernet and WIFI

### Data Transmission Techniques

#### Packet Switching

Internet uses packet switching

- Different flows share resources along their routes
- If one flow is not using any of the shared links, then the other flow can use it
- Flows can change root if needed

##### Pros and cons

- Resources are not pre allocated to a communicating pair of devices
  - Cons: packets may have to wait, may get lost
  - Pros: Better utilization of resources

#### Circuit Switching

Used in telephone networks

- a circuit is reserved for each flow for the entire call duration
- Flows do not share resources
- If one flow is not using its assigned circuit during the call, it cannot be used by another flow
- Not ideal for internet traffic which is busy by nature

##### Pros and Cons

- Resources are reserved for a communicating pair for the entire duration of communication - called a circuit
  - Pros - Guaranteed rate, no losses
  - Cons - Poor utilization or resources for bursty traffic

### Layering

- Network devices perform complex functions
- Better to divide these into layers
- Each layer performs a subset of functions
  - Layer $$N$$ uses the services of layer $$(N-1)$$, and provides services to layer $$(N+1)$$
- Makes it easier to add services to a layer or change its implementation without affecting others
  - Must make sure the basic functionality remains the same

#### The five layers

All layers may not need to be run

##### Application

Consists of user applications or processes which generate the data to be used across the internet.

A standard set of protocols for this layer is shown below.

- HTTP 
- SMTP
- DNS

##### Transport

Provides services to the application layer, it’s main job is to divide the data into packets, and add headers to the packets (transport layer headers), adding sequencing and error correcting information. An application layer program must specify which of the protocols it wants to use

- TCP
- UDP

##### Network

Adds a header to the packets generated by the transport layer - source and destination address. Runs a routing algorithm 

- IP
- Routing protocls

##### Link

Adds a source and destination MAC addresses to the segments generated by the network layer - identifies the NIC cards by the hardware. Every time the packet moves a hop in the internet, the source and destination MAC addresses change.

- Ethernet
- WIFI

##### Physical

Decides how individual bits are transmitted across the phyisical transmission medium

- Separate protocol for each transmission message



#### 

#### Encapsulation

The path taken by the message is identified by the blue path. The message after being generated, passes through each of the five layers at the source, nd then is transmitted. Each of the layers adds new headers to the message.

- Process of adding headers is called encapsulation

Headers are successively removed by the receiving node, until the message reaches the topmost layer within that node

- Only the end and start node run all five layers
  - Most complicated tasks are completed by the end nodes.

![image-20211026085632988](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026085632988.png)

## Application Layer

- A network application is a set of processes running on different host machines can communicate by sending messages over a  network
- Communicating processes form a network application
  - Web, email etc

### Developing a network application 

When dveloping a network application, a developer generally has to develop both the client side and the server side of the program

- Developing onse side of the program is enough for applications specified by standard protocols.



### Sockets

- The application processes are user processes, controlled by the user, run in  the user space in the end host
- The other layers below are controlled by the kernel, and the email must pass through all layers of the program.

Processes send/receive messages via sockets (analogous to door)

- Are APIs between application and network.
- Creating, reading from, writing to sockets is done by system calls.

![image-20211028101644992](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028101644992.png)

### Addressing processes

- Messages need to be addressed to the correct process running within the correct end host
- Any internet device can be identified by its IP address
  - TheIPV4 addresses are 32 bit numbers written as dotted decimal (expressing its decimal equivalent)
  - IPV6 uses 128 bit addresses rather than 32

- Processes can be identified by prot numbers
  - 16 bit numbers randing from 0 to 65535
  - Port numbers 0 to 1023 are reserved for well known network applications - 80 for HTTP
  - Numbers above 1024 can be used by other application programs

### Transport layer services

- Application processes use transport layer services
  - Transport layer is expected to deliver messages to their intended recipients
- All transport layer protocs offer some basic services
  - Packetization, addressing, sequenching, error correctinb its
- Additional services are provided by different transport alyer protocls
  - App developers have to choose a transport layer protocol based on the services the app will require

#### Transport layer services in the internet

- TCP - transmission control protocol
- UDP - user datagram protocol

An application has to choose one of the above

##### TCP

- Reliable and in-order data transfer services - packets can be lost or arrive out of order
- Ensures all data packets ensure that all sent are correctly received, and in the order they were sent
- A connection oriented services - setup required between client and server processes before they start transferring data - TCP handshake
  - Client initiates the TCP connection, indicated by a packet with a special bit with a 1 in its header
  - The syn packet carries no data, just requests initiation of a TCP connection
  - The server then allocates resources for that TCP connection
    - A TCP connection socket
  - Server returns a packets with a syn and ack packet, both set to 1. Time between sent and received is called the round trip time - no data is transferred here
  - After this is received, the client sends a bit with the ack bit sent to 1:
    - Data may only be transferred after this.

![image-20211028102709059](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028102709059.png)

##### UDP

Provides no guarantees on data transfer

- Best offor service - packetizes data and send it to network
  - No effort is made to recover losses

- UDP is faster - no connection set up required, UDP headers are smaller than TCP headers
  - Application such as Internet telephony - Skype uses UDP



### Building network Application

- learn how to build client/server applications which communicate using sockets
- Create sockets, read from and write to sockets

#### socket types

- UDP - connectionless sockets
- TCP - connection oriented sockets

### HTTP

The protocol used by the web, one of the most widely used network applications is the Web

- Web borwsers communicate with web server
- Both processes use HTTP as the application layer protocol
  - Specified by the IETF
- HTTP uses TCP and port number 80 - 

#### Overview

- Client program send HTTP request messages to request a web page
- The server responds with a HTTP response message containing the requested web page
- Structure of the messages and their sequence are specified by HTTP

![image-20211028103812728](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028103812728.png)

#### A web page

Consists of objest

- An object is nothing but a file - HTL, JPEG, Java applet
- Each objext addressable by a URL

A host name, and a path name:

- www.leonchipchase.com/images
- Most pages consist of a base HTML file and several object references from that file



#### How HTTP works

HTTP uses TCP

- Server runs the server process on port 80
- Client initiates TCP connection to server port 80
- TCP connection established after a TCP handshake
- HTTP messages are exchanged
- TCP connection is closed

##### Versions

**Non - persistent (Httpv1.0)**

- Each object is obtained over separete TCP connection, downloading multiple requires multiple TCP connections

HTTP:HTTP v1.2 - persistent

- Multiple object can be sent over a single TCP connection

##### non-persisten HTTP

To download a webpage containing 1 base HTML file and 10 referenced object

1. Client send SYN, the server responds with SYN-ACK
2. Client send ACK packet and adds request for the base HTML file
3. The server established the connection and responds with the base HTML file
4. HTTP server closes TCP connection
5. Clients receives the HTML file and examines it to find 10 other referenced object
6. Client repeates setps 1-4 for each of the 10 objects

###### **Response Time**

- 1RTT to initiate TCP connection
- 1 RTT to send HTTP request and receive first fiew bytes of the requested object
- file transmittion time
- Response time = 2 RTTT + file transmission time

###### Problems

- each object requires at least 2RTT to be downloaded
- For each TCP connection the OS of the server has to allocate some resources  - problematic when the server has to handle a large number of requests



##### Persistent HTTP

- Server leaves initial HTTP open
- Subsequent HTTP messages are exchanged over the connection
- client sends request back to back as soon as it encounters
- All objects downloaded within 2RTT + total data type

#### HTTP Response messages

- two types, request and response
- Written in ASCII - human readable

![image-20211028105052084](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028105052084.png)

##### General Format

Contains data in some of the request messages

![image-20211028105114093](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028105114093.png)

#### Response Message



![image-20211028105217059](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028105217059.png)







### Web Cache - proxy server

satisfy client request without involving origin server

- web clients can be configures to access the web via a web cache
- Browser sends all HTTP requests to cache
  - Object in cache - cache returns object
  - else cach request object from origin server and returns to the client



![image-20211028105329990](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211028105329990.png)



- Typically cache is installed by ISP to serve requests of popular contents
  - Reduces response time for cient request
  - Reduces outgoing traffic of an ISP network, reduces the cost for the ISP
  - reduces traffic on the internet as a whole



## Threads

What is a thread

- A unit of CPU execution
- Single threaded process: one chain of execution running each line sequentially
- Fine if we have to perform a single task
- What if there are multiple similar - testing primality of multiple
- One solution is to run for each in a loop - running a ingle threaded process
- Create a saparate process for each task - inefficient
  - Each process requires its own address space in memory, Code, data could be shared.



![image-20211102211001624](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102211001624.png)





- We can make a process multi-threaded - each thread can perform a saparate task
- Threads share more things with their parent - code, data, heap, opened files, signals
- Comprised of:
  - A thread Id
  - Program counter
  - A register set
  - Stack



### Threads vs Processes

Thread creation has less overhead

- A shares code, data, files with the process which creates them 

Process creation is more overhead

- a child process shares little or no resources with the parent

### Where used

- Most modern applications are multithreaded
- Separate tasks can be implemented with separate threads
  - Update display
  - Responding to keystrokes
  - Spell checking
- All can run concurrently

#### Multi-Threaded web server

- Cannot do this sequentially
  - Each client would have to wait for its predecessor

![image-20211102211654586](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102211654586.png)

- common approach to server architectures
- A busy web server may have thousands of concurrent clients
- If it ran as a single thread, it would only be able to process one request at a time



### Benefits

- Economic - cheaper than process creation, thread switching has a lower overhead than context switching
- Scalability - large number of concurrent tasks
- Responsiveness - may allow continued  execution of one thread even when another is blocked
- Resource sharing - threads share resources of process, easier than thared memory or message passing

### Concurrency vs parallelism

- Concurrency supports more than one task making process
  - A single CPU system may appear to be running tasks concurrently by interleaving their execution

![image-20211102212206399](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102212206399.png)

- Parallelism implies that a system can perform more than one task simultaneously
  - Possible to have concurrency without parallelism

![image-20211102212230598](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102212230598.png)



#### Data and task parallelism

- Data - distributes subsets of the same data across multiple cores, performing the same operation on each core

![image-20211102212336074](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102212336074.png)

- Task - Splits threads performing different tasks across multiple cores
  - Performing two different statistical calculations on an array

Often applications have a mix of both.

### Computing speedup - Ahmdahl’s Law

- The 1 represents the time taken before parallelising

$$Speedup\leq \frac{1}{S+\frac{(1-S)}{N}}$$

- The time taken to run the serial part - $$S$$ smaller than 1: 
- The inner frac is the time taken parallelisable part with $$N$$ cores

### Thread Synchronisation

- Thread 1 - sum = sum + i
- Thread 2 - sum = sum + j

![image-20211102213629176](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102213629176.png)

- between two CPU oeprations, the CPU can change thread

| Step | Thread | Action                | Value          |
| ---- | ------ | --------------------- | -------------- |
| 1    | 1      | register1=sum         | register1=0    |
| 2    | 1      | register1=register1+1 | register1=1    |
| 3    | 2      | register2=sum         | register=0     |
| 4    | 2      | register2=sum+5001    | register2=5001 |
| 5    | 2      | sum=register2         | sum=5001       |
| 6    | 1      | sum=register1         | sum=1          |

#### Race Condition

Threads engage in a race to become the last one to write on the shared variable sum

- Only one of the values is preserved
  - A race to see which one is last
- Race conditions should be avoided by proper synchronization between the thread

#### Mutex locks

- many ways to synchronise

- Common way is mutual exclusion locks - mutex locks

- The idea is that each thread must first aquire a lock to perform updates on shared variables

  ![image-20211102214140675](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102214140675.png)

### Types of threads

- User level

  - Implemented by users in the user space
  - Kernel not aware of their existence - sees it as single threaded
  - There is no kernel involvement in managing such threads - less overheads
  - Cannot run in parallel on different CPUs but can run concurrently

- Kernel leve

  - Implemented by the kernel in the kernel space
  - Kernel can schedule them on different CPUs
  - Can runin parallel
  - Managing these has more over head as managed by kernel

  #### Many to one model

  Many user level threads mapped to a single kernel thread

  - Less overhead - good
  - Multiple threads may not run in parallel, because only one may be in the kernel at a time
  - One blocking thread causes all to block

#### One to one model

Each user level thread maps to a kernel level thread - linux, windows

![image-20211102214920253](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102214920253.png)

- Advantages
  - Other threads can run even when one is blocking
  - Different threads can be run on different cores in parallel
- Disadvantages
  - A user process can create a large number of kernel threads slowing the system down

#### Many to many model

Allows user level threads to be multiplexed onto a smaller or equal level of kernel threads

- User threads take turns to use kernel thread

Advantages

- programmers can decide how many kernal threads to use and how many user threads should be mapped to each one - less overhead
- Kernel thread can run in parallel

![image-20211102215127426](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102215127426.png)

### One thread per request strategy



![image-20211102215155045](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102215155045.png)

Server creating a separate thread to handle each client request

- High overhead for each thread
- High traffic will create a large number of threads, slowing the system down by burdening it

#### Thread Pool

In this model, the server main thread creates a fixed number of worker threads before receiving any requests, they remain active as long as the server runs. The workers, pull requests from a queue in order they come to serve them.

##### Main server thread

```c
int main(){ 
	for(i=0; i < NUM_THREADS; i ++){
        pthread_create(tid[i], NULL, handle_connection, NULL);
    }
    /* Create a TCP socket, bind to specific IP and port, and start listening on the por */
    // KEep accepting connections and adding them to queue
    while(1){
        conn_sock=accept(...,...,...);
        pthread_mutex_lock(&queue_mutex);
        enqueue(work_queue, conn_sock);
        pthread_mutex_unlock(&queue_mutex);
    }
}
```

##### Worker Threads

This implementation works, but the worker keeps checking the queue even when there is no work to be done - wastage of CPU cycles, energy.

```c
void *handle_connection(void *arg){
    ...
    while(1){
        if(!empty(work_queue)){
            pthread_mutex_lock(&queue_mutex);
            dequeue(work_queue, conn_sock);
            pthread_mutex_unlock(&queue_mutex);
            /*process conn_sock*/
            ...
                ...
        }
    }
}
```

####  Condition variables

- Special variable like mutex loxks used to syncrhonise the actinos of different threads
- A thread can use wait(&cond_var,…) to wait on condition variable
- Until some other thhread signals the variable suing signal(&cond_var) pr broadcast(&cond_var)

![image-20211102223421188](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102223421188.png)

##### Advantages of thread pool

- usually slightly faster to esrvice a equest with an exisitng thread than create a new thread
  - generally better than oner per request
- Allows the number of threads in the application(s) to be bound to the size of the pool. This size is decided by the programmer taking into account the number of processing cores, memory in use, expected number of concurrent client requests

#### Signal handling

Signal handling in UNIX system to notify a process about the occurence of certain events

Type of signals:

- Synchronous - internally generated by a process - div by 0
- Asynchronous - externally generated - terminating processs ut CTRL + C sends the signal SIGINT



All signals, regardless ot type, follow the same pattern:

- A signal is generated by a particular event
- A signal is delivered to the process to which it applies
- Once delivered, the signal must be handled



##### Methods of signal handling

- default - every signal has a default handler that the kernel runs when handling signal - SIGINT terminates the process
- user defined - user defined signal handler function can override the default

##### Example

```c
void handler(int sigal){
	/*Actions*/
	/*Oly use signal safe functions*/
	/*printf is not signal safe*/
}


int main(){
	signal(SIGINT, handler);
    /* Other things*/
	
}
```

##### Signal handling in multo-threaded programs

 In multithreaded, a signal can be delivered to 

- All thread
- Specific threads

Synchronous signals are generall delivered to thread generating the signal

Some asynchronous signals like SIGINT should be sent to all

- Signal can be delivered to a specific thread ``` pthread_kill(pthread_t tid, int signal);```



## Selected Networking Topics

### Network Interfaces

- a point of interconnection between a computer and a network
  - A node can have multiple interfaces
- Associated with a NIC but does not have to have a physical form
  - A loopback interface (localhost) is not a physical device. It is a software simulating a network interface, commonly used to test network applications
- Each network interface has an IP address
  - Hardware NIC also has a hardware address or MAC address



For the coursework, there is etho0 and lo - only the ethernet has a MAC address

- You need to intercept and sniff packets at the two interfaces

### Internet packet structure

- protocls specify the structure of internet packets
  - Application message must pass through all layers of the IP stack, and each layer adds its own header.
  - Once received, the headers must be removed one by one
  - Need to analyze these headers to identify network attacks - the feild present in the headers to determine if the packet is suspicous

#### Link layer header

- The firest two feilds are the destination MAC and the source MAC addresses - 48 bits each, describe the hardware addresses  of which the packets last reaveresed 
- Contains the protocol - 16 bit number which uniquely identifies the protocol
  - For IP it is 0x0800 for IPV4
- Total ethernet header - 14 bytes

![image-20211105110847921](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211105110847921.png)

#### IP header

- also made up of 32 bits:
  - IHL - IP header length feild - stores the length of the IP header - 4 bits - number of four byte words
    - Does not have a fixed length - some optional feilds
  - Protocol - 8 bits - describes the protocl of the next layer 
  - Source and destination addresses - 32 bits each

![image-20211105111121175](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211105111121175.png)

#### TCP header

- has two main protocols - UPD and TCP, each has its own header structure - below is for TCP
- Port number - 16 bits - identify the communicating processes
- Data offset - 4 bit - descripbes the length (not fixed) of the header in 4 byte words
- Control bits - 6 bits: URG, ACK, PS, RST, SYN, FIN - indicatye special functions of TCP packets

![image-20211105111341099](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211105111341099.png)

#### SYN attack

- a TCP connection is establised with a SYN packet - SYN bit set to 1
  - Creates a half open connection  - involves allocating resources to that client
    - A buffer to store the packets for  erxample
  - A Server sends a SYN ACK packet to the client - both set to 1
  - Client sends ACK packet back to the cilent
    - After this the connection is established
    - Now they can exchange information using this connecion



SYN attack performed by a malicious client

- client sends a flood of SYN packets from spoofed src IPs
- Creates a half open connection for each syn packet
  - Server creates lots of these, each occupying some reources
  - They get exhausted creating these connections
- On creation, a SYN-ACK packet is sent to the client
  - Attacker never replies
- When a normal user tries to connect, the connection is denied because the server is too busy
  - Kind of a DOS attack



Coursework asks the number of total SYN packets, and the number having a uniqe source of IP addresses

#### ARP cache posioning attack

Need to understand:L

- how MAC addresses work
- What is ARP works

##### MAC addresses work

- source A wants to talk to dest. B, the source and destination IP addresses are added to the network header
  - Used by routers to find a path from source to destination
  - Duhring the journey, the SRC and destination addresses stay the same
- MAC meant to identify IP addresses
- In the link layer, the src mac will be of the original  source, and these will update as it travelx through  the different routers and nodes in the networks

How does each link determine the MAC address of each nect next node

##### ARP protocol

Find the IP address corresponding to the IP address shown

- to find the MAC address, the router boradcasts an ARP request packet to all interfaces on-lnk with sending interfact
- A special MAC address to show that the packet should be broadcast to all connected interfaces.

![image-20211105121215081](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211105121215081.png)

- Opcode tells whether it is a request or a reply
- Only the node with the correct IP address replies
- IP address to MAC address saved for the future.

##### AThe attack

- ARP allows unsolicited replies since the IP addresses can change
- The ARP caches need to be updtaed
- The node that has a new IP address can let all nodes know it has a new IP address by sending an ARP reply
  - An attacker can use this to redirect all messges meant for another node to itself.
  - The attacker only needs to send the an unsolicted ARP reply, saying that they are the IP address
  - Redirect all messages meant for the given node to the attacker

CSWK asks to count the ARP reply messages, if high, strong indication that someone is trying to carry out the attack.



## Process scheduler 

### Readu queue and CPU scheduler

- The os mainatains different queues for processes in different states
  - Records in the queues are PCBs
- Processes in the ready state are put into the ready queue
  - Not necessarily IFIO, can be a PQ or linked list based on the schedluing algroithm

- CPU scheuler or the short term scheduler processes from the ready queue to be executed on the CPU

### CPU and IO bursts

- Increase the utilization of a CPU

- Life of a process - CPU burst followed by an IO burst



![image-20211109153417302](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109153417302.png)

- When a process is in an IO burst, the cPU can be scheduled to some other process
  - Decreases the down time, increasing utilisation

### goals of CPU scheduling

- Maximuse CPU utilisation - fraction of time the CPU remains busy

- Maximise throughput - number of processes that complete their execution per unit time
- Minimise turnaround time - amount of time to execute a particular process
  - Also takes into the job processing time

- Minimse waiting time - amount of time a process has been waiting in the ready queue
  - Minimise the time waiting in here - this is more important, a better measure of performance
- Minimum Response time - amount of time it takes from when a request was submitted until the first response is produced
  - Time between given an input and the response received
  - Should have a very small one of these

### When does  a scheduler schedule

Four circumstances that may trigger this:

- A process switches from the runnning to the waiting state
- A process switches from the running to ready state
- A process switches from waiting to ready
- A process terminates

Non-preemtive scheuling

- CPU is given up voluntarily in scenarios 1 and 4

Preemptive scheduling

- The CPU is snatched from one to another - in scenarios 2 and 3

#### Scheduling algorithms

##### First come first serve

- Processes are assigned to the CPU in the order of arrival

![image-20211109154132850](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109154132850.png)

- wating itmes for 1,2,3 are 0,24 and 27

If p1 arrived last, then the average waiting time would be less, as waiting times are 0,3,6 ms = average of 3

###### Comments

- Performance of this varies greatly depending on the arrival sequence
- If shorter arrive first, then the performance is better
  - Shorter jobs must wait a long time to be processed
- Non-preemptive

##### Shortes job first

Processes with the shortest next CPU burst is selected, if there is a tie, then FCFS

##### ![image-20211109154543055](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109154543055.png)

- SJF is provably optimal, it gives the minimum average waiting time
  - Moving short process before a long one decreases the waiting time of the shorteer prcess more than that the increased wiaitng time for the long process

- Problem is knowing how long the next CPU burst will be, and it can only steimate this
  - Pick the processs with the shortest predcitednext CPU burst

###### Exponential Moving Average



$$1. t_n=$$ actual length of $$n$$^th^ cpu burst

$$2. \tau_{n+1} = $$ predicted value of the next CPU burst

$$3.\alpha,0\leq\alpha\leq1$$

$$4.\tau_{n+1}=\alpha t_n + (1-\alpha)\tau_n$$

Expanding the recusion gives:

- All previous CPU burst times affect the prediction, but all affect nonetheless

![image-20211109155011672](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109155011672.png)

- Graph example
  - Black shows actual,
  - Blue shows prediction

![image-20211109155030844](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109155030844.png)

###### Two versions

- Preemptive
- Non-preemptive

New shorter process arrives when a process is already being executed - the new may be shorter than what is remaining

- Pre emptive - switch to newly arrived process

- Non preemptive - finish the current execution

###### Pre emptive

![image-20211109155315009](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109155315009.png)

- Average is 6.5ms
- If it were non-preemptive it would be 7.75 seconds

##### Problems with pre-emptive

- Can cause race conditions
- If a process is pre-empted while updating sared data
  - LEaves in an in-consitednt state
- Another process is scheduled which accesses the same data
  - See the data in an inconsistent state
- Shared data should be updated within critical sections using proper synchronisation primitives

##### Priority scheduling

- SJF is a special case of PS
- A priority is associated with each process and the CPU is allocated the highest priority process
- Priorities can be indicated by numbers 

One problem:

- Starvation
  - Very low priority proesses may never get scheduled
- A solution is aging - gradually increasing the priority of processes that wait in the system for a long time

##### Round robin

- Each process gets a small unit of CPU time (time quantum $$q$$). After this, the process is preempted and added to the end of the ready queue
- Generally, the scheduler visites the processes in the order of arrival
- If there are $$N$$ processes in the ready queue and the time is $$q$$, then each process get $$1/N$$ of CPU time in chunks of at most $$q$$ units at once. No proccess waits more than $$(N-1)*q$$ units of time

- RR is preemptive - timer interrupts every quantumn to schedule next process
- Performance
  - $$q$$ large $$\implies$$ FCFS
  - $$q$$ small $$\implies$$ too many context switches
  - typically 10 to 100 ms 

![image-20211109160011765](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109160011765.png)



## Synchronisation

### Why synchrnise

- concurrently running threads/processes enter into a race condition when trying to update shared variables
- Only one of the values is preserved
  - A race to see which is last
- Avoid with synchronisation

FOR THIS LECTURE

- Processes to mean both threads and processes
  - Both need synchronisation
- part of the code where a process updates shared variables is going to be called a critical section
  - A process can have multiple

- when a process is in a critical section, no other process should be allowed to be in its critical section - mutex

#### Critical section problem

- consider a set of processes $$\{P_0,…,P_N\}$$
- Each process has a critical section of code
- Critical section problem: design a protocol such that: no two processes can mutually execute their critical sections - MUTEX

![image-20211109160802401](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109160802401.png)

#### Criteria for ideal

- **MUTEX** - if a process $$P_i$$ is executing its critical section, then no other processes can be executing their critical sections
- **Progress** - if no process is executing in its critical section and there exist some processes that wish to enter their critical section, one of the waiting processes must be able to enter into its critical section
- **Bounded Waiting** - No one process should have to wait indefinitely to enter its critical section while other processes are allowed to enter and exit their critical sections continually

#### Possible solution for two processes

- Two processes $$P_0, P_1$$
- Shared varible turn can be 0 or 1

![image-20211109161314795](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109161314795.png)

- the process sets the turn to the value of the other.

- They alternate after each other to execute the critical sections

Mutual exclusion is guaranteed as they can only take one value at a time.

- The CPU switches from P1 to P0, since turn is set to 0, P0 will be able to enter into critical section, execute, critical section, set turn to one, then wait for remainder section 

  - No additional CPU interference is needed

- After this can wait for turn to be 0 again

  - P1 not in critical section, however P0 cannot enter into as turn is still 0 

  - The progress criterion is not satisfied
  - Can only be set to 0 after P1 finishes remainder and change turn to 0

- There is a chance that P1 breaks the while loop and does not change the turn to 0

- P0

**Bounded, progress** are not satisfied - Bounded follows progress.

### Peterson’s Algorithm

- Shared variable turn
- Boolean array flag[2]={false,false} 

![image-20211121173139917](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121173139917.png)

- P0 sets its wish to enter the critical section
- Sets turn to 1 - allows for 1 to enter critical section - keeps checking if flag[1] is true -
- Waits until done for the critical section
  - Now other process can do critiical section

#### MUTEX

- Suppose P0 is in critical section, to show mutex, show P1 cannot be in critical section
- If P0 in critical, just before, it must have found that at least 1 of the two conditions is not holding - 
  - Flag[1] is false, or turn is 0
  - Flag[1] is only true after critical section has been computed
- IF turn is 0
  - Flag 0 must be true, and turn is 0, the P1 cannot enter critical section
  - P1 cannot enter critical section when 0 is

#### Progress

- Suppose none of the process in critical
- Both flags are false
- Suppose P0 wants to enter critical, P1 does not
- P0 first sets flag 0 to true, then checks while statement conditions are true
  - Flag 1 is still false, so will not wait in while loop
  - Will enter critical sections
- If both want to enter critical both will be false
- Both set the turn to the other
- Enter race condition, and the last to write, the other will enter the section.

#### Bounded

Prove none of the process stuck in entry

- Suppose P0 gets stuck in its while loop forever
- This means flag 1 must be true permanently
  - Cannot happen since in P1, flag 1 is set to false after critical section 

#### Problems

- Employs busy wait in entry section
- May fail in modern architectures

In modern architectures, independent read and write operations may be reordered to make programs more efficient

- They are independent assignments, therefore may be reordered for efficiency
  - After setting values to true, critical sections may be enterd - MUTEX does not hold

![image-20211121174006842](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121174006842.png)



### Locks

- more practical techniques
- All based on locking and unclocking
  - Two processes cannot have a lock simultaneously
  - Must release a lock after doing critical section

#### Synchronisation hardware

- locking and unlocking should be performed atomically
  - - non interruptible
- Modern provide special atomic hardware instructions to implement locks
- test_and_set



#### Test_and_set instruction

- Definitions
  - Returns the original value of passed parameter target
  - Set the new value of passed parameter target to TRUE
- Must be implemented atomically by hardware

![image-20211121174351909](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121174351909.png)

Using test and set, lock and unclock happens like this:

- One single cycle
- Variable lock is shared and set to false
- Process wishing to perform critical section perfroms a test and set on the lock variable, if true, the process keeps spinning on a while loop until false
  - The first process to find the variable false sets the variable to true inside test and set and execute critical section
  - Since lock is set to true, no other can enter critical section therefore MUTEX is met
  - After execution the process sets the lock variable to false, so another process can enter critical section
  - If no process is in critical section, then lock will be false
  - Progress satisfied by this
  - Bounded is not - the same process can require the lock after the process 

![image-20211121174432749](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121174432749.png)

#### Better Solution

- MUTEX and progress is preserved
- Bounded also, as each process in the waiting is served next

![image-20211121174835529](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121174835529.png)

### Synchronisation Primitives

- Hardware based solution generally inaccessible to programmers
- OS designers build software tools to solver critical section problem

**Primitives**

- MUTEX locks
- Condition variables
- Semaphores

#### MUTEX Locks

- used to guarantee exclusive access to critical sections
- Calls lock function on a mutex lock
- Makes the function wait until the lock is available
- Lock cannot be obtained by any other process
- After executing, process releases the lock

![image-20211121175210901](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121175210901.png)

- Bool indicates if available
- Pointer to a list of variables

##### **Lock function**

![image-20211121175235857](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121175235857.png)

- If false, not available

  - Then is added to the list waiting for that process

  -  And calls system block call, calling process is blocked until available
    - Avoids busy waiting
  - If the Mutex is available, calling process sets mutex lock to false to make use of other processes

All of these operations must be done atomically

##### **Unlock Function**

![image-20211121175513860](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121175513860.png)

- A process is chosen froma list waiting for the mutex, and proces removed from the list
- The process is then woken up from waiting state using wakeup system cal
- MUTEX lock is set to true
- PRocess which is just woken up can grab mutex lock and go into critical section

Must all be performed atomically 

#### Semaphores

- Can have integer values
  - Makes them more powerful then MUTEX lock
- A zero denotes that semaphore is notavailable
- Positive value indicates it is available
- Can only be accessed via two indivisible operations wait() and signal()
  -   Originally called P() and V()

##### Operations

- wait() checks is semaphore is $\leq$ 0 and if so makes the calling process wait until it becomes positive
- Once the semaphore value is positive wait() function decrements it by 1
- Signal() function increments the value of the semaphore by 1
- Both wait() and signal() must be performed atomically

##### Use

- set initial value to 1,
- A process wanting to enter calls a wait on the semaphore()

- If available decrements from 1 to 0, then process enters its critical section
- Then calls the signal function() to increment back to 1
- acts like a MUTEX lock

From setting the value to a number alrger than 1, we can control the number of process that can concurrently access a given resource

- If we want two, then we can initialise at two
- Each process issues a wait
- If positive, decrements by 1, and accesses the buffer (example)
- If semaphore is not positive, then waits until positive
- After accessing, signal() is used to increment by 1

##### ![image-20211121180333804](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121180333804.png)

##### Semaphore functions

- Only difference is instead of boolean, there is increment and decrement

![image-20211121180405359](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121180405359.png)

### Synchronisation Issues

- Deadlock
- Starvation
- Priority Inversion

#### Dadlock

- two or more processes waiting iondefinitely for an event that can only be caused by the waiting process
  - A waits for B and B waits for B, They are in deadlock

![image-20211121180712762](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121180712762.png)

- Each waits for both to be available 
- Both conditions need be satisfied to enter critical section
- They call wait in a different order
- Both semaphores at 1, process 0 executes wait on S, and make S unavailable, CPU switches from 0 to 1, and causes wait on Q
- This makes Q unavailable
- Executes wait on Q for process 0
- Semaphore Q taken by 1,
  - When 0 executes wait on 0 it will be blocked
- Switches to 1 again, and performs wait on S
  - 1 will be blocked by 1
  - Both are blocked and waiting for the other
- Deadlock
  - Only happens in specific sequence
  - Difficult to detect in general

If both call wait calls in the same order, then ther cannot be deadlock

#### Starvation

- when a specific process has to wait indefinitely while other make progress
  - Opposite of bounded waiting
- Can occur when mutliple processes are waiting on asemaphore and the signal call wakes up  the same process over and over again

- To avoid, the signal should randomly pick the process to wake up

#### Priority Inversion

- Scheduling problem when lower priority process holds a lock needed by a higher priority process
- If a medium priority process is active and does not need the lock whilst higher is waiting, the higher gets blocked
  - solved via priority inheritance protocol

![image-20211121181326682](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121181326682.png)

- M gets CPU instead of H

##### PIP

the priorities of all process contending for the same resource is changed among the highest priority among the contenders

- The priority will be changed of that of H whilst it is waiting for the lock with M
- M cannot be scheduled, as L has a higher priority than H

### Classic Problems

- Bounded buffer probelm
- Reader and writers problem
- Dining Philosihper problem

#### Bounded Buffer Problem

- $n$ buffers, each can hold one item
- Producer process adds an item and write to a buffer while the consumer consumers and item from the buffer
- Producer should be blocked when the buffer is full
- Consumer should be blocked when empty

Three semaphores are needed

![image-20211121181703740](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121181703740.png)

**Producer**

- Wait function decreases the value of empty 
- Then producer waits for the mutex lock to be added
- Then releases the lock
- Increments the full by 1, 

![image-20211121181714534](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121181714534.png)

**Consumer**

- decrements on full by 1 after consumption
- Then waits for the mutex
- Once available consumes an item from buffer and releases
- Then atomically increments the empty semaphore by 1

![image-20211121181819364](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121181819364.png)

#### Readers and writer problem

- A dataset is shared among a number of concurrent processes 
  - Readers - only read the data set
  - Writes can both read and write
- Allow multiple readers at the same time
- Only one single write can access the shared data at the same time

Different version, each with different priorities

- Readers given priority over writers
- If both are waiting, readers read first, then the writes can write one at a time
- Writes may starve

##### Semaphores

Two semaphores

- rw_mutex = 1 - lock to allow at most one write
- mutex = 1 - lock to protext the read count

int read_count - counts the number of reader processes

##### Write Process

- waits for rw mutex to be available
- Write to data set
- After write completion, rw is released

![image-20211121182223776](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121182223776.png)

##### Reader

- before reading, increments rad_count by 1, updated within a mutex lock and unlock
- After incrementing and read_count is 1, then there were no previous active reader, therefore first reader grabs rw_mutex, and makes sure no writer can access the data set
- After reading, read_count decrements within a mutex lock, if 0, then the reader is the last active, therefore r_mutex is released so writers can grab

![image-20211121182542256](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121182542256.png)

#### Philosophers Problem

- Ps spend their lives alternating eating and thinking
- Don't interact with neighbours, occasionally try to pick up two chopsticks (1 at a time) to eat from bowl
  - Need both to eat, then release both when done

- In 5 philosphiers, the share data
  - Bowl of ricew
  - Semaphroe chopstick[5] all initialsed to 1
  - Two neighbouring philosophers cannot eat at the same time 

Means that two neighbouring ps cannot eat at the same

- all can feel hungry at the same time and pick up left chopstick
- Each waits for the right neighbours to drop
- All wait forever
- Deadlock

![image-20211121182805858](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121182805858.png)

- Allow at most 4 to be sitting simultaneously at the table
- Allow p to pick up chopsticks only when both are available
  - Both must be done within a critical section
- Use an asymmetric solution
  - Odd number philosopher picks up first left the right and even vice versa

## Transport Layer

### Services and Protocols

- provides logical communication between app processes running on different hosts
- The physical communciations happens in lower layers
- Hides what actually goes on below the aplication layer from the communicating processes
- The process on the sending sides send the AL messages and the IP adress and port number to TL and hopes that the TL will deliver the messages to the correct receiving process

In End systems

- Send sides
  - Breaks app messages into segments, adds header, passes to network layer
- RCV
  - Reassembles segments into messgaes, passes to app layer
- Transport layer protocol available to APPS
  - TCP UDP - in the internet

### UDP - User Datagram Protocol

- provides the bare minimum services
  - Packetizes application layer data, adds UDP header, and sends them to the NL
  - If the packets reach the desination, UDP delivers them to the right process
- In wrong order or missing UDP makes no effort to fix

- **Connectionless** Each UDP segment is treated independently
  - Has to specify destination for each message even with same recipient
- **No congestion control** - senders can send at any rate they wish even when the network is congested

#### USES

- Fast and less overhead
  - No connection establishment - adds a delay
  - Smaller header size than TCP - no need to recover lost or order packets
  - No congestion control - UDP sender can blast away as fast as possible
  - Video streaming, internet - loss tolerant, rate sensitive

- Reliable transfer over UDP then the facility must be build within the application itself
  - Add reliability at application layer

### TCP 

provides additional services to that by UDP

- Reliable data transfer - recovers losses, re-orders out of order packets
- Flow control - matches the sending speed of the sender to the reading speed of the receiver
- Congestion Control - controls the sending rate according to perceived network congestion

#### TCP Reliable Data Transfer

- Within the internet, packets get lost, corrupted and can arrive out of order
- TCP make suer none of this is perceived by the application processes
  - Processes see reliable communication channel
- Hence, necessary for TCP to enhance the unreiable network layer service toa reliable transport layer service

#### Reliable transfer over unreliable channel

- understand the requirements for building reliable data transfer datat transfer over an unreliable channel
- There can be:
  - Bit errors - bits flipped due to electrical noise
  - Loss packets - packets can get lost due to buffer overflow at routers

#### **Mechanisms needed**

- Checksums - to detect bit errors - include the sum of all 16 bit words in the header
  - If no match, then error
- Acknowledgements - ACKs - to indicate if a packet is correctly received at the receiver
- If correctly received, an ACK is sent, else nothing

- Timeout mechanism - Sender times out if ACK is not received within a timeout interval
- Retransmissions - retransmit lost or corrupted packets - ARQ **Automatic Repeat Request**
- Sequence number - to detect duplicates at the receiver - Packet's can be duplicated if ACK is lost or corrupted

#### Stop and wait ARQ

- Sender sends a packet
- Waits until it receives an ACK
- If ACK arrives, sends the next packet, otherwise retransmits the same after timeout interval

![image-20211121185556238](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121185556238.png)

#### When ACK is lost

Sender send P0 to receiver

- This is correctly received
- Send ACK
- ACK gets lost
- Sender times out
- Retransmits 0
- This received, detects this is duplicate and discards the packet
- Still send ACK back to the sender

![image-20211121185828275](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121185828275.png)

- Duplicate transmission detected because the sequence numbers were used

- Sufficient to use 1 bit sequence number since there can be at most one outstanding packet to be acknowledged by the receiver

Also name the alternating bit protocol  

#### Performance

- Provides reliable transfer
- Sender has to wait until ACK is received for the previously transmitted packet to transmit the next
- Has to wait for the ACK - could be a timeout
- Normally however RTT means that link is unused during this interval
- Poor link utilisation

![image-20211121190119408](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121190119408.png)

- Have a link 1Gbps R
- Packet Length $L=8000$
- $RTT$ is 30 ms

![image-20211121190205433](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121190205433.png)

#### Delay Bandwidth Product

Sender should be allowd to send more without waiting for the ACK

- The sender could send $R\cross RTT$ bits of additional data during the $RTT$, therefore total is $L+R\cross RTT$

![image-20211121190339031](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121190339031.png)

- $R\cross RTT$ is called the delay bandwidth product of the communicating pair and indicates the length of the pipeline

#### How many bits in the pipeline

- The buffer size at the receiver
  - Buffer generally has a finite buffer of $B$ bits
- Receiving process may not be reading from this all the time
- Sender should not send more than $B$ bits at the same time - avoid overflow
- Length of pipeline = $L+R\cross RTT$ bits 
  - Buffer size = B bits
  - Mx number of bits without ACK is $min(B,L+R\cross RTT)$

### Pipelines Protocols

- Pipelines protocols allow multiple un ACK packets in the pipeline
- ACK is sent individually or cumulatively
- Range of sequence numbers must be increased
- Two protocols - Go back N, Selective Repeat

#### Go Back N - Sliding Window protocols

- sender can send up to N packets without ACK
  - N = send window size, depends on the delay bandwidth product, receive buffer and other factors
  - After send, sender waits for an aCK
- Receiver maintains expectedseqnum which keeps track of the next expected sequence number to be received

- If the receiver correctly receives Packet $n$ and $n=expectedseqnum$ then it sends $Ack(n)$ which acknowledges all packets up to and including Packet $n$ Cumulative ACK
  - Incs expectedseqnum by 1

- If $n\neq expectedseqnum$ the receive discards the incoming packet, and sends ACK($expectedseqnum -1)$  showing all but one less correctly received

##### In Action

![image-20211121191944687](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121191944687.png)

- If an ACK is lost, and receiver returns ACK for all bits
  - Does not matter that ACK 0 is lost

![image-20211121192110653](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192110653.png)



#### Selective Repeat

- The SR does not discard out of order packets as long as they fall inside the receive window
- ACK are individual not cumulative
- Sender selectively retransmits packets whose ACK did not arrive
  - Maintains a timer for each unACKed packet in its send window
- The sender does not have to retransmit out of order packets

![image-20211121192207230](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192207230.png)





##### In Action



![image-20211121192328012](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192328012.png)



### TCP Reliable Data transfer

- uses a combination of GBN and SR protocols
  - Uses cumulative ACKs
  - TCP sender only retransmits the segment causing timeout

#### TCP Seq Numbers, ACKS

- Each byte is numbered
- Sequence number of a TCP segment is the byte number of the first byte of the segment

![image-20211121192527029](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192527029.png)

- ACK number is the number of next expected byte number
- TCP uses cumulative ACKs

![image-20211121192612834](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192612834.png)

#### Duplex Communication

- In TCP, data can flow in both ways
- To reduce number of transmissions, TCP piggybacks ACKs onto data segments
- A segment can carry data and serve and ACK 

![image-20211121192732470](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192732470.png)

#### Retransmission Scenario

ACK is lost

- After a while, sender times out and resends same segment, ignores the duplicate and sends the same ACK as before

![image-20211121192859037](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192859037.png)



Delayed ACK scenario

- Receiver send ACK 100 and ACK 120 after receiving
- Get delayed due to network
- By the time received by the sender the sender has resend the oldest segment in its window

![image-20211121192945651](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121192945651.png)

If ACK 100 gets lost, however 120 gets received - 120 serves as a cumulative acknowledgement for all previous

![image-20211121193142014](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121193142014.png)

Lost segment

- the received does not discard the third, stores third in a buffer sends ACK with 100, to show still expecting 100, the sender starts timer for segment 100 which continues after receiving duplicate 100, timer will expire and resend 100, after receiving 100, the receiver send ACK 140

![image-20211121193331659](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121193331659.png)

- Timeout  period is relatively long
  - Long delay before resending lost packet

##### TCP fast retransmit - Duplicate ACKs are good indicateors of packets loss

- They are sent when there is a gap in the received stream of bytes
- duplicate ACK contains number of the first missing byte

- Upon receiving 3 duplicate ACKs for a segment, TCP sender retransmits that segment without waiting for timer

![image-20211121193604573](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121193604573.png)

#### TCP segment Structure

- Sequence number
  - 32 bit sequence number of the segment indicating the number of the first byte
- ACK number
  - Number of the next byte expected
- Receive window
  - Used for flow control

![image-20211121193626962](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121193626962.png)

### TCP Flow Control

- The data in the pipeline should not exceed the receive buffer size
- Otherwise, the receive buffer will overflow and data will be lost

Receiver advertises free buffer space in the receive dinwos feild

​	rwnd

- Sender limits amount of unACKed data to receiver’s rwnd value
- $LastByteSent - LastByteAcked \leq rwnd$
- Guarantees no buffer overflow

#### Congestion Control

-  control the rate of transmission according to level of perceived congestion

#### Causes

- Occurs at a router when input rate > output rate

![image-20211121194355616](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121194355616.png)

Manifestations

- lost packets - buffer overflow
- Long delays - queueing in router buffers

Senders sending packets too fast - network links unable to carry

#### Cost of Delay

- both routers have infinite buffers - no loss in the system

![image-20211121194453784](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121194453784.png)

- The average queueing delay approaches infinity, as the link reaches max capacity 

![image-20211121194623783](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121194623783.png)

- The total input rate at $R_1$ approaching $C$
- If the input rate approaches the capacity of the outdoing link, the delay approaches infinity

**Throughput rate**

- There is no gain in further increasing $\lambda$ as there can be no more throughput

![image-20211121194737775](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121194737775.png)

#### Loop network

- $\lambda_i$ may not equal $\lambda$ as it is shared 
- If $\lambda\leq c/2$ then all traffic is successfully carried - all lambdas are equal
- Else
  - Total traffic at link $i$, is $\lambda+\lambda’$ but the throughput is only $c$ 
  - $\lambda'=\lambda\cdot\frac{c}{\lambda+\lambda’}$ - The same with $\lambda’’$
  - We now have two unknows and two equations
  - ![image-20211121195308401](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121195308401.png)

![image-20211121194945794](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121194945794.png)

##### Throughput:

![image-20211121195331444](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121195331444.png)

- As $\lambda\rightarrow\infty, \lambda''\rightarrow0$

#### Goals of Congestion Control

- limit the rate of senders such that the network never reaches congestion collapse
- Each flow gets a fair share of network resources

#### Congestion Detection

- TCP detects network through losses and delays
- A TCP sender assumes the netowrk congested when
  - Timeout
  - 3 Duplicate ACKs are received
- TCP treats these differently - timeout is more drastic than 3 duplicate ACKs

##### What to control

- TCP is a window based pipelined protocol
  - Send window size determines the transmission rate
- If window is size $W$ bytes, then rate of the transmission is $W/RTT$
- The maximum size of a TCP segment is called Maximum Segment Size
  - Determines the maximum frame size specified by the link layer protocol
- The number of segments to tranmis all data in windows is 

![image-20211121201238934](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121201238934.png)

##### How to control

- Maintains a parameter congestion window size denoted by cwnd and $W=LastByteSent - LastByteAcked \leq min(cwnd,rwnd)$ 
- When rwnd is large, sender sends cwnd determines the rate of transmissions:

![image-20211121201409977](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121201409977.png)

- cwnd is dynamic, function of network congestions



#### AIMD Algorithm

- additive increase, multiplicative decrease

Idea sender linearly increases the transmission rate cwnd until loss occurs, if loss occurs, reduce the rate by a factor if two

- Additive increase: increase cwnd by 1 MSS every RTT until loss detected
  - TCP allows 1 addition segment in each RTT until loss
- Cut cnwd in half after loss

![image-20211121201651286](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121201651286.png)

##### Why

- Why not MIMD or AIAD
- When multiple competing flows, this generates a fair allocation among competing flows

##### Fairness

- 2 TCP senders share the same bottleneck link capacity of $R$ and each should have a throughput $R/2$ 

![image-20211121201907466](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121201907466.png)

##### Relationship

- two competing sessions
- Assitive increase gives slope of 1, as throughput increases
- multiplicative decrease decreases throughput proportionally

![image-20211121202156263](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121202156263.png)

- The line of increase is parallel to the $x=y$ line,  

#### TCP Slow start

- Convergence of AIMD is low
  - Takes a lot of iterations
- To overcome, TCP uses an initial slow start phase, where window size is increase exponentially fast starting at a small value 
  - Slow start continues until a predefined threshold (ssthresh) is reached or loss is detected
- The initial aggressive behaviour ensures that the sender reaches the right operating speed quickly

##### ssthresh

- Slow start: initially cwnd=1MSS then double each RTT until a loss is detected or slow start threshold is reached

- Once ssthresh is reached, start AIMD 

![image-20211121202525148](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121202525148.png)

- ssthresh is selected where it remembers the previous value of loss

### Reacting to losses

- losses are detected through timeouts and 3 duplicated ACKs
  - Duplicate: some segments are lost but some received - lenient
  - Timeout - no segment is received - take drastic measure
- Loss indicated by timeout: 
  - ssthresh = 0.5 cwnd, cwnd = 1 MSS
  - After the sender enters into a slow start phase
- Loss by 3 duplicate ACKs
  - Ssthresh = 0.5cwnd, cwnd = 1/2 cwnd and window then grows lineary
  - This is not enforced
- TCP Tahoe always sets cwnd to 1 (timeout or 3 duplicate ACKS)

#### Example

![image-20211121202941520](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211121202941520.png)

## Deadlocks

A set of processes is in a deadlock when each process in the set is waiting for an event that can only be caused by another process in the set.

- Event never occurs

Events interested in are acquisition or relealse of some type of resource

- MUTEX locks
- CPU
- File
- IO

### System model

- System consists of $m$ differnt types of resources - R1,Rm
- Each resource has a number $W_i$ of instances
  - 3 Mutex, 4 Printers
- A set of processes $\{P_0,…,P-n\}$
- Each process utilizes a resource as follows
  - Request
  - Use
  - Release

### Necessary conditions for deadlock - Not sufficient

- Mutual exclusion - only one process at a time can use a resource
- Hold and wait - there must be a process holding some resources while waiting to acquire additional resources held by other processes
- No Preemption - a resource can be released only voluntarily by the process holding it, after that process has completed its task
- Circular wait - there must exist $\{P_0’,…,P_n’\}\subseteq\{P_0,…,P_n\}$ such that $P_0’$ is waiting for $P_1’$ and $P_n’$ is waiting for $P_0’$ 

### Resource allocation graph

A directed graph $G=(V,E)$

- $V$ is partitioned into two types
  - $P$ is the set of processes in the system
  - $R$ is the set of all resource types in the system

- Request edge - directed edge $P_i\rightarrow R_j$ 
- Assignment edge - directed edge $R_j\rightarrow P_i$ 

#### Symbols

- Process is represented by a circle
- Resource - represented by a box, the number of circles insisde show the isntance count

![image-20211125092012706](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125092012706.png)

#### Deadlock checking

- There is no cycle in the graph, therefore Circular wait is not satisfied

![image-20211125092044881](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125092044881.png)

- In this case there is a cycle, however the circular deadlock condition means that it is only between jobs 1 and 3
- $P_4\rightarrow P_3\rightarrow P_1\rightarrow P_2$

![image-20211125092228944](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125092228944.png)

- There is a cycle, there is a deadlock, $P_1,P_2,P_3$ wait for each other in a circular manner and none of the processes have all their requests satisfied

![image-20211125092349359](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125092349359.png)

#### Implications

- If the graph contains no cycles $\implies$ no deadlock
- If graph contains a cycle $\implies$ need to look further

#### Deadlock Detection algorithm

- Convert to an equivalent table representation
- Keeps track of the number of available instances of a resource - each column represents that
- Also requires a boolean flag for each process
  - When the flag is true, means the process has executed and can be claimed back
  - Proceeds by finding a flag that is still false and by finding the process where the pending requests can be satisfied

![image-20211125092631423](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125092631423.png)

- Final set of columns represent the current requests 

In this case, this proceeds with $P_3$

- Flag for P3 is set to true

![image-20211125093440911](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125093440911.png)

- Re applying the rule, the flag for P2 is set to true, and free the resources allocated to $P_2$

- After this, $P_1$s flag can be set and resources reclaimed

![image-20211125093620144](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125093620144.png)

- If at the end of the algorithm all processes have executed, then the system is deadlock free

### Handling Deadlocks

- Ensure that the system never enters a deadlock state
- Avoid or prevent from happening

#### Prevention

Ensure that at least one of the necessary conditions for deadlocks does not hold

- Mutex
- Hold and wait

- No pre-emption
- Circular wait

Prevention can be restrictive

- Harmless requests could be blocked

![image-20211125094136775](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125094136775.png)

- As shown above
  - P1 and P each requires resorces R1 and R2
  - Each process must request resources in the increasing order
  - If R2 is already held by P1 then the request of R1 by P1 will be blocked
  - It is safe to grant this request since there will be no cycle in the resulting system

#### Deadlock avoidance

- Less restrictive than prevention
- Determines if a request should be granted based on if the resulting allocation leaves the system in a **safe state**
- Safe state is a state where deadlock can never occur, no matter what future requests arrive

##### Algorithm

- needs a priori (advanced) information on resource requirements
- Each process declares the maximum number of instances of each resource type it may need
- Upon receiving a request, the deadlock avoidance algorithm checks if granting the resource immediately leaves the system in a safe state

##### Determining state safety

- Check for cycles
- Cycle does not guarantee unsafeness
  - Banker’s safety algorithm

##### Banker’s Algorithm

- Five processes P0 to P4
- 3 Resource types:
  - A(10 instances), B(5 instances), C(7 instances)
- Snapshot at time T0

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125094748332.png" alt="image-20211125094748332" style="zoom:50%;" />

- The context of the matrix NEED is defined to be MAX - ALLOCATION
- Currently, P1 is a process which additional needs can be satisfies with the available process
- Reclaim processes from P1

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125095115963.png" alt="image-20211125095115963" style="zoom:50%;" />

- Then reaply

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125095145103.png" alt="image-20211125095145103" style="zoom:50%;" />

- Finally

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125095205713.png" alt="image-20211125095205713" style="zoom:50%;" />

- At the end if there is a safe sequence, then the starting state is safe

##### Formal Description

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125095344297.png" alt="image-20211125095344297" style="zoom:50%;" />

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211125095409397.png" alt="image-20211125095409397" style="zoom:50%;" />

- By this algorithm the state safety can be determined
- To determine if a request for a resource can be granted immediatly, need to determine if by granting the erquest we are safe
- Pretend the request is granted, and determine the resulting state is safe using Banker’s Safety Algorithm
