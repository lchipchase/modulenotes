---
layout: default
title: Artificial Intelligence
math: true
---
# CS255 Artificial Intelligence

1. [CS241 - Operating Systems and Computer Networks](./CS241)
2. [CS255 - Artificial Intelligence](./CS255)
3. [CS258 - Databases](./CS258)
4. [CS260 - Algorithms](./CS260)

## Introduction to AI, Rational Agents, and Agent Architectures and Hierarchical Control

<u>Books:</u>

- Foundations of computational agents
- Artificial Intelligence A Modern Approach
- Other Reading (check Moodle) - check for Bayesian AI
- Examinable

----------------------------------------------------------------------------------------

**<u>Learning Outcomes</u>**

- Develop application for knowledge based systems, intelligent agents and architectures.
- Understand a wide variety of knowledge representation and artificial intelligence approaches.

- Understand various methods for search, planning and reinforcement learning.
- Understand various methods for representing and reasoning under uncertainty.

----------------------------------------------------------------------------------------

### Module Introduction

### What is AI

- The automation of activities that we associate with human thinking, such as decision making, problem solving and learning.

Falls into four categories

- Thinking humanly
- Thinking rationally
- Acting humanly
- Acting rationally

Is success a measure of human behaviour or rational behaviour.

----------------------------------------------------------------------------------------

#### Turing Test

- Have a human interrogator
- Have a human and an AI
- Can the human differentiate between the computer and the human
- Suggests the main components
  - Knowledge
  - Reasoning
  - Language
  - Learning
- Is it a good test of intelligence?
- General view is that it isn't
  - Not reproducible
  - Can’t use mathematical analysis
  - Lots of human intelligence factors not considered



**<u>Searle’s Chinese Room</u>** (counter example)

- Consider a room containing a human who understand only English, a rule book in English

----------------------------------------------------------------------------------------

#### Thinking Humanly

- Cognitive modelling
- Need a precise theory of the mind
- Can express this theory as a program
- Cognitive Science draws on AI and psychology

----------------------------------------------------------------------------------------

#### Thinking Rationally

- Derivation of conclusion from particular premises
- Various forms of logic, notation and rules of inference
- Task or problems expressed in logic, then a solution is deduced from here
- Problems:
  - Difficult to express tasks in logic
  - How to cope with uncertainty
  - Computational expense

----------------------------------------------------------------------------------------

#### Acting Rationally

- Doing the right thing
- Acting to maximise goal achievement given the available information and memory 
- Thought not necessarily involved
- Should be in pursuit of rational action.

----------------------------------------------------------------------------------------

#### Specialised AI vs Artificial General Intelligence

- AI designed to perform a specific tasks
- A systems which tries to perform intellectual tasks that a human can do (20 - 100 years)

----------------------------------------------------------------------------------------

#### Rational Agents

- An agent is an entity that perceives and acts
- We are concerned with designing rational agents
- An agent can be viewd as a function from percept histories to actions, $$f:P \rightarrow A$$

- Agents typically required to exhibit autonomy
- For any given class of environments and tasks, we seek the agent(s) with the best performance
- Perfect rationality usually impractical - we must work within out computational restraints
- Design the best thing possible for a set of resources.

----------------------------------------------------------------------------------------

#### What is AI

- They synthesis and analysis of computational agents that act intelligently
- An agent acts intelligently if:
  - Its actions are appropriate for its goals and circumstances
  - it is flexible to changing environments and goals
  - It learns from experience
  - It makes appropriate choices given perceptual and computational limitations

----------------------------------------------------------------------------------------

#### Goals Of AI

- Scientific goal.
- Engineering goal.

----------------------------------------------------------------------------------------

####  Applications of Rational Agents

- Air traffic
- Electricity distribution
- Information management
- Data mining
- Etc.

----------------------------------------------------------------------------------------

### Rational Agents

#### Inputs to an Agent

- Abilities - the set of possible actions
- Goals/preferences - what it wants 
- Prior Knowledge - what background knowledge does it have
- History of stimuli:
  - Current Stimuli - what it receives from the environment now
  - Past experiences - what has it seen before

Example of an agent - rescue robot

- abilities: movement, gripping, speech
- goals: rescue prople, explore
- prior Knowledge: what are important features, categories of object
- stimuli: vision, sonar, speech
- past experiences: effects of steering etc.

----------------------------------------------------------------------------------------

#### Goals in Rational Agents

- WLOG “goals” can be specified by some performance measure defining a numerical value
- Rational action is whichever action maximises the expected value of the performance measure given the percept sequence to date.
- Previous perceptions are typically important.

----------------------------------------------------------------------------------------

#### Rational Agents

- rational $$\neq$$ omniscient
  - A rational agent would know the actual outcome of its actions - agents are very rarly this since unexpected situations occur in dynamic environment. It must only do its best given the current percepts
- rational $$\neq$$ clairvoyant
  - Not expected to foresee future changes in its environment
- rational $$\neq$$ succesful
  - Rational action is defined in terms of expected value, rather than the actual value. A failed actional can still be rational.

----------------------------------------------------------------------------------------

#### Dimensions Complexity

| **Dimension**        | **Possible Values**                                          |
| -------------------- | ------------------------------------------------------------ |
| Modularity           | flat, modular, heirarchical                                  |
| Planning horizon     | non-planning, finite stage, indefinite stage, infinite stage |
| Representation       | states, features, relations                                  |
| Computational Limits | perfect rationality, bounded rationality                     |
| Learning             | knowledge is given, knowledge is learned                     |
| Sensing uncertainty  | fully observable, partially observable                       |
| Effect uncertainty   | deterministic, stochastic                                    |
| Preference           | goals, complex preferences                                   |
| Number of agents     | single or multiple                                           |
| Interaction          | offline, online                                              |

----------------------------------------------------------------------------------------

##### Modularity

- one level of abstraction: flat
- interacting modules that can be understood separately: modular
- modules that are (recursively) decomposed into modules: hierarchical

Flat adequate for simple systems, complex biological systems, computer systems, organizations are all hierarchical

----------------------------------------------------------------------------------------

##### Planning Horizon

- Static - world does not change
- Finite Stage - agent reasons about a fixed, finite number of time steps
- Indefinite stage - agent reasons about a finite, but not predetermined number of time steps
- Infinite stage - the agent plans for going on forever

----------------------------------------------------------------------------------------

##### Representation

Much of AI is finding compact representation and exploiting this for computational gains.

Agent can reason in terms of:

- Explicit states - one way the world can be - chess board
- Features or propositions
  - 30 binary features can represent $$2^{30}$$
- Individuals and relations
  - There is a feature for each relationship on each tuple of individuals
  - Often an agent can reason without knowing the individuals or when there are infinitely many individuals

----------------------------------------------------------------------------------------

##### Computational Limits

- Perfect rationality - the agent can determine the best course of action
- Bounded rationality - must make a good decision based on its computational capabilities

----------------------------------------------------------------------------------------

##### Learning from Experience

- Knowledge is given
- Knowledge is learned from data or past

----------------------------------------------------------------------------------------

##### Uncertainty 

Two main dimensions for this, sensing and effect, in each dimension an agent can have:

- No uncertainity - knows what is true
- Disjunctive uncertainity - set of states that are possible
- Probabilistic uncertainty - a probability distribution over the worlds

----------------------------------------------------------------------------------------

##### Why probability

- Agents must act even if uncertain
- Predictions are needed to decide
  - Definitive - you willl run out of power
  - Disjunctions: Charge for 30 mins or run out of power
  - Point Probabilities: probability you will run is 0.01 if you charge for 30 minutes else 0.8

- Agents who do not use probabilities, it will lose to those who do
- This can from data and prior knowledge

----------------------------------------------------------------------------------------

##### Sensing uncertainty

- Fully observable - can observe the entire state
- Partially observable - there is a number of states that can be perceived

----------------------------------------------------------------------------------------

##### Effect Uncertainty 

If an agent knew the state and its action could it predict the resulting state

- Deterministic: The resulting case is determined from the actions and the state
- Stochastic: there is uncertainty about the resulting state

----------------------------------------------------------------------------------------

##### Preference

What does the agent try to achieve

- Achievement goal - complex logical formula
- Complex preferences
  - May involve tradeoffs between various
  - Ordinal - only order matters
  - Cardinal - absolute values also matter

----------------------------------------------------------------------------------------

##### Number of agents

Are there multiple reasoning agents that need to be taken into account?

- Single agent reasoning: any other agents are part of the environment
- Multiple agent reasoning: an agent reasons strategically about the reasoning of other agents

Each of these can have their own goals or be independent.

----------------------------------------------------------------------------------------

##### Interaction

When does the agent reason to determine what to do?

- offline - before acting
- online - while interacting

----------------------------------------------------------------------------------------

##### How dimensions interact

- Partial observability makes multi-agent and indefinite horizon reasoning more complex

- Modularity interact with uncertainty and succinctness: some levels may be fully observable, and some partially observable
- Three values of dimensions promise to make reasoning simpler
  - Hierarchical
  - Individuals and relations
  - Bounded rationality

----------------------------------------------------------------------------------------

### Representations of Rational Agents:

![image-20211004111258326](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004111258326.png)

#### What do we want in a representation

- Rich enough to express the knowledge
- As close to the problem as possible - compact, natural and maintainable
- Amenable to efficient computation
  - Able to express features of the problem that can be exploited for computational gain	
  - Able to trade off accuracy and computation time and/or space
- Able to be acquired from people, data and past experiences

----------------------------------------------------------------------------------------

#### Defining a Solution

- Given an informal description of a problem, what is a solution
- Typically much is left unspecified, but unspecific parts cannot be filled in arbitrarily
- Much work in AI is motivated by common-sense reasoning - the computer needs to make common-sense conclusions about the unstated assumptions

----------------------------------------------------------------------------------------

#### Quality of solutions

- Does it matter if the answer is wrong or answers are missing?

**Different classes of solutions:**

- An **optimal solution** is a best solution according to some measure of solution quality. Often unachievable 
- A **Satisficing solution** is one that is good enough, according to some description of which solutions are adequate
- An **approximately optimal solution** is one whose measure of quality is close to the best and theoretically possible
- A **probable solution** is likely to be a solution

----------------------------------------------------------------------------------------

#### Decisions and Outcomes

- Good decisions can have bad outcomes; bad decisions can have bad outcomes
- Information can be of value because it leads to better decisions: value of information
- Often can trade off computation time and solution quality. An anytime algorithm can provide a solution at any time; given more time it can produce a better solution

An agent is not just concerned about finding the right answer, but about acquiring the appropriate information, and computing it in a timely manner.

**Solution quality vs computation time**

![image-20211004112559381](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004112559381.png)

A solution sooner could be better than one which is later. The time discount can be a function of elapsed time for example, which leaves a discounted quality to decide when to take the solution.

----------------------------------------------------------------------------------------

#### Choosing a Representation

We need to represent a problem to solve it on a computer

problem

​	$$\rightarrow$$ specification of problem

​			$$\rightarrow$$ appropriate computation

![image-20211004112809012](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004112809012.png)

For this module, mainly high level specification.

----------------------------------------------------------------------------------------

#### Physical symbol system hypothesis

- A symbol is a meaningful physical pattern that can be manipulated
- A symbol system creates, copies, modifies and destroys symbols

Physical symbol system hypothesis:

- A physical symbol system has the necessary and sufficient for general intelligent action.

----------------------------------------------------------------------------------------

#### Knowledge & Symbol Levels

Two levels of abstraction seem to be common among biological and computational entities

- The knowledge level is in terms of what an agens knows and what its goals are
- The symbol level is a level of description of an agent in terms of what reasoning it is doing

The knowledge level is about the external world to the agent.

The symbol level is about what symbols an agent uses to implement the knowledge level

----------------------------------------------------------------------------------------

#### Mapping from Problem to Representation

- What level of abstraction uses?
- What individuals and relations?
- How can the agen represent the knowledge to ensure that the representation is natural, modular, and maintainable?
- How can an agent acquire the information data, sensing, experience or other agents?

----------------------------------------------------------------------------------------

#### Choosing a level of abstraction

- A higher level is easier for a human to speccify and understand
- A low-level description can be more accurate and more predictive;  high-level descriptions abstract away details that may be important for solving the problem
- The lower the leve, the more difficult to reason with
- You may not know all required information for a low-level description

Sometimes it is possible to use multiple levels of abstraction

----------------------------------------------------------------------------------------

#### Reasoning and acting

Reasoning is the computation required to determine what an agent should do

- Design time reasoning and computation - carried out by the designer
- Offline - done by the agent before it has to act, background knowledge and data
- Online computation - computation that is done by an agent between receiving information and acting

----------------------------------------------------------------------------------------

### Agent Architectures and Hierarchical Control

#### Agent Systems

An agent system is made up of a agent and an environment

- An agent receives stimuli from the environment

- An agent carries out actions in the environment

  ![image-20211004114259967](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004114259967.png)

----------------------------------------------------------------------------------------

#### Agent System Architecture

An agent is made of a body and a controller:

- An agent interacts with the environment through its body
- The body is made up of:
  - Sensors - interpret stimuli
  - Actuators - carry out actions
- The controller receives percepts from the body
- The controller send commands to the body
- The body can also have reactions that are not controlled.

![image-20211004114658813](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004114658813.png)

(Controller is the main focus of the module)

The body itself can have some reactions which aren’t controlled (don’t need any reasoning)

----------------------------------------------------------------------------------------

#### Implementing a controller

- Brains of the agent
- Agents are situatiod in time, they receive sensory data in time, and do actions in time
- Controllers have limited memory and computational abilities
- The controller specifies the command every time
- The command at anu time can depen of the current and previous percepts - can’t store all

----------------------------------------------------------------------------------------

#### Agent functions

- Let $$T$$ be a set of time points
- A **percept trace** is a sequence of all past, present and future percepts received by the controller
- A **command trace** is a sequence of all past, present, and future command output by the controller
- A **transduction** is a function from percept traces into command traces
- A transduction is **causal** if the command trace up to time $$t$$ depend only on percepts up to $$t$$
- A **controller** is an implementation of a causal transduction
- An agents **history** at time $$t$$ is a sequence of past and present percepts and past commands
- A causal transduction specifies a function from an agent’s history at time $$t$$ into its action at time $$t$$.

Despite the fact that a casual transduction is a function of an agent’s history, ti cannot be directly implemented, because an agent does not have direct access to its entire history.

----------------------------------------------------------------------------------------

#### Belief States

- An agent does not have access to its entire history, only what is remembered
- The **memory** of **beleif state ** of an agent at time $$t$$ encodes all of the agent’s history that is has access to
- The belief state of an agent encapsulates the information about its past that is can use for current and future actions
- At every time a controller has to decide on:
  - What should it do?
  - What should it remember? - How is memory  updated, most recent decisions or most significant?
  - [As a function of its percepts and its memory].
- An example of a belief state could be: The belief state for an agent that is following a fixed sequence of instructions may be a program counter that records its current position in the sequence.
- A belief state can contain specific facts that are useful (where a delivery robot  leaves a parcel).
  - If is often useful to remember anything that is reasonably stable and cannot be immediately observed.
- The belief state could be a representation of the dynamics of the world and the meaning of its percepts, and the agent could use its perception to determine what is true in the world

**A Belief state transition function** for discrete time is a function:

$$remember : S \cross P \rightarrow S$$ 

Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts. $$s_{t+1} = remember(s_t,p_t)$$ means that $$s_{t+1}$$ is the beleif state following beleif state $$s_t$$ when $$p_t$$ is observed.

**A command function** is a function:

$$do: S \cross P \rightarrow C$$

Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts, and $$C$$ is the set of possible commands. $$c_t=do(s_t,p_t)$$ meant that the controller makes that command when seeing the states and percepts: $$s_t,p_t$$

----------------------------------------------------------------------------------------

#### Controller



![image-20211004115640946](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004115640946.png)

- If there exist a finite number of belief states, the controlelr is called a **finite state controller** or a **finite state machine**
- If there exist a finite number of features and each feature can have a finite number of possible values, the controller is a **factored finite state machine**

#### Functions implemented in a Controller



![image-20211004115713697](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004115713697.png)

For a discrete time, a controller implements:

- **Belief state function:** remember(belief state, percept) returns the next belief state [Blue]

  - **A Belief state transition function** for discrete time is a function:

    $$remember : S \cross P \rightarrow S$$ 

    Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts. $$s_{t+1} = remember(s_t,p_t)$$ means that $$s_{t+1}$$ is the beleif state following beleif state $$s_t$$ when $$p_t$$ is observed.

- **Command Function**: do (memory, percept) returns the command for the agent [Red]

  - **A command function** is a function:

    $$do: S \cross P \rightarrow C$$

    Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts, and $$C$$ is the set of possible commands. $$c_t=do(s_t,p_t)$$ meant that the controller makes that command when seeing the states and percepts: $$s_t,p_t$$

----------------------------------------------------------------------------------------

#### Agent Architectures

You do not need to implement an agent viewed as:

​															Perception $$\rightarrow$$ Reasoning $$\rightarrow$$ Action

As three independent modules feeding into the next:

- Too slow
- High level strategic reasoning takes more time than the reaction time needed to avoid obstacles
- The output of the perception depends on what you will do with it.

----------------------------------------------------------------------------------------

#### Hierarchical Control

- A better architecture is a **hierarchy of controllers**
- Each controller sees the controllers below at as a **virtual body** from which it get percepts and sends commands
- The lower-level controllers can
  - Run much faster, and hence react to the world more quickly
  - deliver a much simpler view of the world to the higher level controllers

**Hierarchical Robotics System Architecture**

![image-20211004125049935](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125049935.png)

----------------------------------------------------------------------------------------

#### Functions Implemented in a Layer

In general, there can be multiple different features passed from layer to layer and between states. There are three types of **inputs** to each layer at each time:

- The features that come from the belief state, which are referred to as the remembered or previous values of the features
- The features representing the percdepts from the layer below in the hierarchy
- The features representing the commands from the layer above in the hierarchy

There are three types of **outputs** from each layer at each time

- The higher level percepts for the layer above
- The lower level commands for the layer below
- The next values for the belief state features

![image-20211004125224283](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125224283.png)

- memory function: remember (memory, percept, command) - [Blue]
- command function: do(memory, percept, command) - [Red]
- percept function: higher percept(memory, percept command) - [Black]

----------------------------------------------------------------------------------------

#### Qualitative Versus Quantitative Representations:

Much of science an engineering considered **quantitative reasoning** with numerical quantities, using differential and intergral calculus as the main tools. **Qualitative reasoning** is reasoning often using logic about qualitative distinctions rather than numerical values.

**Qualitative reasoning** is important for:

- An agent may not know what the exact values are. Take a coffee pouring robot which may not know what the optimal angle is to pour a coffee, but a control rule may suffice to fill up the cup suitably.
- Reasoning may be applicable regardless of the quantitative values. You may want a kettle to heat up water so long as it is within specific parameters: $$\leq 100 \degree C$$ 
- An agent needs to do qualitative reasoning to determine which quantitative laws are applicable.

**Quantitative reasoning** is important for

- Landmarks - values which make qualitative distrinctions in the individual being modelled. Taking a coffee cup example, some qualitative distinctions include whether the cofee cup is empty or full. Which can then be used to determine what happens if the coffee cup is dropped for example.
- Orders of magnitude reasoning - approximate reasonign that ignores minor distinctions
- Qualitative derivative - indicate whether some value is increasing, descreasing or staying the same.

A flexible agent needs to do qualitative reasoning before it does quantitative reasoning. Sometimes qualitative reasoning is all that is needed. Thus, an agent does not always need to do quantitative reasoning, but sometimes it needs to do both.





#### Example - Delivery Robot

- Three actions: straight, right, left
- Can be given a **plan** consisitng of a set of names locations for the robot to go to in turn
- Robot must avoid obstacles
- It has a single **whisker sensor** pointing forward and to the right.
  - The robot can detect if the whisker hits an object
  - The robot knows where it is
- The object and locations can be moved dynamically - obstacles and new locations can be created dyncamically.

##### **Decomposition of the delivery robot**

![image-20211004125751159](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125751159.png)

- Highest Level:  follow the plan, takes a plan as an Input from an external source, chooses how to follow the plan. The middle layer acts as an interface between the two outer layers.
- Middle Level: Takes the information - how to avoid obstacles, passes steering input back to the lower level.
- Lowest Level: takes information from the environment, steers the robot, find obstacles and position.

----------------------------------------------------------------------------------------

#### What Should be in an Agent’s Belief State?

- An agent decides what to do based on its belief state and what it observes
- A purely **reactive** agent does not have a belief state, a **dead reckoning** agent does not perceive the world, - neither of these work well in complicated situations - not for the deliver robot example.
- Often it is useful for the agent’s belief state to be a model of the world (itself and the environment which it is present)

----------------------------------------------------------------------------------------

#### Agents Modelling the World

- Belief state definition is often very general and doesn’t constrain what should be remembered.
- A model of a world is a representaion of the state of the world - regardless of how accurate.
- One method is to maintain its beleif about the world and to update this based on its commands.
  - Requires both the state of the world, and the dynamics of the world. Given these, the state at the next time can be predicted.
- When the world is dynamic, or noisy actuators (say a wheel slips), these inconsitencies can accumulate and become very significant.

Another approach is for the agent to use perception to build a model of the relevant part of the world. This tends to be ambiguous and noisy - in a 3D world, it is very difficult to determine the world from one image or one scan. 

A way to improve on this is to:

- If both noise of forward prediction and sensor noise are modelled, the next belief state can be estimated using Bayes’ rule (filtering)
- With more complicated sensors, a model can be used to predict where visual features can be found, then vision can be used to look for these features close to the predicted location - makes the task simpler, and reduces the errors in position arising from forward prediction.

A control problem is separable if the best action can be found by first finding the best model of the world, then using that to determine the best action. (Most world problems are not separable)

-------------------

### Agent Functions

- An agent can be viewed as being specified by the **agent function**  mapping percept sequences to actions $$f:P \rightarrow A$$
- **Ideal rational agent:** does whatever action is expected to maximise performance measure on basis of percept sequence and built-in knowledge
- In principle, there is an **ideal mapping** of percept sequences to actions corresponding to ideal agent
- Simple approach - lookup table:
  - Table too large
  - Time to build
  - No autonomy
- Lookup table suggests an “ideal mapping”
- One agent function (or a small equivalence class) is **rational** can be seen to approximate ideal mapping
- The aim: - find a way to implement the rational agent function
- Need to implement this concisely, and the implementation must be:
  - Efficient
  - Exhibit autonomy [if required]
  - Get as close as possible to the ideal mapping

----------------------------------------------------------------------------------------

#### Embedded and Simulated Agents

There are multiple ways an agent’s controller can be used:

- An **embedded agent** is one that is run on the real world, where actions are carried out in a real domain and the sensing comes from a domain.
- A **simulated agent** is one that is run with a simulated body and environet - wheree a program takes in the commands and returns approximate percepts. This is often used to debug a controller before its deployment.
- An **agent system model** is where there are models of the controller, the body, and the environment which can answer questions about how the agent will behave. This can be used to prove properties of agents before they are build, or to use hypothetical questions about an agent that may be difficult or dangerous to answer with a real agent.

These are useful for different purposes:

- Embedded mode is how the agent must run to be useful.
- A simulated agent is useful to test and deub the controller when many design options must be explored, where building a design is expensive, or then environment is unaccessible.
  - Models must always abstract some information, therefore depending on the sensitivity of the agent, this could mean that as much detail is required as possible to determine the effectiveness of the agent.
- A model of the agent, a model of the set of possible environment, and a specification of the correct behaviour allows us to prove theorems about how the agent will work in such environment. 
- Given a model of the agent and the environment, some aspects of the agent can be left unspecified and can be adjusted to produce the desired optimal behaviour. this is the general idea behind optimization and planning.
- In reinforcement learning, the agent improves its performance while interacting with the real world.

-------------------

#### Acting with Reasoning

- Previously, we assumed than an agent has some belief maintained through time. For intelligent agents, this can become extremely complex.
- An intelligent agent requires some internal representation of its own belief state.
  - **Knowledge** is the information about a domain that is used for problem solving within that domain
  - This can include general knowledge applied to particular situations.
  - Therefore, it is more general than the beliefs about a specific state.
- A knowledge based system is a system that uses knowledge about a domain to act or solve problems.

Knowledge tends to mean general information that is taken to be true.

Belief tends to be information that can be revised based on new information. This often comes with measures of how much they should be believed and models of how the beliefs interact:

- In an AI system, knowledge is typically not necessarily true, and is justified as only being usefly. This can become blurry when one module of an agent reats information as true, but another may be able to revise that information.

- The image below shows a model for a knowledge based agent
  - A **knowledge base** is build offline and is used online to produce actions. This is orthogonal to the layered view of an agent - an intelligent agent requires both hierarchical organisation and knowledge bases



- **Online**, when the agent is acting, it uses its knowledge base, its observations of the world and its goals and abilities to choose what to do, and how to update its knowedge base.
  - This is its long term memory, where it keeps information that is needed to act in the future.
  - This comes from proior knowledge, and is combined what is learned from data and past experiences
  - The **beleife state** is the short term memory, which  maintains the model of the current environment needed between time steps
  - (There is not always a specific distinction between general and specific knowledge)
- **Offline** before the agent must act, it can build the knowledge base that is useful for it online.
  - The role of offline computation is to make online computation more efficient
  - The knowledge is built on prior knowledge, and from datat of past experiences (not what is learned from data) - either its own experience or what it has been given.
  - In most domains, the agent must se whatever information is available, and therefore requires both rich prior knowledge, and lots of data.

The goals and abilities are given online, offline, or both depending on the agent. The online computation can be made more efficient if the knowledge base is tuned for the particular goals and abilities, however this is often not possible when goals are determined at runtime.

![image-20211008110950107](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211008110950107.png)

-----------------

##### Design Time and Offline Computation

The knowledge base required for online computation can be built initially at design time and then augmented offline by the agent.

An **ontology** agent is a specification of the meaning of the symbols used in an information system. 

- This specifies what is being modeled and the vocabulary used in the system
- In simplest form, if an agent is using explicit state-based representation with full observability, the ontology specifies the mapping between the world and state.
  - Without this, the agent may know its state, but without the ontology will now know what to do.
- In other cases, the ontology defines the featrues of the individuals or relationships. This is what is needed to convert raw sense data into something meaningful for the agent 
- Ontologies are built by communities - independently of a particular knowldge base or application. It is this what allows for effective communication and interoperation of the data from multipe sources.

An ontology typically comes before the data and the prior knowledge - we require an ontology to have data or to have knowledge. Without this, data are just sequences of bits. The ontology often evolves as the system is developed.

- It may also specify a level or levels of abstraction. If the ontology changes - the data must change

The knowledge base is typically build offline from a combination of expert knowledge base and data. Usually this is built before the agent knows teh particulars of the environment.

##### Roles of Offline Computation 

- Software engineers - build the inference engine and the UI. They typically know nothing about the contents of the knowledge base. No need to be experts in the use of the system they implement, but must be experts in the language they use.
- Domain experts - the people who have the appropriate prior knowledge about the domain. They know about the domain, but typically know nothing about the particular case that is being considered. 
  - They do not typically know about the internal workings of the AI system. They only have a semantic view of the knowledge and no notion of the algorithms used by the engine. The sytem should interact with them in terms of the domain.
- Knowledge engineers - design, build, and debug the knowledge base in consolation with domain experts. They know the details of the system and about the domain through the domain expert. Nothing is knows about any particular case. Should know useful inference techniques and how the complete system works.

![image-20211008115039198](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211008115039198.png)

##### Roles of Online Computation

Online, the information about particular cases becomes apparent and the agent must act. Online, the following roles are involved.

- User - a person who has need for expertise or has information about individual cases.
  - Typically not experts in the domain of the knowledge base.
  - Often do not know what information is needed by the system
  - Unreasonable to expect them to vlunteer infrmation about a particular case
  - A simple interface must be provided, as users typically do not understand the internal structure of the system.
  - They often must make an informed decision based on the recommendation of the system (along with an explanationas to why)
- Sensors provide information about an environment.
  - Passive sensors - continuously feeds information to the agent. 
  - Active sensors - controlled or asked for information.
  - Sensors that are passive sensors may be seen as active sensors at a higher levels of abstraction
- External knowledge sources - web sites or a database can be asked questions and provide the answer for a limited domain
  - Asking a weather website for the weather
  - The interface between an external knowledge source and an agent is called a wrapper;
  - A wrapper translates between the representation the agent uses, and the queries the external knowledge source is prepared to handle
  - When website and databases can be used together, they can be used together because the same symbols have the same meaning
  - This is called **semantic interoperability**







#### Agent Types

Find fundamental types of agents:

- Simple reflex agents

  $$\rightarrow$$ condition action rules - essentially if, else

- Reflex agents with state

  $$\rightarrow$$ retains knowledge about the world

- Goal based agents

  $$\rightarrow$$ has a representation of which states are desirable

- Utility based agents

  $$\rightarrow$$ able to discern some useful measure (cost or quality for example), between different possible means of achieving a certain state

- Learning agents

  $$\rightarrow$$ able to modify their behaviour based on their performance, or given new information.

----------------------------------------------------------------------------------------

#### Simple Reflex Agents

- Based on a set of condition-action rules
- We can view this as summarising the notional lookup table
- Although simple, reflex agents can still achieve relatively complex behaviour
- Learning of new rules possible
- Very quick and efficient

![image-20211004131536572](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004131536572.png)



- The environment is outside the box,

-The sensors feed information to the agent, and decides based on the current state (no history).

----------------------------------------------------------------------------------------

#### Reflex agents with State / Model based Reflex Agents

- This includes the current state with the memory
- Still takes the current state from the environment
- How you update beliefs changes how the world appears to be

![image-20211004131801447](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004131801447.png)

- Pure reflex agents only work if the “correct” decision of what to do can be made on the current percept
- Typically need some internal state to track the environment
- The update state function uses knowledge about how the world evolves and how actions affect the world, a different way of deciding what to remember changes how the world will be perceived
- Agents can use this information to track unseen parts of the world

----------------------------------------------------------------------------------------

#### Goal Based Agents

- For more complex problems
- Still takes in the state and history

![image-20211004132048605](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132048605.png)

- Knowing current environment state not normally enough to choose action - need to know what is trying  to be achieved
- Can combine information about the environment, goals, and effects of actions to choose what to do
- Relatively simple if goal is achievable in single action
- Normally, this is not the case, and the goal required a sequence of actions - use search or planning
- Much more flexible than a reflex agent

----------------------------------------------------------------------------------------

#### Utility Based Agents

- Initially look similar to goal agents
- Think about what benefit each action has

![image-20211004132411310](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132411310.png)

- Often different ways to achieve a goal, some desirable some less so:
  - Might achieve free space on hard drive
- Can use a **utility function** to judge resulting states of the world
- Enables a choice about which goals to achieve - select the one with the highest utility
- If achievement if uncertain we can measure the importance of the goal against the likelihood of achievement

----------------------------------------------------------------------------------------

#### Learning Agents

- Can be build on the foundations of the previous four
- The previous four fit in the **performance element** of the agent.

![image-20211004132737788](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132737788.png)

- Adds three components:
  - critic
  - learning element
  - problem generator
- Learning element 0 changed the performance element
  - Takes information about how the performance element is operating and changes that
  - Find information on how it is doing
  - Generates learning goals
- If the agent designer has incomplete information about the environment - an agent needs to be able to learn
- Learning provides an agent with independence or autonomy - behaviour determined by its own experience
- **Idea**: - percepts ca be used to improve agent’s ability to act in the future (as well as for choosing an action now)
- Learning results from interaction between the agent and the world, and observation by agent of its decision making

----------------------------------------------------------------------------------------

##### Learning Agent Components

Typically, this has four conceptual components:

- **Learning element:** responsible for making improvements - takes knowledge of the performance element and feeds back on the agent’s success, then suggests improvements to the performance element

- **Performance Element:** responsible for acting - take percepts and returns actions (in non learning agents this is considered to be the whole agent)
- Design of the learning element dependent on design of the performance element:
  - Learning algorithms for improving logical inference is different to those for improving decision-theoretic agents
- **Critic:** tells the learning agent how well the agent is doing, using a **performance standard**
  - The performance standard should be a fixed measure and external to the agent
- **Problem Generator:** responsible for suggesting actions in pursuit of new (and informative depending on the situation) experiences
  - Without the problem generator, the performance element would always do what it thinks is best. However, performing a “suboptimal” action may lead to discovery of better actions in future (route finding).

----------------------------------------------------------------------------------------

## Problem Solving and Uninformed Search

- Problem Solving agents
- Problem types
- Problem formulation
- State space graph
- Basic tree and graph search

----------------------------------------------------------------------------------------

### Problem Solving

- Often not given an algorithm to solve a problem, but a specification of what a sultion is - a solution must be found
- A typical problem is when the agent is in one state, it has a set of deterministic actions it can carry out, and wants to get to a goal state
- Many AI problems can be abstracted into the problem of finding a path in a directed graph
- Often there is more than one way to represent a problem as a graph.



##### Problem solving as Search

The idea of search is straightoforward: the agent contructs a set of potential partial solution to a problem that can be checked to see if they are truyly solutions, or if they could lead to solutions. This proceeds by repeatedly selecting a partial solution, stopping if it is a path to a goal, and otherwise extending it by one more arc in all possible ways.

- This underlies much of AI: “When an agent is given a problem, it is usually given only a description that lets it recognize a solution, not an algorithm to solve it. It has to search for a solution.” - Book

----------------------------------------------------------------------------------------

#### Problem Solving: Example

- Suppose you are in Cov and goal is getting to London
  - How is this achieved, which method of transport
  - Do you go via somewhere else
- A computer needs to know which of these is bad and which is good
- Finding a sequence of actions that would result in you being in london is **problem solving**

A **problem solving agent** is a goal-based agent that will determine sequences of actions that lead to desirable states

----------------------------------------------------------------------------------------

#### **Four Problem Solving Steps**

- **Goal Formulation:** identify a goal given the current situation
- **Problem Formulation:** identify permissible actions (or operators), and the states to consider
- **Search:** find the sequence of actions to achieve the goal
- **Execution:** perform the actions in the solution

**Two types of problem solving**

- **Offline** - complete knowledge of problem and solution (Coventry example - assuming no external issues)
- **Online** - involves acting without complete knowledge of problem and solution.



#### A simple Problem Solving Agent

----------------------------------------------------------------------------------------

**BEGIN ALGORITHM** 

**function** $$SimpleProblemSolvingAgent(p)$$ - *This is offline* 

​		**returns** $$action$$ 

----------------------------------------------------------------------------------------

​	**Inputs:** $$p$$, as percept

​	**Static**: $$s$$, an action sequence, initially empty

​               $$state$$, a description of the current world state: 

​		    	$$g$$ a goal, initially null

----------------------------------------------------------------------------------------

​	$$state \leftarrow$$ $$UpdateState(state,p$$) 

​	**if** $$s = \emptyset$$ **then**

​		$$g \leftarrow$$ $$FormulateGoal(state)$$

​		$$problem \leftarrow FormulateProblem(state,p)$$

​		$$s \leftarrow Search(problem)$$ 

​	$$action \leftarrow Recommendation(s,state)$$

​	$$s \leftarrow Remainder(s,state)$$

​	**return** $$action$$

**END ALGORITHM**

----------------------------------------------------------------------------------------

##### Dimensions of (typical) State-space Search

| **Dimension**        | **Values**                                                   |
| -------------------- | ------------------------------------------------------------ |
| Modularity           | flat                                                         |
| Representation       | states                                                       |
| Planning Horizon     | indefinite stage - most complex part, finite number of steps, don’t know how many |
| Sensing uncertainty  | fully observable                                             |
| Effect uncertainty   | deterministic                                                |
| Preference           | goals                                                        |
| Number of agents     | single                                                       |
| Computational limits | perfect rationality - assumption                             |
| Interaction          | offline                                                      |

- This is a simplistic problem (almost all are simplest)

-----------------

#### Problem Types	

- Deterministic, fully observable $$\rightarrow$$ **single-state problem**
  - Sensors tell the agent current state and knows exactly what its actions do
  - Means it knows exactly what state it will be in after any action
- Deterministic, partially observable $$\rightarrow$$ **multi-state problem**
  - limited access to state, but knows what actions do
  - can determine a set of states resulting from an action
  - instead of manipulating single states, agent must manipulate sets of states
- Stochastic, partially observable $$\rightarrow$$ **contingency problem**
  - don’t know current state, and don’t know what state will result from action
  - must use sensors during execution
  - solution is a **tree** with branches for contingencies
  - often **interleave** search and execution
- Unknown state space - knwoedlge is leaned $$\rightarrow$$ **exploration problem** (online)
  - agent doesn’t know what its actions will do

---------------

#### Vacuum Cleaner World

- Simple examle environment
- Vacuum cleaner robot, in a world with just two locations
- Each location may or may not contain dirt
- Agent in one of the locations
- Agent can move left or right, or such up dirt
- Example State:![image-20211004141624238](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004141624238.png)

- Single state: (fully observable)  - move to the right and solve

- Multiple state (partially observable) start in any of $$\{1,2,…,8\}$$
  - Right will take you to $$\{2,4,6,8\}$$
- Contingency (partially observable) start in 5:
  - Solution $$ =$$ [right, **if** dirt **then** suck]

----------------

#### Problem Formulation: State-space problems

State space searching **assumes** that:

- The agent has perfect knowledge of the state space and can observe what state it is in
- The agent has a set of actions that have known deterministic effects.
- Some states are goal states, that the agent wants to reach one of these goals, and can recognise a goal
- A solution is a sequence of actions that will get the agent from its current state to a goal state

A **state-space problem** consists of:

- a set of states
- a subset of states called the start states
- a set of actions
- an **action function:** given a state and an action, returns a new state
- a set of goal states, specific as function, $$goal(s)$$ 
- a criterion that specifies the quality of an acceptable solution (possibly), maybe any solution within 10% optimality is acceptable



##### Selecting a State Space

The real world is very complex - state space may need to be **abstracted**

- Abstract state $$\approx$$ set of real states
- Abstract operations $$\approx$$ compbination of real actions e.g. Coventry $$\rightarrow$$ Warwick comprises all possible routes.
- Abstract solution $$\approx$$ a set of paths that exist in the real world.

##### Example: The 8 puzzle

- States? locations of tiles
- Operators? move: up, down, left, right,
  - Another operator is moving the blank tile up and down, is equivalent  (this is better)
- Goal Test? given goal state
- Path cost? 1 per operator

##### State space for Vacuum world

- States? integer dirt and vacuum locations
- Operators? left, right, suck
- Goal test? no dirt
- Path cost? 1 per operator

![image-20211004142938938](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004142938938.png)



-------------------

#### State Space Graphs

A general formulation of a problem solving problem is a  **state space graph**

- A (directed) **graph** consisting of a set $$N$$ of **nodes** and a set $$A$$ of ordered pairs of nodes, called **arcs** or **edges**

- Node $$n_{2}$$ is a **neighbour** of $$n_{1}$$ if there is an arc from  $$n_{1}$$ to  $$n_{2}$$. 

  **If** $$\langle n_{1},n_{2}\rangle \in A$$

- A **path** is a sequence of nodes $$\langle n_{0},n_{1}, ... , n_{k}\rangle : \langle n_{i-1},n_{i}\rangle \in A$$

- A **cycle** is a non-empty path such that the end node is the same as the start node: $$\langle n_{0},n_{1}, ... , n_{k}\rangle : n_0 = n_k, k\neq 0$$

- A directed graph without any cycles is called a Directed Acyclic Graph **(DAG)**

- A tree is a DAG where there is one node with no incoming arcs and every other node has exactly one incoming arc. The node with no incoming arcs is called the root of the tree, and nodes with no outgoing arcs are called leaves

- The **length** of the path  $$\langle n_{0},n_{1}, ... , n_{k}\rangle$$ is $$k$$

- Given a set of **start nodes** and **goal nodes**, a **solution** is a path from a start node to a goal node.

Sometimes, there is a cost associated with arcs. This is written as $$cost(\langle n_i,n_j \rangle)$$, which consequently induces a cost of paths:

- Given a path $$p= \langle n_{0},n_{1}, ... , n_{k}\rangle$$, the cost of that path is the sum of the costs of all the arcs in the path.
- An optimal solution is one of the least cost, such that there is no path $$p’:cost(p')<cost(p)$$  

------------------------

Often, this graph is not given explicitly, it is dynamically constructed as needed. All that is needed for the search algorithms that follow is a way to generate the neighbours of a node and to determine if a node is a goal node.

- **Forward Branching Factor** the number of arcs leaving the node
- **Backward Branching Factor** the number of arcs entering the node

These measure provide complexity of graphs, and when using time complexity, these are intended to be constant. They are very important,as they are key components of the size of the graph

### Basic Tree Search and Graph Search Algorithms

#### Tree Search Algorithm

- Offline simulated exploration of the state space
- Starting with a start state expand one of the explored states by generating its successors to build a search tree.

---------------------

**BEGIN ALGORITHM**

**function** $$TreeSearch(problem,strategy)$$

​		**returns** $$solution | | failiure$$ 

​		Initialise search tree using initial state of $$problem$$

​		**loop do**

​				**if** no candidates for expansion **then return** $$failiure$$ 

​				choose leaf node for expansion according to $$strategy$$

​				**if** node contains a goal state **then**

​						**return** corresponding $$solution$$ 

​				**else** expand node and add resulting nodes to search tree

**END ALGORITHM**

----------------

 ##### Implementing Tree Search

- A State represents a physical configuration
- A node is a data structure comprising part of search tree
- Node: state, parent, children, depth, path cost
- Nodes waiting to be expanded called the **frontier**
- Represent frontier as a queue

(Variable $$g$$ typically used to represent the current **cost**)

------------------

##### Search Strategies

- Strategy defined by **order or node expansion**
- Evaluation along several dimensions
  - **completeness:** always find solution (if exists)?
  - **optimality:** always find the least-cost solution?
  - **time complexity:** number of nodes expanded
    - maximum depth branching factor **$$b$$** 
    - depth of least cost solution $$d$$ 
    - maximum depth of state $$m$$ (could be $$\infty$$)
  - **space complexity:** maximum number of nodes in memory (measured in terms of $$b,d,m$$)

---------------------------

##### Simple Uninformed Tree Search Strategies

**Uninformed** search only uses information from the problem definition

Two Examples are:

- Breadth-first
- Depth-first

-------------------------

###### Breadth-First Tree Search   

- Complete - always finds solution if finite
- Time
  - $$1 + b + b^2 + b^3 + … + b^d = O(b^d)$$  - technically $$O(b^{d+1})$$ because check for goal state occurs when the node is selected for expansion rather than when generates - all nodes at depth $$d$$ are expanded
- Space
  - Each leaf node is kept in memory: $$O(b^{d-1})$$ explored and $$O(b^d)$$ in frontier, so complexity is $$O(b^d)$$
- Optimal
  - If cost is **non-decreasing** function of depth - operators equal cost then yes. Otherwise not
- **Space** is the main problem

**Time and memory requirements:**

($$b=10, 10,000$$ nodes/sec; $$1000$$ bytes /node)

| Depth | Nodes     | Time        | Memory        |
| ----- | --------- | ----------- | ------------- |
| 2     | 11,000    | .11 sec     | 1 megabyte    |
| 4     | 111,100   | 11 sec      | 106 megabytes |
| 6     | $$10^7$$ | 19 min      | 10 gigabytes  |
| 8     | $$10^9$$ | 31 hours    | 1 terabyte    |
| 10    | $$10^{11}$$ | 129 days    | 101 terabytes |
| 12    | $$10^{13}$$ | 35 years    | 10 petabytes  |
| 14    | $$10^{15}$$ | 3,523 years | 1 exabyte     |

- Each depth increases exponentially.
- Can solves the BFS tree in 5 moves.

---------------------------------

###### Depth-first Tree Search

- Expand deepest unexpanded node
- $$QueueingFn = $$ insert successors at front of queue (LIFO)
- Incomplete - fails in infinite depth, spaces with loops
- Change search to avoid loops $$\Rightarrow$$ complete in finite spaces
- Time
  - $$O(b^m$$) terrible if $$m$$ is much larger than $$d$$. If solutions are dense may be much faster than BFS
- Space
  - $$O(bm)$$ linear space, store the path from the root to leaf an unexpanded system of nodes.
- Not Optimal.

--------------------------------

### Graph Search Algorithms

- With tree search, state spaces with loops give rise to repeates states that cause inefficiencies, and complete tree is infinite
- Practical way ot exploring the state space that can account for the repetitions
  - Given a graph, start nodes, and goal nodes, incrementally explore paths from the star nodes
  - Maintain a **frontier** of paths from the start node that have been explored
  - As search proceeds, the frontier of paths from the start node that have been explored
  - The way in which the frontier is expanded defines the **search strategy**

![image-20211004153843019](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004153843019.png)

- Nodes marked in red are the frontier, gradually expanding outwards

-------------------

#### Graph Search Algorithm

- Which value is selected from the frontier at each stage defines the search strategy
- The neighbours define the graph
- $$goal$$ defines what is a solution
- If more than one answer is required, the search can continue from return call
- We can modify to return all possible solutions

--------------

**BEGIN ALGORITHM**

**Input:** a graph

​			 a set of nodes

​			 Boolean procedure $$goal(n)$$ that test if $$n$$ is a goal node.

-----------

$$frontier := \{\langle s\rangle : s$$ is a start node$$\}$$

**while** $$frontier \neq \emptyset$$

​		**select** and **remove** path $$\langle n_0,…,n_k \rangle$$ from $$frontier$$

​		**if** $$goal(n_k)$$

​				**return **$$\langle n_0,…,n_k \rangle$$

​		**for each** neighbour $$n$$ of $$n_k$$ 

​				**add** $$\langle n_0,…,n_k, n \rangle$$ to $$frontier$$

**end while**

**END ALGORITHM**

--------------

#### Breadth First Graph Search

- Treats the frontier as a queue
- It always selects one of the earliest elements added to the frontier
- If the list of paths on the frontier is $$[p_1,p_2,….,p_r]$$:
  - $$p_1$$ is selected and its neighbours are added to the end of the queue, after $$p_r$$ 
  - $$p_2$$ is selected next



**Complexity of Breadth-first** Graph Search

- If the branching factor for all nodes is finite, BF graph search is garunteed to find a solution if one exists - it is garunteed  to find the path with fewest arcs
- Time complexity is exponential in the path length: $$b^n$$, where $$b$$ is branching factor, $$n$$ is path length
- Space complexity is exponential in path length: $$b^n$$
- The search is unconstrained by the goal

--------------

#### Depth First Graph Search

- Depth-first search treats the frontier as a stack
- It always selects one of the last elements to the frontier
- If the list of paths on the frontier is $$[p_1,p_2,…]$$:$$p_1$$ is selected and the paths that extend $$p_1$$ are added to the front of the stack (in front of $$p_2$$)
- $$p_2$$ is only selected when all paths from $$p_1$$ have been explored

**Complexity of DFS**

- DF graph search is not guaranteed to halt on infinite graphs or on graphs with cycles

- The space complexity is linear in the number of arcs from the start to the current node

  (Supposing $$\langle n_0,…,n_k \rangle$$ is selected from the frontier. In DFS every other path on the frontier has the form: $$\langle n_0,…,n_k, n \rangle$$ for some index $$i < k$$ and some node $$m$$ that is a neighbour of $$n_i$$ . Therefore it follows the selected path for a number of arcs and has one extra node. So, the frontier has the current path and paths to neighbours of the nodes on this path. There can be at mode $$k(b-1)$$ paths on the frontier, and so space is linear in the number of arcs from the start to the current node)

- If the graph is a finite tree, with a forward branching factor $$\leq b$$, and all paths from the start have at most $$k$$ arcs, worst case time complexity is $$O(b^k)$$ 
- The search is unconstrained by the goal

--------------

#### **Lowest-Cost-First Search**

- Sometimes there are costs associated with arcs
- The cost of a path is the sum of the costs of its arcs

$$
cost(\langle n_0,...,n_k\rangle) = \sum_{i=1}^kcost(\langle n_{i-1},n_i\rangle)
$$

​	An optimal solution is one with minimum cost

- At each stage, LCFS selects a path on the frontier with lowest cost
- The frontier is a PQ ordered by path cost
- The first path to a goal is a least-cost path to a goal node
- When arc costs are equal $$\Longrightarrow$$ BFS

------------

#### Summary of Graph Search Strategies (as of yet)

| **Strategy**      | Frontier Selection | Complete | Halts | Space  |
| ----------------- | ------------------ | -------- | ----- | ------ |
| Breadth-First     | First node added   | Yes      | No    | Exp    |
| Depth-First       | Last node added    | No       | No    | Linear |
| Lowest-Cost-First | Minimal $$cost(p)$$ | Yes      | No    | Exp    |

**Complete** - guaranteed to find a solution if there is one (for graphs with finite number of neighbours, even on infinite graphs)

**Halts** - on finite graph (perhaps with cycles)

**Space** - as a function of the length of current path

---------------

------------------------

## Informed Search

### Overview

Uninformed search is generally very inefficient; if we have extra information about the problem then it is good to use it

- Improve the search using specific knowledge - **informed search**

- This is still offline search

-------------

### Heuristic Search

- Idea: don’t ignore the goal when selecting paths
- Often there is extra knowledge that can be used to guide the search: heuristics
- $$h(n)$$ is an estimate of the cost of the shortes path from node $$n$$ to a goal node
- $$h(n)$$ needs to be efficient to compute
- $$h$$ can be extended to paths: $$h(\langle n_0,…,n_k \rangle)=h(n_k)$$ 
- $$h(n)$$ is an **underestimate** if there is no path from $$n$$ to a goal with cost strictly less than $$h(n)$$  - typically want there to be an underestimate
- An **admissible heuristic** is a nonnegative heuristic function that is an underestimate of the actual cost of a path to a goal.

---------------

#### Example Heuristic Function

- If nodes are points on a Euclidean plane, and the cost is the distance $$h(n)$$ can be the straight line distance from $$n$$ to the closest goal
- A heuristic function can be found by solving a simpler version of the problem

----------------

### Best-First Search

- We can use the heuristic function to determine the order of the stack representing the frontier
- **Idea** select the path or node that is closest to a goal according to the heuristic function
- Heuristic DFS select a neighbour so that the best neighbour is selected first
- Greedy best first search selects a path on the fronteir with the lowest heuristic value
- Best-First search treats the frontier as a PQ ordered by $$h$$

![image-20211011084452241](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011084452241.png)

- The graph above is bad for simple BFS: heurist DFS will select the node below $$s$$ and never terminate. Greedy best-first seach will cycle between the node below $$s$$ never finding an alternative route

-------------------

#### Complexity

- Space is exponential in path length: $$b^n$$, where $$b$$ is the branching factor, and $$n$$ is the path length
- Time complexity is exponential in $$b^n$$
- Not guaranteed to fund a solution
- Does not always find the shortest path.

-------------

### A* Search

- uses path cost and heuristic value
- $$cost(p)$$ is the cost of path $$p$$ - referred to as $$g(p)$$
- $$h(p)$$ estimates the cost from the end of $$p$$ to a goal



Let $$f(p) = g(p) + h(p)$$

- $$f(p)$$ estimates the total path cost of going from a start node to a goal via $$p$$

![image-20211011085000129](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011085000129.png)

-----------------

#### Algorithm

- Is a mix of lowest cost-first and best-first search
- It treats the frontier as a PQ ordered by $$f(p)$$
- It always selects the node on the frontier with the lowest estimated distance from the start to a goal node constrained to go via that node

--------

#### Admissibility of A*

- A searhc algorithm is admissible, if whenever a solution exists, it returns an optimal solution
- A* is admissible if:
  - The branching factor is finite
  - Arc costs are bounded above 0 - some $$\epsilon$$
  - $$h(n)$$ is nonnegative and an underestimate of the cost of the shortest path from $$n$$ to a goal node

Why is A* admissible

- If a path $$p$$ to a goal is selected from a frontier, can there be a shorter path to a goal? 
- Suppose $$p’$$ is on the frontier, Because $$p$$ was chosen before $$p’$$ and $$h(p) = 0 ~$$: $$g(p) \leq g(p’) + h(p’)$$
- Because $$h$$ is an underestimate: $$g(p’) + h(p’) \leq g(p’’)$$ for any path $$p’’$$ that extends $$p’$$ 
- So $$g(p) \leq g(p’’)$$ for any other path $$p’’$$ to the goal

A* can always find a solution if there is one:

- The frontier always constrains the initial part of a path to a goal, before that goal is selected
- A* halts, as the costs of the paths on the frontier keeps increasing, and will eventually exceed any finite number
- Admissibility **does not** guarantee that every node selected from the frontier is on an optimal path.
- Admissibility ensure that the first solution found will be optimal, even in graphs with cycles.

-----------------

#### Good Heuristics

Selecting the correct heuristic is very important:

Suppose $$c$$ is the cost of an optimal solution. Consider what happens to a path $$p$$ where

- $$cost(p) + h(p)  < c$$
- $$cost(p) + h(p) = c$$
- $$cost(p) + h(p) > c$$ 

A better function can help because:

- A* expands all paths from the start in the set $$\{p: cost(p) + h(p) <c\}$$
- As well as some paths from the set $$\{p: cost(p) + h(p) =c\}$$
- Increasing $$h$$ while keeping it admissible reduces the size of the sets.
- If the second is large, there can be a large amount of variability in the space and time complexity of A*

--------------

#### Complexity of A*

Time:

- Exponential in relative error in $$h\cross$$length of solution

Space:

- Exponential: keeps all nodes in memory - the main problem with the algorithm

#### Optimality of the A* algorithm

An algorithm is optimal if no other search algorithm uses less time or expands fewer nodes with a guarantee of solution quality.

- The optimal solution would be the algorithm which picks the correct node at every choice, this is not effective as we can not directly implement it,



**Optimality of A*** - among search algorithms that only use arc costs and a heuristic estimate of the cost from a node toa goal, no algorithm expands fewer nodes than A* and guarantees to find a lWowest-cost path:

**“Proof” Sketch**

Given only information about the arc costs and heuristic information, unless the algorithm has expanded each path $$p$$, where $$f(p)$$ is less than the cost of an optimal path, it does not know whether $$p$$ leads to a lower cost path.

- Suppose an algorithm $$A’$$ found a path for problem $$P$$ where some path $$p$$ was not expanded s.t. $$f(p)$$ was less than the solution found.
- Suppose there was another problem $$P’$$ which was the same as $$P$$ except that there was a path via $$p$$ with cost $$f(p)$$. The algorithm cannot tell $$ P’$$ from $$P$$, because it did not expand the path $$p$$, so would report the same solution for both $$P,P’$$ but the solution found for $$P$$ would not be optimal for $$P’$$ as the solution found has a higher cost than the path via $$p$$. Therefore, an algorithm is not guaranteed

##### Counter Example

Consider an algorithm that does a forward $$A^*$$ like search, and a backward dynamic programming search, where steps are interleaved (swapping forward and backward steps). The backward dynamic programming search builds a table of the $$costToGoal(n)$$ values of the actual discovered cost from $$n$$ to a goal, and maintains a bound $$b$$ where it has explored all paths of cost less than $$b$$ to a goal. The forward search uses a PQ on $$cost(p) + c(n)$$, where $$n$$ is the node at the end of the path $$p$$ and $$c(n)$$ is $$costToGoal(n)$$ if it has been computed, otherwise $$c(n)$$ is $$max(h(n), b)$$. The intuition is that if a path exists from the end of path $$p$$ to a goal node, it either uses a path that has been discovered by the backward search, or a path that costs at least $$b$$  This algorithm is guaranteed to find a lowest-cost path and often expands fewer nodes than A∗

##### Conclusion

Having a counterexample seems to mean the optimality of A* is false, however the proof seems correct for algorithms only doing forward search.

##### Dynamic programming and A*

Dynamic programming can be used to construct heuristics for A* and branch-and-bound searches.

- A way to build a heuristic function is to simplifying a problem until that simplified problem has a small enough state space.
- Dynamic programming can be used to find an optimal path length in a simplified problem, which can then be used as a heuristic function for the original problem.

----------------

### Summary of Search Strategies

| **Strategy**          | Frontier Selection | Complete | Halts | Space  |
| --------------------- | ------------------ | -------- | ----- | ------ |
| Breadth-First         | First node added   | Yes      | No    | Exp    |
| Depth-First           | Last node added    | No       | No    | Linear |
| Lowest-Cost-First     | Minimal $$cost(p)$$ | Yes      | No    | Exp    |
| Heuristic depth first | Local min $$h(p)$$ | No       | No    | Linear |
| Best-first            | Global min $$h(p)$$ | No       | No    | Exp    |
| A*                    | Minimal $$f(p)$$ | Yes      | No    | Exp    |

-----------------

### Cycle Checking, Path Pruning

The simplest way of pruning the search tree, whilst guaranteeing that a solution will be found in a finite graph, is to ensure that the algorithm does not consider neighbours what are already on the path from the start.

- The search can prune a path that ends in  a node already on the path, without removing an optimal solution
- In depth-first method, checking for cycles can be done in a constant time in path length - hash function, however this varies depending on implementation. Where the space complexity is exponential, a cycle check takes linear time.
- Other methods, this can be done in linear time in path length

---------

#### Multiple-Path Pruning

In staring in $$s$$, there are two paths, one is better than the other, so only one needs be considered

- Prune a path to node $$n$$ if the seach has already found a path to $$n$$
- Implemented by maintaining an explored set, the closed list, of nodes that at the end of expanded paths
- The closed list is initially empty

- When a path $$\langle n_0,…,n_k\rangle$$ is selected, if $$n_k$$ in closed list the path is discarded, otherwise $$n_k$$ is added to the closed list and algorithm continues as before

![image-20211011091417126](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011091417126.png)

-----------

#### Multiple-Path Pruning and Optimal Solutions

What if the subsequent path is shorter than the first?

- ensure this does not happen, the shortest path is always found first
  - This is hard to do
- Remove all paths from the frontier that use the longer path - if there is a path $$p=\langle s,…,n,…,m\rangle$$ on the frontier, and a path $$p’$$ is found with a lower cost than the portion of $$p$$ from $$s$$ to $$n$$, then $$p$$ can be removed from the frontier.
- Change the initial segment of the paths on the frontier to use the shorter path:  if there is a path $$p=\langle s,…,n,…,m\rangle$$ on the frontier, and a path $$p’$$ is found with a lower cost than the portion of $$p$$ from $$s$$ to $$n$$, then $$p’$$ can replace the initial path of $$p$$ to $$n$$

-----------------

### Multiple Path Pruning and A*

A* does not guarantee that when a path to a node is selected for the first time it is the lowest cost path to that node - the admissibility theorem guarantees the minimum path cost for the goal node, but not for every path.

- suppose path $$p’$$ to $$n’$$ was selected, but there is a lower cost path to $$n’$$. Suppose the lower cost path is via path $$p$$ on the frontier
- Suppose path $$p$$ ends at  node $$n$$
- $$p’$$ was selected before $$p$$ $$f(p') \leq f(p)$$, so $$cost(p’) + h(p’) \leq cost(p) + h(p)$$ 
- Suppose $$cost(n,n’)$$ is the actual cost of a path from $$n$$ to $$n’$$. The path to $$n’$$ via $$p$$ is lower cost than via $$p’$$ so: $$cost(p) + cost(n,n’) < cost(p’)$$
- Resulting in : $$cost(n,n’)<cost(p’) - cost(p) \leq h(p) - h(p’) = h(n) -h(n’)$$
- We can ensure this does not happen if: $$|h(n) -h(n’)| \leq cost (n,n’)$$ 

Multiple path pruning includes a cycle check, because a cycle is another path to a node, and is therefore pruned.

- Can be done in constant time if the graph is explicit ally stored, by setting a bit on each node to which a path has been found.
- Can be done in logarithmic time (in the number of expanded nodes if indexed properly), if the graph is dynamically generated, by storing the closed list of all the expanded nodes.

This is always preferred in breadth-first search where virtually all nodes must be stored anyways, however for DFS strategies, the algorithm does not otherwise have to store all of the nodes already considered - this would make it exponential in space. Therefore, cycle checking is preferred.

---------------

#### Monotone Restriction

- Heuristic function $$h$$ satisfies the monotone restriction if $$|h(m) -h(n)| \leq cost(m,n)$$ for every arc: $$\langle m,n\rangle$$
- If $$h$$ satisfies the monotone restriction it is consistent meaning $$h(n) \leq cost(n,n’) + h(n’)$$ for any two nodes $$n,n’$$
- A* with a consistent heuristic and multiple path pruning always finds the shortes path of a goal

These factors increase the strength of the admissibility criterion. 

----------------

### Direction of Search

- The definition of searching is symmetric: find a path from start nodes to goal node or from goal node to start nodes:
- **Forward branching factor** number of arcs out of a node 
- **Backward branching factor** number of arcs into a node
- Search complexity is $$b^n$$: should use the forward search if the forward branching factor is less than the backward branching factor and vice versa
- When the graph is dynamically constructed, the backwards graph may not be available

------------------

#### Bidirectional Search

- Search from the goal and forward and start simultaneously:
- This is good, because: $$2b^{k/2} << b^k$$
- Main problem is ensuring they meet.
  - Depth first in both directions is unlikely to meet as the small search frontiers are likely to pass one another, however, breadth-first search in both directions would guarantee a meeting
  - A combination of depth-first search in one direction and breadth-first in the other would guarantee the required intersection of the frontiers, but which one to choose in which direction can be difficult.
- This is often used with one breadth first method that builds a set of locations that can lead to the goal and in the other direction a method can be used to find a path to these interesting locations

------------

#### Island Driven Search

- **Idea** find a set of islands between $$s$$ and $$g$$

  $$s\rightarrow i_1 \rightarrow i_2 \rightarrow … \rightarrow i_m \rightarrow g$$ 

- Can be effective as $$mb^{k/m} << b^k$$ 

- The problem is to identify the islands that the path must pass through - difficult to ensure optimality

- The subproblems can be solved using islands $$\Longrightarrow$$ hierarchy of abstractions

  - Incorrect choice of island could make the problem harder or impossible to solve

---------------

### Dynamic Programming

**Idea** for statically stored graphs, build a table of $$dist(n)$$ the actual distance of the shortest path from node $$n$$ to a goal

This can be build backwards from the goal:

- $$dist(n) = 0$$ if $$isGoal(n)$$,
- otherwise $$dist(n) = min_{\langle n,m \rangle \in A}(|\langle n,m \rangle | +dist (m))$$ 
- Following this policy will take the agent from any node to a goal along a lowest cost path. Given a $$costToGoal$$, determining which arc is optimal takes constant time with respect to the size of the graph - assuming a bounded number of neighbours for each node.
- Dynamic programming take time and space linear in the size of the graph to build the table for the $$costToGoal$$ or $$dist(n)$$ table



When building a cost to goal function, the searcher implicitly determined which neighbour leads to the goal, instead of determining at runtime which neighbours is on an optimal path, it can store this information

#### Useful When:

- The goal nodes are explicit 
- A lowest cost path
- The graph is finite and small enough to store the cost to goal values for each node
- The goal does not often change
- The policy is used a number of times for each goal, so that the cost of generating the cost of goal valued can be amortized over many instances

#### Problematic When

- it requires enough space to store the graph
- The dist function must be recomputed for each goal
- the time and space required is linear in the size of the graph, where the graph size for finite graphs is typically exponential in the path length.


-----------------

### Bounded Depth-First Search

- A bounded depth-first search takes a bound (depth or cost) and does not expand paths that exceed that bound
  - Explores part of the search path
  - Uses linear space in the depth of the search

----------------

### Iterative-deepening search

This is used to combine the benefits of the space complexity of DFS with the optimality of BFS.

- Start with a bound $$b=0$$
- Do a bounded depth-first search with bound $$b$$
- If a solution is found return that solution
- Otherwise increment $$b$$ and repeat
- You must distinguish:
  - Failure because the depth bound was reached - **Failing unnaturally**
    - Here the search needs to be retried with a larger depth bound
  - Failure that does not involve reaching the depth bound - **Failing naturally**
  - - Here it is a waste of time to try again with a larger depth bound, because no path exists no matter what the depth.



This finds the same first solution as a breadth-first search, since using a depth-first search iterative-deepening uses linear space.

- Iterative deepening has an asymptotic overhead of $$(\frac{b}{b-1})$$ times the cost of expanding the nodes at depth $$k$$ using breadth-first search
- When $$b=2$$, there is an overhead factor of 2, when $$b=3$$ there is an overhead factor of 1.5.
  - As $$b$$ increases, the overhead factor reduces

A clear problem is the wasted computation that occurs at each step, however this is not as prevalent when the branching factor is high.

Assume a branching factor $$b>1$$

- Consider the search where the bound is $$k$$. At this depth, there are $$b^k$$ nodes, and each of these has been generated once.
- Those at $$k-1$$ have been generated twice, $$k-2$$ have been generated three times …
- The total generated nodes is:

$$
b^k +2b^{k-1} + 3b^{k-2}+...+kb = b^k(1+2b^{-1} + 3b^{-2}+kb^{1-k})\\
\leq b^k\bigg(\sum^{\infty}_{i-1}ib^{1-i} \bigg) \\
=b^k\bigg(\frac{b}{b-1}\bigg)^2
$$

- This means there is a constant overhead pf $$(b/(b-1))^2$$ times the cost of generating the nodes at depth $$n$$.
  - When $$b=2$$, there is an overhead factor of 4, when $$b=3$$, there is an overhead factor of 2.25.

#### Iterative Deepening Pseudocode - from book

![image-20211013112439463](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211013112439463.png)

#### Iterative Deepening and A* (IDA*)

This can be applied to A*, and performs repeated depth-bounded depth-first searches. Instead of a bound on the number of arcs, it is a bound on the value of $$f(n)$$. This threshold starts at the value of $$f(s)$$, where $$s$$ is the starting mode with the minimal $$h$$ value. This then carries out depth-first depth-bounded search but never expands a node with a higher $$f$$ value than the current bound.

- If it fails unaturally, the next bound is the minimum of the $$f$$ value that exceed the previous bound.
- Checks the same nodes as $$A*$$, but recomputes them with depth-first search instead of storing.

------------

### Depth-first Branch-and-Bound 

The goal is to maintain the lowest-cost path to a goal found so far, particularly when many paths exist, and we want an optimal path - A* search for example

- Combined depth-first search with heuristic information
- Find optimal solution
- Most useful when there are multiple solution and an optimal one is sought after
- Uses the space of a depth-first search.

Suppose we want to find a single optimal solution

- Suppose bound is the cost of the lowest-cost path found to a goal so far

- What if the search encounters a path $$p$$ such that $$cost(p)+h(p) \geq bound$$

  $$\Rightarrow p$$ can be pruned

- What can we do if a non-pruned path to a goal is found?

  $$\Rightarrow bound$$ can be set to the cost of $$p$$ and $$p$$ can be remembered as the best solution so far.

- Uses linear space

- Guarantees an optimal solution

-----------

#### How should bound be initialised

- The bound can be initialised to $$\infty$$
- The bound can be set to estimate the optimal path cost
- After a depth-first search terimantes, either:
  - A solution was found
  - No solution was found, no path pruned
  - No solution was found, path was pruned
- This can be combined with iterative deepening to increase the bound until a solution is found, or there is no solution.
- Cycle pruning works well with depth-first branch-and-bound
- Multiple-path pruning is not appropriate as storing explored set defeats space saving of depth-first search.

#### Bound and a Solution

The algorithm will return an optimal solution - a least-cost path from the start node to a goal node - if there is a solution with cost less thatn the initial bound.

- If this is slightly above the cost of a lowest-cost path, the algorithm can find an optimal path expanding no more arcs than A* search. This happens when the initial bound is such that the algorithm prunes any path that has a higher cost than a lowest-cost path. Once a path is found, it only explores path whose $$f$$ value is lower than the one found.
- If no solution is returned:
  - When $$bound = \infty$$ then no solution exists
  - When $$bound$$ is finite, then none exist with costs less than that
- This can be combined with iterative deepening to increase the bound until a solution is found or it can be shown there is no solution.

--------------

### Heuristics 

A way to determine which node seems the most promising.

- A function $$h(n)$$ take a node $$n$$ and returns a non-negative real number that is an estimate of the path cost from node $$n$$ to the goal node.

- It must only use information that can be readily obtained about a node

  - Often a trade off between the qeuality of the heuristic and the work required to determine it.

  $$\Rightarrow$$ Often, a way to derive a heuristic function is to solve a simpler problem and to use the actual cost in the simplified problem.

- The heuristic function can be extended to applicable paths, and the heuristic value of the path is the value of the node at the end of the path:

  - $$h(\langle n_0,…,n_k\rangle)=h(n_k)$$ 



A use of this is ordering neighbouring nodes that are added to the stack representing the frontier in depth-first search - the best neighbour will be selected first.

- This is **heuristic depth-first search**
- Chooses the locally best path
- Explores all paths from the selected path before selecting another.
- Often used, suffers from the problems of DFS.



Another use of this is to always select a path on the frontier with the lowest heuristic value:

- Best-first search
- Usually does not work well - can follow paths that look promising, however their costs may keep increasing

#### Admissible Heuristics example

- 8 puzzle
- Branching factor around 3 - typical solution around 20 steps
- Exhaustive search: $$3^{20}$$ 
- Need a decent heuristic

#### Possible heuristics

- $$h_1(n) = $$ number of misplaced tiles
  - Admissible since each misplaced tile must have been moved at least once.
- $$h_2(n) =$$ city block, or Manhattan distance: number of squares from desired location
  - Admissible since a single move can only move one tile step closer to goal

#### Characterising Heuristics

**Definition** if A* tree-search expands $$N$$ nodes and a solution is $$d$$, the effective branching factor $$b^*$$ is the branching factor a uniform tree of depth $$d$$ would have to contain $$N$$ nodes

- $$N = 1 + b^* + (b^*)^2 + … + (b^*)^d$$ 
- Closer this gets to 1, the larger the problem that can be solved.

This can be estimated experimentally: 

![image-20211011122723216](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011122723216.png)

- If $$h_2(n) \geq h_1(n)$$ then $$h_2$$ dominates $$h_1$$ 



On average A* tree-search using $$h_2(n)$$ expands less nodes than using  $$h_1(n)$$ 

- A* expands all nodes with $$f(n) <f^*$$ where $$f^*$$ is the cost of optimal path
- All nodes with $$h(n) <f^* - g(n)$$ are expanded

**Conclusion** a higher value is better, provided it is admissible

- need to make sure that the required computation required to determine the heuristic does not outweigh the savings.

#### Deriving heuristics - Relaxed problems

- Can derive admissible heuristics from the exact solution cost of a relaxed version of the problem
- A problem with less restrictions on operators is a relaxed problem
- If relaxed rules of the 8-puzzle so that a tile can move anywhere, then $$h_1(n)$$ gives the shortest solution
- If relax rules so that a tile can move to any adjacent square,m then $$h_2(n)$$ gives the shortest solution

We would like to generate heuristics automatically

- If the problem is described in a formal language, relaxed problems can be generated.





**Example** The 8-puzzle can be described as: a tile can move from square A to square B if A is adjacent to B and B is blank

- Generate relaxed problems
  - A tile can move from square A to square B if A is adjacent to B
  - A tile can move from square A to square B if B is blank
  - A tile can move from square A to square B
- Each of these problems give a heuristic measue



If one dominate choose that one, however typically there may be several admissible heurisitics

- If none dominate, then use $$h(n) = max(h_1(n),…,h_m(n))$$ 
- Since all are admissible, then $$h$$ is admissible
- $$h$$ dominates all of its constituent heuristics

#### Deriving heuristics - Subproblems

- Derive admissible heuristic from solution cost of a subproblem of a given problem
- Cost of subproblem= lower bound on cost

#### Deriving heuristics - Pattern Databases

- Store exact solution costs for each possible subproblem instance
- Heuristic $$h_{DB}=$$ cost of solution to corresponding subproblem
- Construct the database by searching backwards from the goal
- Combine pattern databases as before $$h(n) = max(h_1(n),…,h_m(n))$$ 

#### Deriving heuristics: Disjoint Pattern Databases

- Choice of tiles 1,2,3,4 was arbitrary, could have done the same for 5,6,7,8
- Cannot add the solution together, as each subproblem is likely to share moves
- If shared moves are discounted, then we can add the heuristics $$\rightarrow$$ disjoint pattern database
- Need to be able to divide up the problem so moves only affect a single subproblem

#### Deriving Heuristics - Other

- Statistical approach - run over training problems and gather stats
- Select features of state that contribute
  - In chess, number of pieces left
  - Determine the weightings of factors 





## Constrain Satisfaction Problems

A CSP is characterized by 

- A set of variables: $$(V_1,V_2,…,V_n)$$
- Each variable has an associated domain $$D_{v_i}$$ of possible values.
- There are hard constraints on various subsets of the variables which specify legal combinations of values for these variables
- A solution to the CSP is an assignment of a value to each variable that satisfies all the constraints.

CSPs as optimization problems:

- For optimization problems there is a function which gives the cost for each assignment of a value to every variable
- A solution is an assignment of values to the variables that minimizes the cost function

### States VS features

Typically, there are too many constraints for an agent to reason with. Also, most problems do not come with an explicit set of constraints, they need to be derived or thought about. Therefore, the states are typically described implicitly in terms of features.

- When describing a real state space, it is more natural to describe the features that make up the state space rather than explicitly enumerating the states.

The definition of states and features are intertwined - they can be described in terms of each other.

**States** - can be defied in terms of features: features can be  primitive and state corresponds to an assignment a value to each feature

**Features** - can be defined n terms of state: the states can be primitive and a feature is a function of the sates. Given a state, the function returns the value of the feature on that state.

#### Using features vs states

Every feature has a domain that is the set of values that it can take on. The domain of the features is the range of the function of the states.

​	One of the advantages of reasoning in terms of features is the computational savings - a binary features the domain has two values, therefore many states can be described by only a few features.

- Reasoning in terms of features may be easier than reasoning in terms of a billion states, - Reasoning in terms of 100 features is not that many, however reasoning in terms of 2^100^ states is not possible
- Often, the states are not independent, therefore there are sets of combination of features that are possible to happen to each other.

### Types of CSP

CSP’s are described in terms of features, without explicitly considering time - CPSs will be described in terms of possible worlds.

- **Possible worlds** - possible way the world could be - when representing a crossworl puzzle, the possible worlds could correspond to the ways the crossword could be filled out.
  - In an electrical environment, a possible world specifies the position of every switch and component

#### Algebraic variables

- a symbol used to denote the features of possible worlds - will be wreitten starting with an upper case letter. Every variable has a given domain $$dom(V)$$, which is the set of values which it can take on.

Discrete variables:

- Finite domains: n variables of domain of domain size: $$d\Longrightarrow O(d^n)$$ complete assignments 
  - Boolean CSPs
- Infinite domains - integers, strings
  - Lecture and seminar scheduling variables are start/end times
  - Cant enumerate all assignments - need a constraint language
  - linear constraints are solvable, nonlinear undecideable

Continuous variable - time
- Domain is continuous
- Linear constraints solvable in polynomial time by linear programming methods



Possible worlds can be defined in terms of variables or variables can be define d in terms of possible worlds.

- Variables can be primitive and a possible world coressponds to a total assignment of a value to each vairable
- Worlds can be primitive and a variable isa  function from possible worlds into the domain of the variable in that possible world.

#### Types of Constraints

- **Unary constraints** - involve a single variable
- **Binary constraints** - involve pairs of variables
- **Higher-order constraints** involve 3 or more variables
- **Preferences / soft constraints** a 1005 is better than 0805 for a lecture to start - represented by a cost for each variable

A world satisfies a set of constrains if for every constraint, the value assigned in the world to the variables in scope of the constraint satisfy the constraint

#### Cryptarithmetic Example

![image-20211018120208478](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018120208478.png)

- letters which you must assign digits to in such a way that the sum works
  - O + O = R, W + W  = U, T + T = O
- Represent with the contraint graph above
  - Circles are variables
  - Circles as possible remainders
- Boxes represent contraint
  - A constraint with, O, R $$X_1$$ 

Variables:

- $$F,T,U,W,R,O,X_1,X_2,X_3$$ 

Domains:

- $$\{0,1,2,3,4,5,6,7,8,9,\}$$ 

Constraints:

- $$allDiff(F,T,U,W,R,O)$$  - letters = different digits

  $$O+O = R + 10\cdot X_1$$ 

  $$X_1 + W + W = U + 10 \cdot X_2$$

  $$X_1 + U + U = O + 10 \cdot X_3$$

  $$X_3 = F$$

  

#### N-Queens Problem

Find the config that places $$n$$ queens on an $$n \cross n$$ board with no pairs of queens attacking.

#### Scheduling Problem

- Variables $$A,B,C,D,E$$ that represent the starting times of activities.
- Domains: $$D_A = \{1,2,3,4\}, D_B = \{1,2,3,4\}, D_C = \{1,2,3,4\}, D_D = \{1,2,3,4\}, D_E = \{1,2,3,4\}$$ 

-  Constraints: $$(B\neq 3) \and (C\neq 2)$$ for example. 

#### Real-world CSPs

- Assignment problems
- Timetabling
- Hardware configuration
- Spreadsheets

### Solving CSPs

#### Generate-and-Test Algorithm

- Generate the assignment space - the cartesian product of all domain - the set of total assignments for example
- Test each of these with the contraints
- There need to be $$ d^n $$ assignments.

- Incredibly inefficient.

#### Backtracking Algorithms

- Systematically explore $$D$$ by instantiating the variables one at a time
- Evaluate the constraint predicate as soon as all its variables are bound
- Any partial assignment that does not satisfy the constraint can be pruned

Properties:

- Every solution appears at depth $$n$$ - this means that we can use depth first search

- Path is irrelevant, so can use complete-state formulation

- Branching factor $$b(n-l)d$$ at depth $$l$$ - $$n!\cdot d^n$$ leaves

  Top level branching factor is $$nd$$ since any of $$d$$ values can be assigned to any of $$n$$ variables - the next level branching factor is $$(n-1)d$$ etc

- Variables are commutative

- Only need to consider assignments to a single variable at each node

- Backtracking search is the basic uninformed algorithm for CSPs

- Can solver n-queens for $$n\approx 25$$

**Algorithm**

![image-20211018122443859](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018122443859.png)

- Is a recursive algorithm



#### Improving Search Efficiency

General-purpose methods can give huge gains in speed

- Which variable should be assigned next - **MRV & degree heuristic**
- In what order should its values be tried - **LCV**
- Can we detect inevitable failure early - **Consistency Algorithms**
- Can we take advantage of problem structure - **Cutset Conditioning & variable elimination**

Can avoid the need to use domain specific knowledge.

#### CSP Backtracking Search Heuristics

##### MRV 

- Choose the variable with the fewest legal values
  - Also called “fail first” - will pick variable most likely to cause failure - if exists a variable with 0 possible assignments will pick and fail immediately

##### Degree Heuristic

- Tie breaker among MRV variables
  - Choose the variable with the most constraints on remaining variables
  - Attempts to reduce branching factor of future choices

##### Least Constraining Value

- Given a variable, choose the LCV - the one that rules out the fewest values in the remaining variables.

The combination of these can solve n-queens for $$n \approx 1000$$ 

### CSP as Graph Searching

A CSP can be solved by graph -seraching

- A node is an assignment of values to some of the variables

- Suppose node $$N$$ is the assignment of $$X_1 = v_1,…,X_k = v_k$$ that is not assigned in $$N$$

  For each value $$y_i \in dom(Y)$$ 

   $$X_1 = v_1,…,X_k = v_k, Y=y_i$$ is a neighbour of $$N$$ if it is consistent with the constraints

- The start node is an empty assignment

- Goal node is a total assignment which satisfies the constraint

#### Consistency Algorithms

- Prune the domains as much as possible before selecting values from them
- A variable is **domain consistent** if there is no value of the domain of the node is reeled impossible by the  constraints

#### Constraint Network

- circular node for each variable
- Rectangle node for each constraint
- There is a domain of values associated with each variable node
- There is an arc from variable $$X$$ to each constraint that involves $$X$$

#### Arc Consistency

- An arc $$\langle X, r(X, \overline{Y})\rangle$$ if for each value $$x \in dom(x)$$, there is some value $$\overline{y} \in dom(\overline{Y})$$ such that $$r(x,\overline{y})$$ is satisfied
- A network is arc consistent if all its arcs are arc consistent
- What if the arc is not consistent?

  - All values of $$X$$ in $$dom(X)$$ for which there is no corresponding value in $$dom(\overline{Y})$$ can be deleted from $$dom(X)$$ to make the arc $$\langle X, r(X, \overline{Y})\rangle$$ 

##### Arc consistency algorithm

- the arcs can be considered in turn making each arc consistent
- When an arc has been made arc consistent, it needs to be revisited if the domain of one of the $$Y$$‘s is reduced.
- Three possible outcomes when all arcs are made arc consistent
  - One domain is empty $$\Longrightarrow$$ no solution
  - Each domain has a single value $$\Longrightarrow$$ unique solution.
  - Some domains have more than a single value.$$\Longrightarrow$$ there may or may not be a solution.



##### Arc consistency algorithm example

- Variables A,B,C,D,E represent the starting times of various activities

- Domains:

  $$D_A = \{1,2,3,4\}, D_B = \{1,2,3,4\}, D_C = \{1,2,3,4\}, D_D = \{1,2,3,4\}, D_E = \{1,2,3,4\}$$ 

- Constraints:

  $$(A>D)\and (D>E) \and (C\neq A) \and (C>E)\and(C\neq D) \and (B \geq A) \and (B\neq C)\and (C\neq D + 1)$$

![image-20211018153610838](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018153610838.png)

- boxes have been omitted for contraints.
  - Constraints in this only are binary





| Arc                    | Relation | Value(s) Removed |
| ---------------------- | -------- | ---------------- |
| $$\langle D, E \rangle$$ | $$D>E$$ | $$D=1$$      |
| $$\langle E,D \rangle$$ | $$D>E$$ | $$E=4$$      |
| $$\langle C,E \rangle$$ | $$C>E$$ | $$C=1$$      |
| $$\langle D,A \rangle$$ | $$A>D$$ | $$D=4$$      |
| $$\langle A,D \rangle$$ | $$A>D$$ | $$A=1,A=2$$  |
| $$\langle B,A \rangle$$ | $$B>A$$ | $$B=1,B=2$$  |
| $$\langle E,D \rangle$$ | $$D>E$$ | $$E=3$$      |

##### Arc Consistent Graph

![image-20211018154346808](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018154346808.png)

##### Finding solutions when AC finished

- If some domains have more than one element - search
- Split a domain, then recursively solve each half $$\Longrightarrow$$ domain splitting or case analysys
  - Idea is to split a problem into a number of disjoint cases and solve each case separately
  - The set of all solutions to the initial problem is the union of the solution to each case
- Often best to split in half
- No need to restart AC, just consider arcs that are possibly no longer consistent as a result of the split

![image-20211018154923257](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018154923257.png)

Splitting the domain of $$D$$ - there are two cases, $$D=2,3$$ there are constraint graphs correspond to the only two solutions

#### Hard and Soft Constraints

given a set of variables assign a value to each variable that either

- Satisfies a set of constraints - satisfiability problems
- Minimizes some cost function where each assignment of values to variables has some cost - optimization problems.

#### Problem Structure

- suppose that each subproblem has c out of n variables
- There are $$n/c$$ subproblems each of which takes at most $$d^c$$ to solve. Worst-case solution cost therefore, $$n/c \cdot d^c$$

### Tree-Structured CSPs

**Theorem** - if the constraint graph has no loops, the CSP can be solved in $$O(n\cdot d^2)$$ time.

#### Algorithm for Tree-Structured CSPs

- Choose a variable as a root, order variables from root to leaves such that every node’s parent precedes it in the ordering

- For j from n down to 2, remove inconsistent domain elements for $$\langle Parent(X_j), X_j\rangle$$

  At this point, CSP is directionally arc consistent, so no backtracking in 3, reverse order checks ensure deleted values do not endanger consistency of processed arcs - $$O(n\cdot d^2)$$ 

- For j from 1 to n, assign $$X_j$$ consistently with $$Parent(X_j)$$

#### Nearly Tree-Structured CSPs

- Conditioning - instantiate a variable, prune its neighbours’ domains - assign variable so remainder is a tree
- Cutset Conditioning - instantiate in all ways a set of variables such that the remaining constraint graph is a tree
- Cutset size $$c \Rightarrow$$ runtime $$O(d^c (n-c)d^2)$$ - very fast for small $$c$$



#### Variable elimination

- eliminate the variables one-by-one passing their constraints to their neighbours

**Algorithm**

- If there is only one variable return the intersection of the constraints that contain it
- Select a variable $$X$$ 
- Join the constraints in which $$X$$ appears forming constraint $$R_1$$
- Project $$R_1$$ onto its variables other than $$X$$ forming $$R_2$$
- Replace all of the contraints in which $$X$$ appears by $$R_2$$ 
- Recursively solve the simplified problem forming $$R_3$$
- Return $$R_1$$ joined with $$R_3$$



When there is a single variable remaining, if it has no values, the network was inconsitent

- The variables are eliminated according to some elimination ordering
- Different elimination orderings result in different size intermediate constraints

##### Variable elimination example:

![image-20211018161517912](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161517912.png)

- First of all, make the network arc-consitent

![image-20211018161543250](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161543250.png)

- Eliminating $$C$$ 

![image-20211018161601860](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161601860.png)

- Resulting network without $$C$$ 
  - There is now another relation showing the relation $$r_4$$

![image-20211018162205614](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018162205614.png)

## Local Search

### Iterative Improvement Algorithm

- Just the goal state we are looking for - goal state is the solution
- Local Search uses a single current state - not multiple paths and typically moved to neighbours of state
- Not systematic
  - Low memory useage
  - Can find reasonable solution in continuous spaces
- Useful for optimization problems, including CPSs – finding the best state4 according to some objective function

#### The State Space

- The set of complete configurations
- A goal is a particular configuration
  - The travelling salesman - the shortest route visiting each city exactly once
  - N-Queens problem
- Can use iterative imporvement
  - Keeps track of current state, trying to improve it
- Constant space typically

Consider all states on a surface of a landscape

- Height of any point - evaluation of that point
- Move around the landscape trying to find the highest peaks

#### Hill Climbing

- always try to improve state - reduce cost for example
- Each iteration move in direction of increasing value - or decreasing if cost
- No search tree, just keep current state and cost
- If several have equal choose randomly



**function** $$HillClimbing(problem)$$ **returns** solution state

​	**inputs** - problem

​	**state** - current - a node, next - a node

​	$$current\leftarrow MakeNode(InitialState[problem])$$ 

​	**loop do**

​		$$next \leftarrow$$ highest values neighbour if current

​		**if** $$Value[next]<Value[current]$$ 

​			**then** **return** current

​		$$current \leftarrow next$$

​	**end**



##### Problem of Hill Climbing

- Local maxima - local peak, lower than highest peak, the algorithm will halt witha  suboptimal solution
- Ridges - steep sides, top with gentle slope - search may oscillate from side to side, not making progress
- Plateaux  - flat area will conduct a random walk
- Plateaux - may be a local maximum - no uphill or a shoulder - possible to progress 
  - If this is reached, allow sideways moves to try and get off the shoulder
  - Limit the number of sideways moves - otherwise can conduct infinite random walk on plateaux

![image-20211026093426746](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026093426746.png)

### Greedy Descent - local search for CSPs

- Maintain an assignment of a value to each variable
- Repeat:
  - Select a variable to change
  - Select a new value for that variable
- Until a satisfying assignment is found



**Aim** - find an assignment with zero unsatisfied constraints

- Given an assignment of a value to each variable, a conflict is a violated constraint
- The goal is an assignment with zero conflicts
- Heuristic function to be minimized - as the number of conflicts

#### Greedy Descent Variants

- Find a variable-value pair that minimizes the number of conflict
- Select a variable that participates in  the most conflicts, select a value that minimizes the number of conflicts
- Select a variable that appears in any conflict, select a value what minimizes the number of conflicts
- Select a variable at random, select a value that minimizes the number of conflicts
- Select a variable and a value at random; accept the change if it does not increase the number of conflicts.

#### Complex Domains

- When domains are small or unordered, the neghbours of an assignment can sorrespond to choosing another value for one of the variables
- When the domains are large and ordered, the neighbours of an assignment are the adjacent value for one of the variables
- If the domains are continuous, gradient descent changes each variable proportionally to the gradient of the heuristic function in that direction:
  - The value of variable $$X_i$$ goes from $$v_i$$ to $$v_i - \eta \frac{\delta h}{\delta x_i}$$ where $$\eta$$ is the step size

### Randomized Algorithms

As well as downward steps, we can allow for - prevent getting stuck in local minima

- Random steps - move to a random neighbour
- Random restarts - reassign random values to all variables

1- dimensional Ordered Examppe

![image-20211026150648764](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026150648764.png)

- Greedy descent with random restart should find the optimal quickly, and random walk would not work well, since so many random steps needed to escape local minima **a**
- Random restart quickly gets stuck on a peak and does not work very well, but eandom walk and greedy descent can escape local minima - **b**

### Stochastic Local Search



- Greedy d3scent - move to lowest neighbour
- Random walk - taking some more random steps
- Random restart - reassigning values to all variables



#### Random Walk Variants

- When choosing the best variable-value pair randomly choose a raondom variable value pair
- When selecting a value:
  - Sometimes choose any variable that participated in the most conflicts
  - Sometimes choose any variable that participates in any conflict
  - Sometime choose any variable
- Sometimes choose the best value and sometimes a random value

#### Comparing Stochastic Algorithms

How can you compare when:

- One solves 30% of time very quickly but doesn’t halt for other 70%
- One solves 60% of the cases resonably quickly but doesn’t solve the rest
- One solves the problem in 100% of cases but slowly

Mean, median and mode runtime do not make much sense



![image-20211026152553166](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026152553166.png)

- Plot the runtime or number of steps, and the proportion of the runs that are solved within that runtime
- Which algorithm is best depends on how much time is available or how important it is to find a solution

### Simulated Annealing

- Pick at random and a new value at random
- If it is an approvement adopt it
- If it is not an improvement adopt it proabilistically depending on a temperature parameter $$T$$
  -  With current assignement $$n$$ and proposed assignment $$n’$$ we move to $$n’$$ with probability $$e^{(h(n’)-h(n)))/T}$$ 
- Temperature can be reduced over time

Probability of accepting a change:

![image-20211026153033427](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026153033427.png)

### Tabu Lists

- To prevent cycling, we maintain a tabu list of the $$k$$ last assignment
- Do not allow an assignment that is already in that list
- If k=1, we do not allow an assignment of the same value to the variable chosen
- We can implement it more efficiently thatn as a list of complete assignent - using a hashmap list, or including a list of steps
- Can be expensive



### Parallel search

A total assignment is called an individual

- **Idea** maintain a population of $$k$$ individuals instead of one
- At every state, update each individual in the population
- Whenever an individual is a solution, it can be reported
- Like $$k$$ restarts, but uses $$k$$ times the minimum number of steps.



### Beam Search

- Like a parallel search with $$k$$ individuals, but choose the best $$k$$ out of all of the neigbhours
- When $$k=1$$ it is greedy descent
- When $$k=\infty$$ it is BFS
- The value lets us limit space and  parallelism

### Stochastic Beam Search

- Like beam search, but probabilistically chooses the $$k$$ individulas as the next generation
- The probability that a neigbhbour is chosen is proportional to its heuristic value
- This maintain diversity amongst the individuals
- The heuristic value refelext the fitness of the individual
- Like asexual reproduction - each individual mutates and the fittest ones survive

### Genetic Algorithms

- Related to stochastic beam search
- Successor states obtained from two parents
- Starts with population of $$k$$ randomly generated individuals
- Each individual represented as a string over a finite alphabet
- Each individual in the population is evaluated by a fitness function
- Fitness function should return higher values for better states
- Fitnesss function determines probability for being chosen for reproduction
- Pairs of individuals chosen according to these probabilities - individuals below a certain point being discarded

- Fir each chosen pair, a random crossover point is chosen 
- Offspring generated by crossing over parent strings at chosen crossover point
- First child gets first part of string from parent and remainder from the second parent - vice versa for second child
- Parents: “12341234, 56785678” with crossover point at position three will give:
  - “12385678” and “5674123”

- Finally, each location in the newly  created children is subject to random mutation with a small probability

- For 8 queens, there is a small probability of taking a random queen and moving it to some random position

![image-20211026154535342](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026154535342.png)

- Only four individuals in this case. 
  - The probabilities determine the likely hood of choosing the individuals for reproduction
  - Populations would typically be hundreds of thousands of indivuduals



If two parents are quite different, crossover can produce state that is very different from both parents

- GAs take large steps early in the search, smaller steps as population converges
- Primary advantage is thea bility to crossover large blocks that have evolved independently to perform useful functions
- Choice of representation is fundamental - GAs work best if representation corresponds to meaningful components of solution
- Useful for optimisation problems

## Adversarial Search

- Consider an agent in a xompetitive multi-agent environment, where goals are in conflict
  - Gives rise to adversarial search
- Other agents, opponents, introduce uncertainty
- An adversarial search must deal with contingencies
- Complexity of game is high and time usually important - best guess based on expe4rience available
- Chess has a branching factor of around 35, and games are often 100 moves long 
- - Search tree has 35^100^ nodes
- Have to make the best move given the situation

### Uncertainty

May rise from:

- Opponents trying to make the best move for themselves
- Randomness  - dice
- Insufficient time to determine exact consequences

Makes this interesting because of the uncertainty, they are easy to represent, usually fully observable, and more like the real world

- Have to make some decision even if optimal decision is infeasible
- Inefficiency penalised severely, inefficient search takes longer, an inefficient chess program loses.

#### Perfect Decisions

Start considering perfect decision - even though they are not generally practical

- Utility function gives numeric value to outcome of game
- Many games fit this model - chess, go etc
- More abstract games need other approaches 

#### Formal View of Game

- Initial State - the board position and the player to move
- Set of operators - a successor function - defines the legal moves and resulting states
- A terminal test - determines when the game is over
- A utility function - gives a numeric value for terminal states

Can build a game tree based on initial state and operators.

**Note** - even for a simple game, the search tree is very complex 

##### Example

Consider a (zero sum) game with two players, Max and Win

Max moves first, then take turns, and points awarded at the end of the game

- For a normal search, Max could just find a sequence of actions to achieve a winning state
- Max must form a strategy that will win whatever Min does

- Strategy should include correct move for Max for each possible move for Min

##### Two ply game tree

Game is one move deep 

- Contains two half moves or ply
- Possible moves for Max at root are $$A_1, A_2, A_3$$ 
- Possible replies for $$A_1$$ for Min: $$A_{11},A_{12}, A_{13}$$



### Minimax

- Minimax gives an optimal strategy for the Maximising player
- Choose the move with the highest minimax value
- Definition - maximum value of a state is the utility fo(for max) of being in that state assuming both players play optimally from that state until end of the game
- Minimax value of terminal state = utility
- Max prefers maximum values, Min prefers minimal:

When: 

- $$n$$ is a terminal, $$MinimaxValue(n)=Utility(n)$$
- $$n$$ is a Max node, $$MinimaxValue(s) =max_{s\in Successors(n)}MinimaxValue(s)$$
- $$n$$ is a Min node, $$MinimaxValue(s) =min_{s\in Successors(n)}MinimaxValue(s)$$

#### Algorithm

- Obtains best possible playoff against best play

Algorithm:

- Generate complete game tree
- Use utility function to rate terminal states
- Use utility of termial states to give utility of nodes one level up
- Continue backing up tree until root
- Max should choose move that leads to the highest utility 
  - Maximises utility assuming the opponent wil play to minimise it





![image-20211026183129394](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026183129394.png)

- First rate terminals, then back up minimal values - Min’s move, then back up maximum values (Max’s move)
  - In this case A1 would be chosen

#### Properties of Minimax

- Complete - if finite
- Optimal if against optimal opponent
- Space $$O(bd)$$ 
- Time - $$O(b^d)$$ - very bad with real games
- Minimax requires complete search tree - not normally practical
- Forms a basis for more realistic algorithms



#### Multi Player Minimax

Can extend to multiple player games using vectors of utilities

Backed up value of n is the utility vector of whichever successor is best for the player choosing at n

#### Alpha Beta Pruning

- Complete search tree impractical - alternative is to prune branches that will not influence decision
- Idea - consider node $$n$$ that player might move to. If player has a better choice $$m$$ either at the parent of $$n$$ or further up the tree. Then n will never be reaches in actual play and it can be pruned
- As soon as we discover there is a better choice than $$n$$ - by looking at descendants, we prune it

![image-20211026183819620](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026183819620.png)

Considering $$n$$, but there is a better choice $$m$$, then the path to $$n$$ is pruned.

Minimax is a DFS and Alpha-beta pruning gets its name from the parametes backed up the path:

- $$\alpha =$$ value of best choice along the path for Max - highest value
- $$\beta =$$ value of best choice along the path for Min - lowest value

It updates $$\alpha,\beta$$ as it searches, prining as soon as value of current node is known to be worse than current $$\alpha,\beta$$ for Max or Min respectively

- Done by terminating the recursive call

##### Effectiveness

This is dependent on the order of examining successors

- **Solution** - try to examine best successors first
- If this is possible, alpha-beta looks at $$O(b^{d/2})$$ instead pf $$O(b^d)$$ - twice the lookahead
- For random order of successors, alpha-beta looks at $$O(b^{3d/4})$$ nodes
  - In practice, a simple ordering function can give significant advantage
- In chess, captures, threats, forward moves, then backward moves

#### Imperfect Decisions

- Alpha-Beta prunes much of the search tree, while minimax needs the complete tree
- Alpha-best still needs to search to the terminal states for some of the tree
- Where a move must be timely, this is impractical
- An alternative is to cut off the tree earlier
  - A Heuristic function to get the value for states
  - A cut off test to determine when to stop going down a tree

##### Evaluation Functions

An evaluation function gives an estimate of expected utility for a given position

- Cutting off trees turns nonterminal nodes into terminal leaves
- Evaluation function should:
  - Order terminal states as per utility function 
  - Approximate actual utility of state
- Uncertainty is unavoidable - incomplete tree

Mostly, evaluation functions calculate features of a state:

- For chess, number of pawns, rooks, knights, king safety
- These determine equivalence class of states - each class leads to a win , draw or loss with some probability
  - Can evaluate expected value of class
- Can combine features with a weighted linear function: $$w_{1}f_1 + w_{2} f_{2} +…+w_nf_n$$, where the $$w$$‘s are weights and the $$f$$’s features
  - Assume all are independent, otherwise a nonlinear function is needed

#### Cutting off Search

Simplest approach is to set a fixed depth - cutoff test succeeds at depth $$d$$ 

- More robust approach is to use iterative deepening - continue until out of time, then return best move found so far
- Both unreliable - due to appx in evaluation function 

##### Solution

- Only apply evaluation function to quiescent position - those whose value is unlikely to change in the near future
- Non quiescent position expanded until quiescent positions reached
- This extra search is called quiescent search
- Quiescent search restricted to certain types of move to quickly resolve uncertainties

**Horizon Problem**

- Faced with an unavoidable damaging move from opponent, a fixed depth search is fooled into viewing stalling moves as avoidance

- Singular extension search as a means of avoiding horizon problem
  - Singular extension - a move that is clearly better than all others
- Forward pruning - immediately prune some moves from a node with no further consideration
- Only in safe special cases
  - If two moves are symmetric or equivalent only consider one of them
  - nodes very deep in search tree

#### Games with Chance

Legal moves dependent on the roll of a dice - a complete game tree cannot be included

- Can include chance nodes
- Labelled with a result and probability
- Can calculate expected value taken over possible results of chance node

Generalise the minimax function to use expected values: $$ExpectIminimax(n)=$$

![image-20211026190557904](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026190557904.png)

- Considers all outcomes of chance node $$O(b^mn^m)$$ where $$n$$ is the number of distinct outcomes
- Can prune chance node without looking at children if we put bounds on the utility function - a chance the node will be an average, we know the bounds within which it lies



### Monte Carlo Tree Search 

- Estimate the value of state from the average utility over simulation - playouts, of complete games starting from the state
  - For some games (go) can learn from self play using neural networks
  - For some games can use specific heuristic - capture moves in chess

Pure MCTS - do $$N$$ simulations starting from current state, and track which moves from current position has highest win percentage

- as $$N$$ increases, this converges to optimal play
- Typically to expensive, therefore a selection policy is needed to focus search on important parts of the game tree. Need to balance:
  - Exploration of states having few playouts
  - Exploitation of states having done will in past playouts to increase accuracy of estimate

#### MCTS steps

- Maintains a search tree and growing it on each iteration using:
  - Selection - starting at root,, choose a move (using a selection policy), leading to successor, and repeat moving to a leaf
  - Expansion - grow tree by generating new child of selected node
  - Simulation - perform playout from the newly generated child node - determine outcome but do not record these moves in the tree
  - Back propagation - use the result of playout to update search tree going up to root

- Repeat this for a fixed number of iterations, or until out of time, then return move with the highest number of playouts - 65/100 is better than 2/3 - less uncertainty with more tries

#### UCTS - upper confidence bounds applied to trees

An effective selection policy based on an upper confidence bound formula called UBC1:

![image-20211026191738681](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026191738681.png)

Where $$U(n)$$ is total utility of playouts through $$n$$, $$N(n)$$ is number of playouts through $$n$$, $$Parent(n)$$ is $$n$$‘s parent tree. and $$C$$ is a constant which balances exploitation and exploration

- The time to compute a playour is linear in the depth of the tree since only once move taken at each point



## Planning with Certainty

### Knowledge bases

- Knowledge base - Domain specific content
  - A database of facts/beliefs
  - A set of senences in a formal knowledge representation language
  - Implementation vould be anything - linked lists, arrays SQL db
- Inference enginge - domain independent algorithm
  - A mechanism for reasoning about those beleifs



#### Declarative approach to building an agent or system

- Agent can ask itself what to do, answers should follow from the KB through inference
- TELL and ASK are standard names for adding sentences and querying the KB
- Result of ASK must follow from the previous sells.

#### Knowledge based agent

- Can reason using inference and their knowledge
  - Can accept new tasks in the form of goals
  - Can adapt to environmental change by updating knowledge
  - Are able to infer unseen properties of the world from perceptions
- Can often find better solutions than timple search
- More flexibile with respect to
  - Adopting new goals
  - Partially observable envirnomants
  - Dynamic environments



##### 3 Levels

- Knowledge level - what is know, regardless of implementation - allows us to work at abstract level using ASK and TELL - declarative approach
- Logical level- knowledge encoded in formal sentences
- Implementation level - data structures in KB and algorithms that manipulate them



##### Simple knowledge based agent

- represent states, actions
- Incorporate  new percepts
- Update internal representations of the world

- Deduce hidden properties of the world
- Deduce appropriate actions

![image-20211101111306223](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101111306223.png)

- Knowledge base may contain initial background knowledge
- Each iteration, TELL knowledge base of perceptions, ASK what actions to perform
- Tell and ASK refer to KB - internal
- Representation details hidden by MakePerceptSentence and MakeActionQuery - lets us work at knowledge level
- Inference details - logical level, hidden in Tell, Ask

#### Wumpus world

##### Environment - percepts

- Squares adjacent are smelly
- Squares adjacent to pits are breezy
- Glitter iff gold is in the same square
- Bump if walk into a wall
- Scream if Wumpus is killed

##### Environment actions

- shooting kills if facing
- Shooting uses only arrow
- Grabbing picks up the gold if in the same square
- Releasing drops the gold in the same square
- Climb leaves the cave only from the start square

1000 points for leaving cave, 100000 penalty for death, 1 point penalty per action

##### Notes

- No perception of current location
- Location of Wumpus and gold random uniform except square
- Any square except start may be a pit, probability 0.2
- In most environments agent can get gold, sometimes agent must choose between chance to get gold or leaving empty handed
- Sometimes no solution - gold in pit or surrounded

##### Properties

- Deterministic
- Not fully observable
- World is static
- Discrete

##### Cautiousness

- Only moves if it is OK
- Using the combination of different smells and senses, can calculate where the different threats are.
- After each action, the knowledge base is told. - can backtrack again

Only were able to do this due to the inferences given from the perceotions

- Combined knowledge at different times
- Used lack of perception
- Relied on persistence of knowledge

Are possible scenarios where it is not possible to know exactly where it is

- If things are uniformly distributes, can take a probabilistic decision here

- Could go in the start state, shoot the arrow, use inference from that - **coercion**

Given agent has fairly complex reasoning, beyond capabilities of most animals

- Done through logical reasoning

### Reasoning with planning on inference

- logics are formal languages for representing information such that conclusions can be drawn
- Syntax defines the sentences in the language
- Semantics define the ‘meaning’ of sentences - the truth of a sentence in a world

#### Entailments

$$KB = \alpha$$ 

- Knowledge base KB entails sentence $$\alpha$$ iss $$\alpha$$ is true in all worlds where KB is true
- If the KB is ‘kilo is black’ and ‘kilo is a dog’ then this entails ‘either kilo is a dog or kilo is black’
- This is important since it provides a strong way of showing that if certain propositions are true, then some other porisition must be true - finding the wumpus

##### Levels

- Representation level - sentences entail sentences
- World - facts folllow facts
- Semantics give a mapping of sentences to facts
- Logical inference generates sentences that are entailed by existing sentences and and should ensure relationship mirrored in real world
- Inference procedure that generates only entailed sentences is sound or truth preserving
- By considering the semantics of a language we can extract the proof theory of the language - what reasoning steps are sound

#### Inference

- $$KB \vdash_i \alpha = $$ sentence $$\alpha$$ can be derived from KB by procedure $$i$$
  - **Soundness** $$i$$ is sound if whenever $$KB \vdash_i \alpha$$ it is also true that $$KB \models \alpha$$
  - Completeness $$i$$ is complete if whenever $$KB \models \alpha$$ it is also true that $$KB \vdash_i \alpha$$

- We need logic which is expressive enough to say almost anything of interest and for which there exists a sound and complete inference procedure - the procedure will answer any question whose answer follows from what is know by the KB

### Propositional logic  

#### Syntax

- The proposition symbols $$P_1,P_2$$ are sentences
- Negation - if $$S$$ is a sentence $$¬S$$
- Conjunction - if $$S_1, S_2$$ is a sentence, $$S_1\and S_2$$ is a sentence
- Disjunction - if $$S_1, S_2$$ is a sentence, $$S_1\or S_2$$ is a sentence
- Implication - if $$S_1, S_2$$ is a sentence, $$S_1\implies S_2$$ is a sentence
- Equivalence - if $$S_1, S_2$$ is a sentence, $$S_1\Leftrightarrow S_2$$ is a sentence

#### Semantics

- Each model specifies true/false for each proposition symbol
- Rules for evaluating with respect to model $$m$$:

![image-20211101115058572](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101115058572.png)

#### Inference by enumeration

- Let $$\alpha = A \or B$$ and $$KB = (A\or C)\and (B\or ¬C)$$
- - check all possible models
  - is alpha true whenever knowledge base is true
  - Create a truth table and check

#### Wumpus reasoning

- percepts as sentences put in KB
- Constraints sentences in the form $$S_{2,1}$$ representing stench in $$[2,1]$$ 
- Agent given som eknowledge of the world
- If stench in $$[2,1]$$, then wumpus is ina neigbhouring square:
  - $$S_{2,1} \implies W_{3,1} \or W_{2,2} \or W_{1,1} \or W_{2,1}$$ 
- Conclusions can then be drawn with standard inference
- Have some action rules

#### Problems

- Too many propositions 
  - 64 rules (16 suquares x 4 orienteations)
- If the world is larger, the problem gets worse, need thousands of rules for a competent agents
- Writing rules is a problem, as is space, inference also slows
- World may change - need a different symbol for each time step

Planning provides a solution

### Search vs Planning

- Consider a task, get milk, bananas, cordless drill - home with nothing
- Standard search could fail miserably

![image-20211101120342709](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101120342709.png)

- For search we must specify initial state, operations and possible a heuristic function
- Branching factor may be huge depending on how operators are defined
- Length may be dozens
- Too many actions and states to consider
- Agent is forced to construct full sequence of actions
- Must decide what to do in initial state first



After the fact heuristic goal test is inadequate

- heuristics can only choose which state is closer to goal - cannot eliminate actions from considerations
- Evaluation function ranks these guesses, but all must be considrered
- need to work on appropriate part of sequence
- Principal difficulty - unconstrained branching - hard to apply heuristics

#### Planning systems

- open up action, state and goal representations to allow selection - represent in first order logic
  - States and goals = set of setnences
  - Actions = description of preconditions and effects
- Allows planner to make direct connections between states and actions

- Divide and conquer by subgoaling
- Planner can consider several smaller subproblems and combine
- Works as there is little interaction between subplans - otherwise cost of combining solutions outweighs the gain
- Relax requirement for sequential constructions of solutions
- Allows planner to add  actions where needed, so can make ‘obvious’ or important decisions first, reducing branching factor

- No connection between order of planning and execution
- This can only be done because of logic - at (supermarket) represents a class of states, search requires a complete state description and not possible
- In real world, planning tends to do better than search

#### Comparing the two

|         | Search                | Planning                       |
| ------- | --------------------- | ------------------------------ |
| States  | Data structures       | Logical sentences              |
| Actions | code                  | preconditions and outcomes     |
| Goal    | code                  | logical sentence - conjunction |
| Plan    | sequence from $$S_0$$ | constraints on actions         |

#### Simple planning agent

![image-20211101121258861](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101121258861.png)

**Algorithm**

- update knowledge base
- If not already executing a plan, generate a goal and construct a plan to achieve it
- Agent must be able to cope if the goal is infeasible or achieved - set action to NoOp
- Once agent has a plan it will execute to completion
- Minimal interaction with the environment - perceive to determine initial state, but then just execute plan - no relevance checks

### Partial order planning

#### Situation Calculus

- way of describng change in first order logic
- world viewed as a sequence of situations - snapshots of the state of the world
- Situations generated from previous situations by actions

- Functions and predicates that change - called fluents with time given a situation argument. Those that do not change are called eternal or atemporal
- change represented by function Result(action, situation) which denotes the result of performing action in situation

##### Axioms

- Possibility axioms - describes when it is possible to execute an action - Precondition $$\implies$$ POSS(a,s)
- Effect axioms - changes due to that action - POSS(a,x) $$\implies$$ changes

##### Planning in Situation Calculus

- can be seen as logical inference problem using siuation calculus
- Logical sentences to describe initial state, goal and operators
- Initial state - sentence about a situation $$S_0$$
- Goal state - logical query for suitable situations

![ ](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101122831378.png)

- Goal state is $$\exists seq, At(G,[1,1], Result[seq, S_0])$$ 
- From first possibility axiom - $$Poss(Go(x,y), s)$$ 
- From first effect Axiom - $$At(Agent, y, Result(Go(x,y),s))$$ 
- Agent cannot grab gold
  - Truth value of $$At(Agent, y, Result(Go(x,y),s))$$ is unknown
  - Nothing in the KB says the location of the gold remains unchanged

#### Frame Axioms

- Need to deescribe how the world stays the same
- Non changes due to an action
- If there are $$F$$ fluents and $$A$$ actions it requires $$O(AF)$$ fram axioms
- the frame problem is a long standing problem in AI
  - Representational - proliferation of frame axioms 
    - Representational problem is largely solves
  - Inferential - having to carry properties through inference steps even if remaining unchanged
    - Avoided by planning - we do not address it for inference systems

#### Successor state axioms

- Solve the representational frame action
- Each axiom is about a predicate, not an action
- General form - $$P$$ true afterwards $$\equiv$$ (an action made $$P$$ true $$\or$$ $$P$$ already true and no action made $$P$$ false)
- Need a successor state axiom for each predicate that can change over time
- Axiom must list all ways the predicate can become true or false

For example:

$$Poss(a,s) \implies At(o,y,Result(a,s)) \Longleftrightarrow (a = Go(x,y)\and(o=Agent \or Holding(o,s)) $$ $$\or (At(o,y,s))\and ¬(\exists z:y\neq z \and a = Go(y,z)\and (o=Agent\or Holding(o,s))))$$ - if the action is for the agent to move, then objects other than the Agent, that are not held by the agent remain where they are

#### Restricted language

- Theoretically this is all that is required - unpractical - time, space, semi-decidability
- To make practical we use a restricted language
- Reduces the possible solutions to search through
- Actions represented n a restricted language - allows creation of efficient planning algorithms
- Need a language and a planning algorithm for that

#### STRIPS

- Most planners use the STRIPS language or extensions
- NB use a planner rather than a general purpose theorem prover
- Stanford Research Institute Problem Solver - a planner not a problem solver
- States = conjunctions of function -free ground literals - predicates applied to constants
- State descriptions may be incomplete
- Closed-world assumption - most planners assume that if state description does not mention a positive literal, can assume to be false (can be dangerous)
- Goals = conjunctions of literals - may contain variables
- Planner - asks for a sequence of actions that make the goal true if executed

##### Operators

- comprise three components
  - Action
  - Precondition
  - Effect
- Preconditions are conjunctions of positive literals
- Effects are conjunctions of function free literals
- No explicit situation information - preconditions implicitly refer to the situation immediately before action, and the effects to the result of the action
- STRIPS divided effects into add list and delete list according to positive or negative



- Can be represented graphically:

  - OP (Action:Buy(X))
  - Precond: At(p) $$\and$$ Sells(p,x)
  - Effect: Have(x)

  

![image-20211101132720556](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101132720556.png)

An operator with variablres in an operator scehma - a family of actions requiring instantiation

An operator is applicable in state $$s$$ if we can instantiate each variable to that the precondition is true in $$s$$



#### State space vs plan space

- standard seach - node = concrete world state

- Planning search -  node = partial plan

  Seach through the space of plans

- An open preconidition is a precondition not yet fulfilled

- Operates on partial plans 

  - Add a link from an existitng action to an open condition
  - Add a step to fufill an open condition
  - Order one step with respect to another

- Gradually move from incomplete/vague plans to complete correct plans





#### Planning Terminology

- progression - start from initial siutation and search forward to the goal - large branching factor and search space
- Regression - search back from the goal to the initial situation - reduces branching factor, but complicated in conjunctions in goals and ensuring all conjunctions achieved
- Partial plan - an incomplete plan, with some steps not instantiated
- Partial order planning - some steps are ordered with respect to others

- Total order - all steps ordered

#### Partial ordered plans

- Principal of least commitment - leves choices as long as possible
- Comprised of
  - Set of steps - corresponding to operators
  - Set of ordering constraints on steps $$S_i \prec S_j$$
  - Set pf variable bindings
  - Set of casual links $$S_i\rightarrow^{c} S_j$$ - $$S_i$$ achieves c for $$S_j$$
  - Once all variables are bound - fully instantiated plan
  - A plan is complete iff every precondition is achieved
  - A precondition is achieved iff it is the effect of an earlier step and no possibility intervening step undoes it
  - A plan is consistent iff there are no contradiction in ordering or binding constraints



A problem defined by a partial plan containing just start and finish

- Initial state is the effect of start
- Goal is the prcondition of finish
- Ordering constraints added as arrows between actions

##### OVERVIEW

- POP
- Regression planner to search through plan space
- Each iter add a step to pachieve preconditions backtrack if inconsistent
- Only add steps to achieve causal links - without breaking other links - links are pprotected
- POP is sound, complete and systematic

###### Initial state

- Defined by operator
  - A special step with no preconditions, but has the effects which describe the start state
    - Op(Action, Effect)

###### Goal state

- Defined by operator: 
  - Op(Action : Finish, precond:) - the state of the world we want to achieve, no effect

###### Actions

Operators describes in strips - they have a name, an action, a precondition and an effect.

The variables need to be bound to constants for he plan to be fully instantiated and executed



#### Planning Algorithm

- start with a minimal partial plan
- Each iteration find a step to achieve the precondition  $$c$$ of $$S_{need}$$ 
- Do this by choosing an operator to achieve the precondition
- Record casual link to the newly achieve precondition
- Resolve any threats to causal links
- If fail to find operator or resolve threat to causal link then backtrack

![image-20211101134832299](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101134832299.png)

![image-20211101172101457](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101172101457.png)

![image-20211101172158147](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101172158147.png)

#### Clobbering

- A threat, or clobber is a potentially intervening step that destroys the condition achieved by a causal link

- Causal links are protected
- Protect them, by ensuring that threats - steps which might delete the link are ordered before or after

#### Block world example

Start state:

- On(c,a) On(A,table), Clear(b), On(B,Table), Clear(C)condi

- Now must find a step with open precondition:
  - Only the finish step has this. Either need On(A,B) or On(B,C)

- Pick On(B,C)
  - Clear(b), N(B,z), Clear(C)
  - First establishes a single causal link - On(B,C)
- Clear(c)
  - Establish causal link from start step - clear(c)
  - Assign table $$z$$ to the table
  - Clear(b) is also 

- On(A,B)
  - Cant be achieved from exisitng
  - PutOn(A,B) - this clobbers clear(b)
  - Adding an ordering link, keep iterating and backtracking are all fufilled

Finish:

- On(A,B), On(B, C)

![image-20211101173340922](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101173340922.png)

**Each iteration represents a single causal link **



### Problems for STRIPS and POP

#### hierarchical plans

- Want to specify plans at different levels of detail
- have several levels before reaching executeable action - several iterations of solution
- Makes computation manageable and resulting plan understandable
- Allows human specification of abstract and partial plans to guide the planner
- Often want to give the planner guidance - especially on safety critical applications

#### Complex conditions

- although oeprators contain variables, there is no quantification
- STRIPS use of variables is limited - cannot express that Pickup(carrierbag) causes all to be lifte
  - operators are unconditional - cannot express an action of having different effects according to conditions
  - STRIPS cannot express that Pickup(carrierbag) pics up objects if bag not overloaded otherwise dumps them on the floor

#### Time

- In situational calculus time is discreet and action occur instantaneously
- Need to represent thata actions take time, may only be applicable at certain times, and the goal may have a deadline

#### Resources

- real problems have limited resources - money, time, quantitiy, machiner
- Actions have a cost - need to represent cst
- Action descriptions need to represent the resource requirements of performing the action

- planning must handle constraints on the resources

#### Hierarchical decomposition

- POP allows production of solutions at a high level
- Such solutions are removed from an agents effectors
- In order to execute a low level solution is needed
- Length of low level solution means that the space of plans is sufficieantly large for standard POP to struggle



Need to extend STRIPS To include non primitive operators and modify planner to replace nonprimitives with decomposition

- Solution is to intriduce abstract operators that can be decomposed into steps to implement them
- Decompositions are predetermined and stores in a library of plans - works best when ther are several possible decompositions

#### Extending the language

A  plan $$p$$ correctly implements a non primitive operator o if it is a complete and consistent plan for achieving the effects of the  o given the preconditions of o 

- p must be consistent
- Each effect of o must be asserted by a step of p 
- each precondition of steps in p must be achieved by a previous step or be a precondition of o

Guarantees that a nonprimitive operator can be replaces by its decomposition in the plan

- Still need to check threats when introducing new steps from the decomposition

#### Modifying the planner

- Every iteration - try too add a step and a resolve the on primitive
- Solution check - Must check that all operators are primitive

![image-20211102092301266](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102092301266.png)





- Steps: add steps of the method to the plan, remove $$S{nonprim}$$
- Bindings -   add bindings of the method to the plan, fail and back track if there is a contradiction
- orderings - following the least commitment - replace $$S_a \prec S_{nonprim}$$ by ordering $$S_a$$ before latest steps of the method - replace $$S_{nonprim} \prec S_z$$ by ordering $$S_z$$ after eatlist steps of the method. Then resolve threats - could invlove adding more ordering constraints
- Links - replace links to the non primitive with links to the steps of the method thata achieve the precondition

#### Broadening Operator descreiptions

- make operators more expressive to widen the application
- Conditional effect - avoids premature commitment - have effect condition
- Conditional effects have the form - EFFECT:/…$$\wedge ¬Clear(y)$$ when $$y\neq Table$$ 
- In SelectSubgoal must consider preconditions of conditional effects if the effec supplies a protected causal link
- In ResolveThreats any steps having effect $$¬c$$ $$when$$ $$p$$  is a possible threat to link: $$S_i \rightarrow^c S_k$$ - resolve threat by ensuring that $$p$$ is not true - confrontation
- Allow negated goals - ability to call ChooseOperator with goal not p
- Must treat initial state different - avoid representing all conditions - consider not p matched by explicit effect or initial state if it does not contain p
- Disjunctive preconditions - allow SelectSubgoal to make a nondeterministic choice between disjuncts- use principle of least commitment
- Disjunctive effects - introduces nondeterminism - may be able to address with coercion - shooting the Wumpus for example 
- Universally quentified preconditions - instead of Clear(b), we can use $$\forall$$ for example
- Allow universally quantified effects
- Restricted form of first order logic - worl is finite, static and objects have types. The initial state must give all objects a type
- Preconditions and effects of form: $$\forall x \cdot T(x) \implies C(x)$$ - $$T$$ is type, and $$C$$ is condition
- since the world is finite, static and types, can always expand universal quantification into a conjunction

#### Resource constraints

- Most problems have resources, and limit on how much can be consumed
- Introduce numeric values - measures 
- In situation calculus have $$FuelLevel(S_n)$$ but came make situation implicit - FuelLevel. Called measure fluent
- Some measure the planner has no control over
- Others are resources such as FuelLevel that can be produced and consumed 

- Introduce inequality test in operator preconditions
- Allow assignments of measures in the operators effects

- General idea - plan for scarce resources first, delaying choice of causal links where possible 
- Pick the steps of the plan first, then do a rough check to see if resource requirements are satisfiable
- If so, continue by resolving threats
- Allows us to check for failure without a finished plan

#### Temporal contraints

- Time can generally be treated as s resource
- Initial state must specify that time
- For each operator, specify how much time is takes in its effects
- Two main differences
  - Parallel operators cost the maximum of the respective times, not the sum
  - Time never goes backwards - if a deadline and a partial plan that goes over the deadline, there is no point in continuing with the plan - more cannot be produces

### Real World

- so far assumed that the world is observable and deterministic
- Also assumed that action descriptions are correct and complete
- Real world is not like this
- Often have to deal with incomplete and incorrect information

#### Fixing a flat tire:

- there is a flat tire, and a spare tire which is not flat

Things can go wrong here:

- we do not know whether tire 1 is intact
- Disjunctive effects - inflating the tire can either inflate, or the pump can break, or the tire can burst
- Incorrect information is not correct - the spare tire is flat,  missing/ incorrect postconditions in the operators
- Qualification problem - never finish listing all preconditions and possible outcomes of actions

![image-20211102094446961](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102094446961.png)

#### Solutions

- Conditional planning
  - Plan to obtain information - sensing actions
  - Subplan for each contingency
  - Expensive, as produces many unlikely cases
- Monitoring/replanning
  - Assume notmal states and outcomes
  - Check the progress during execution replan if necessary
  - Unanticipated outcomes may lead to failiure
- Typical a combination is needed

##### Conditional planning

- Execution - check p against KB, execute then or else
- Conditional planning - like POP but, if an open condition can be established by an observation action
  - Add the action to the plan
  - Complete the plan for each possible observation outcome
  - Insert a conditional step with these subplans

The key difference is that steps have a context, at execution the agent must know the state of the condition.

- To ensure the plan is executable, insert actions to find information
- But sensing actions may have other effects - checking tire by putting it in water makes it wet

###### Example

- Initial state 

![image-20211102095502730](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095502730.png)

- Teo open conditions to stisfy
- On(x) satisfied by link from start tteps with $$\{x/Tire1\}$$
- Inflated(x) satisfied by adding inflate(Tire1) which has preconditions Flat(Tire1)and intact(Tire1) 

![image-20211102095628260](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095628260.png)

- The plan to inflate Tire1 will on work if Tire12 is intact
- Add sensing action Che3dck(Tire1)
- Context(Intact(Tire1)) added to the finish step, since it assumes Tire1 is intact
  - Alls teps are annotated with the context that Tire1 is intact
  - There is another outcome where Tire 1 is not intact

![image-20211102095723669](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095723669.png)

- Need to be able to cope with Tire 1 not being intact 
  - Add a second step to the contact

![image-20211102095821355](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095821355.png)

- Finally satisfy On(Spare) by adding Remove(Tire1) and PutOn(Spare)
- make conditional link froe check(Tire 1) to remove(Tire1) threatening causal link protecting On(Tire1) in the first step

![image-20211102095944557](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095944557.png)

Here we needed to add finish steps for each possible observation

##### Parameterised Plans

- A sensing action may have any number of outcomes - checking the colour of an object
- Such sensing actions can be used in paratmererised plans, where exact actions are now known until runtime

![image-20211102133507464](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102133507464.png)

- could have just pained chair and table pink regardless of exisiting
- A maintainence goal allows certain facts to be preserved
- Can add a causal link from statr to finihs step protecting the table’s coour
- Can also include loops While9Known(#‘Uneven)’)
- Plans start to look like program
  - Planner starts to look like an automatic programming tool

##### Monitoring 

- Plan monitoring 
  - Effects of an action not being as predicted
  - Failure = preconditions of remaining plan not met
  - Preconditions = causal links at current time
  - Check current precondition with perceived state

- Action monitoring
  - Failure - preconditions of next action not met
- in both cases, need to replan
- Precondition of remaining plan obtains from causal links starting at or before step and ending after it

![image-20211102133821872](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102133821872.png)

- suppose ordering is going to supermarket is going next, then check preconditions, this would be action monitoring
- For plan monitoring, draw a line from the plan that has been executed, and each of the causal links which cross this line correspond to the preconditions of the remaining plan

#### Kinds of plan failiure

- Bounded indeterminancy - unexpected effects of actions can be enumeration - use conditional planning to deal with type of indeterminacy
  - Picking up a bag - either works or it doesnt
- Unbounded indeterminacy - set of all possible outcomes too large to enumerate - can plan for at most a limited number of contingencies, and must replan when things go wrong

#### Replanning

- Simple - start from scratch - inefficient, and may get stuck in a cycle of re-planning and failing
- Better - plan to get back on track - re-join original, by reconnecting to best continuation



This generates a loop until behaviour

- Consider painting a chair, where paint leaves unfinished areas - will loop until done, but loop is implicit
- Ideally introduce learning, so that the agent does not loop forever 

![image-20211102134740228](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102134740228.png)

- In the above example, on failiure

#### Alternatives to replanning

- Coercion - performing an action to force world into a particular state to reduce uncertainty
- Abstraction - ignore details of problem that may be unknown
- Aggregation - treat a large number of objects which have individual uncertainty as one aggregate predicate object

#### Reactive planning

- Alternative to traditional
- Abandon domain - independed planning and use some domain spceific knowledge
- Agen has procedural knowledge - a library of partial plans representing a collection of behaviours
- When the agent needs a plan, select the best from library - not from scratch, no plan in lib = failiure
- Many variations from networks of competences with spreading activation levels (neural networks) to explicit library of plans

## Representing Knowledge

- Intelligent behaviour in humans is conditioned on knowledge
  - What we do based on what we know
  - Humans ignore what we know to do things they shouln’d
  - Central to our behaviour
- Thinking is the process:
  - Retreiving what is relevant in our knowledge store to our present situation
  - Reasoning with the knowledge to make decisions
- Central to achieving AI
  - Achieving intelligent behaviour through computational means
- We focus on representing what we know with symbolic structures and methods for reasoning with those structures that are computationally tractable

### What is knowledge

- A relation defined by the propositional attirtude between the knower and a proposition
  - J knows that AL was assasinated
  - J hopes that M will be at the party
  - J believes …
    - There is no commitment to whether the proposition is true or false
- What matters is whether the proposition is true or false
  - Defines the state of the world, according to an agent

### Reasoning

- Explictly representing all propositions believed to be true is difficult
- Reasoning bridges the gap between represented and what believed
  - Let KB be a set of propositions believed to be true, $$\alpha$$ be a propositionnot in KB
  - THen $$\alpha$$ is said to be logically entailed by KB, $$KB\models \alpha$$ if we believe $$\alpha$$ to be implicitly true given the propositions in KB
- Knowledge representation languages need to have a well defined notion of entailment
  - What does it mean for a P to be true or false,
  - What else can be decidded is true or false based on that knowledge
- The expressivelness of a representationlanguage has an impace on the computational complexity
  - Logically complete - compute all propositions entailed by the KB
  - Logically sound – guarantee that any proposition believed to be true as a result of reasoning is actually true

### Knowledge Representation Hypothesis

- Any mechanically emobdied intelligent process will be comprised of structural ingredients that
  - We as external observers naturally take to represent a propositional account knowledge that the overall process exhibits
  - Independent of such external semantic attribution, play a formal but causal an essential role in engendering the behaviour that manifests tha knowledge
- SImply , a process or agent
  - Contains a collection of propositions which it believes to be true
  - Reasons with the propositions during its operation

- Why build such systems
  - Add new tasks that depend on previous knowledge
  - Extend existing behavoiur by adding new benifits
  - Debug fault behaviour by locating errneous beliefs
  - Concicely exmplain and justify the behaviour of the system

### Propisitions

- Natural to represent knowledge as logical formula rather than a table
  - Easy to check for correctness
  - Incrementally add to formulae
  - Can extend infintely many variables and domains
- For efficient reasoning, we can explit the boolean nature of such 
  - A propisition is a statement that is true or false, which can be naturally be represented as a logical formulae 

### Semantics

- When creating KB, we must choose vars that will be used to build propositions
- These should have meaning to the KB designedr
- Give the system knowledge about the domain, and make enquiries
  - Kwoledge takes form of a definite clause. $$h\leftarrow b$$ which consistes of two parts.
  - $$h$$ - the head is a variable that can be determined of $$b$$ being true
  - $$b$$ - the body is a logical formulae that can evaluate to true or false
- System does not understand the meaning of the synbold
- User must interpret the semantic meaning of the symbols

#### View of Semantics

**User view**

- Define the taks domain - intended interpretation
- A variable must be associated with each propositionthe user wants to represent
- Tell the syste clauses that are true in the intended interpretation known as aximatizing the domain
- If $$KB\models \alpha$$, then $$\alpha$$ must be true in the intended interpretation
- users interpret the system’s responses using the intended interpretation of the symbols

**Sytem’s view**

-  The system does not have access to the intended interpretation
- It is only aware of the knowledge base
- The system can determine if a particular formula is a logical consequence of KB, but does not understand what that formula really means

### Expert systems

- An expert system is a computer program that represents and reasons with knowledge of some specialist subject with a view of solving problems or giving advice
  - Simulates human reasoning
  - Performs reasoning over a representation of human knowledge
  - Solves problems with heuristic or approximate methods
  - Knowledge + Inference = Expert System
- Who is an expert
  - Has knowledge focussed on a specific domain
  - Capable of solving problems
  - Capable of explaining how they solve problems

#### MYCIN System

- one of the earliest expert systems
- Provides advice to a physicican on a selection of antibiotics treating blood infections 
  - Generates diagnostic hyptheses and weights them based on evidence
  - Make therapy recommendations
- Problem
  - Blood infections therapy
    - Drugs to kill or arrest groth of bacteria
    - No single drug effective against all
  - Therapy process
    - identiy organism invloved
    - Choose the best combination of drugs

![](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109112101607.png)

##### Knowledge representation

KB contains a number of rules

- The rule tally states how certain the conclusion is, given that the conditions are satisfied
- The certainty associated with a conclusion is a function of the combined certainties of the conditions and the rule tally

Also in KB are:

- Lists such as a list of all known organisms
- Knowledge tables containting clinical parametes and the values the take
- A classification of clinical parametes according the context in which they apply

##### Patient data

- stored in a tree - the root will be a patient ID

![image-20211109112450338](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109112450338.png)

- this then tracks details about the patient



##### Control structure

- consulation is a search through a tree of goals
  - Child nodes in the tree are sub goals that must be achieved to achieve the goal represented by the parent
- MCYIN top level goal specified as the following
  - If there is an organism which requires therapy, and consideratio has been given to any other organism requiring therapy
  - Then compile a lsit of possible therapies and determine the best one in the list
  - The action part of this rule is the root node of the tree of goals
- Leaves of the tree are facts such as lab data

- Each sub goal is a generalised form of the oroginal
  - Oif the goal is to prove organism is ecoli - sub goal is to determine the identity of the organism
- Every rule relevant to teh goal is used unless one of them succeeds with certainsty
- If the current sub goal is a leaf node, the user is asked for data

##### AND/OR Tree

Any or all of these could be true

- Disgnosis is an OR node
- Arc is an and - referring that both children must be true for it to be true.

![image-20211109112816383](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109112816383.png)

##### MYCIN Therapy rules

- The number is the probaility that an organism of class pseduomonas will be sensitive to thedrug
- Choose drug that maximises sensitivity, minimises number of drugs administered and contr-ndications of the grug

![image-20211109113027630](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109113027630.png)

##### Notes

- Never actually used in hostpital wards - tests show that MYCIN performed as well as Stanford experts
- Knowledge base is incomplete
  - Appx 400 rules
  - Does not cover much of the domain of infections diseases
- Running it would require more computing power that what hospitals could afford
- UI design was not applealing to doctors

### Rules as knowledge

- seen how infor stored as propositions
- Tend to associated intellgent behaviour with regularites in behaviour
  - Rational people act consistently
- Prediction rules are a formalistm that has been used in automata theory, formal grammars and the design of programming languages
  - First used in psychological modelling in early 1970s
- In expert systems literature, referred to as condition-action rules or situation-action rules

#### Canonical systems

Formal systems based on:

- An alphabet A for making string
- Some string that are taken as axioms
- A set of productions of the form:
  - $$\alpha_1 \$$_1,…,\alpha_m\$$_m\implies \beta_1\$$_1'…,\beta_n\$$_n'$$
  - Grammar rules for manipulating strings of symbols
  - Also known as rewrite rules - regular expressions

- A = a,b,c
- Axioms = $$a,b,c,aa,bb,cc$$
- Productions:
  - $$\$$\rightarrow a\$$ a$$
  - $$\$$\rightarrow b\$$ b$$ 
  - $$\$$\rightarrow c\$$ c$$
- Generates all the palindromes based on the alphabet through applications of productions



#### Knowledge representaion

Alphabet replaced by Vocab that consists of

- A set O of names of objects in the domains
- A set A of attributes of the objects
- A set V of values that these attributes can take

Grammar for generating symbols structures

- object-attribute-value triples
- $$(o,a,v),o\in O,a\in A,v\in V$$
  - (ORGANISOM-1, moprpholoogy, rod)
  - (ORGANISM-1 (morphology rod) (aerobicity aerobic))

#### Working memory

- a store of facts - assertions/propositons
  - Define the initial state of KB/model
  - Rules deinf operators allowing transitions from one state to another
- Each fact is referred to as Working Memory Element
- Described using the vocab and grammar of the system
- Can be interprested as an existential sequence in the First Order Logic
  - $$(Student(name$$ $$john)(department$$ $$computerScience))$$ 
  - $$\exists x:[student(x)\wedge(name(x)=john)\wedge(department(x)=computerScience)]$$ 

#### Production memory

If $$P_1 \wedge P_2 \wedge …\wedge P_m$$ are TRUE, then perform actions $$Q_1,Q_2,…,Q_n$$ 

Two part structure:

- An antecendent set of conditions - if part of the rule
  - A condition is represented by an o-a-s vectore
  - Type $$attribute_1:specification_1,…,attribute_l:specification_k$$ 
  - Set of conditions is interpreted conjunctively
  - A conditon (that is not negated) must match a WME
  - MAtching implies the type is identical
  - Each attribute-specification pair in the conditon has a correspnding attribute-value pair in the WME, where the value matches the spec
  - If there is a WME for each condition, the consequent action will be performed
- A consequent set of actions - the  then part of the rule
  - Actions operate (add/delete/modify facts) on working memory

#### CLIP syntax

- Assume the Wrking meomory contains a set of facts
- If KB contains a rule
- Then 
  - MAtch WME of type patient with first condition
  - Bind variable ord, if not already bound, to organism -1
  - Match WME of type organism
- Rule fires to assert the new fact (consequenct) in working memory
- Organism (name organism-1) (identity …) (confidence $$p$$)

#### Rule interpreter

Recognise-act cycle

- Match the antecedent condition of rules against elements in working memory
- If more than ore rule antecedent matches, choose one of the rules based on some conflict resolution strategy
- Appy the rule
- Repeat the cycle

Cycle halts when no ruels become active, or if the action of rule is fired to halt

#### Controlling Inference Behaviour

- Global
  - Domain independent
  - Hard coded within inference engine
- Local control
  - Domain dependent
  - Coded in the form of meta rules - reaon about which rule to fire rather than about objects in the domain

Typically a mix of both is used

#### Behaviours as proof

- When facts match a rul’s condition part, the rules fire
  - Adding a new fact
  - Arriving at a solution
- Producing a solution from KB is akin to logical proof
- We are demonstrating that something logically follows from the initial state of the system
- We call the sries of rules we fire an inference chain
- Two main ways
  - Forward and backwward chaining

#### Forward and backward chaining

- Forward:
  - Data drive - starts from known data
  - Facts that can be inferred will be inferred even if they are not related to the goal
- Backward chaining
  - ​	Goal driven: System has a goal and the inference engine attempts to find the evidence to prove it
  - Only use data which is needed to determine the goal 

##### Forward Chaining

- bottom up ground proof procedure
- SImple procedure of matching production rules that can be fires
- Sekect rules that produce new WME for the KB
- Repeated until no rules can fire, then check to see if the desired solution is now in the KB
- Forward chaining is both sound and complete
- May undertake significant processing on rules that do not contribute towrards goal

**Example**

FACTS: $$A,B,C,D,E$$

Rules:

- $$Y\&D\rightarrow Z$$
- $$X\&B\&E\rightarrow Y$$ 
- $$A\rightarrow X$$ 
- $$C\rightarrow L$$ 
- $$L\& M\rightarrow N$$

![image-20211109120653931](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109120653931.png)

- Inferring $$L$$ was wasteful in this case:

![image-20211109120733057](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109120733057.png)

##### Backward Chaining

- knwon as top-down definite clause proof procedure
- Start from the query, work backwards to determine if it is a logical consequence of the KB
- Query is a clause containing WMEs that we want the KB to contain
- We select an alement of the clause, and find a prouction rule that results in that element being added to the KB
- We replace the element with the condition of th eproductio rule
- Repeat until the query clause is entirely made up of elements that already exist in the KB

- Unlike fwd, backward is non-deterministic, based on choice of production rules
- Can stop early if elements of the query cannot be derived
- Since query made up of conjunctives, if one element cannot be derived from the KB then the whole query cannot be derived
- When choosing the production rule that derives the element, a dead end may be hit
  - Does not mean that we must stop, merely we must select a new production rule
  - We can only definitively say an element cannot be derived if all clauses explored

##### Example

FACTS: $$A,B,C,D,E$$

Rules:

- $$Y\&D\rightarrow Z$$
- $$X\&B\&E\rightarrow Y$$ 
- $$A\rightarrow X$$ 
- $$C\rightarrow L$$ 
- $$L\& M\rightarrow N$$

![image-20211109121424269](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109121424269.png)

- D is true, by Y is not, is there a rulw which as Y in the kb

- B,E in the KB, X is not
- A is in the knolwedge base,
- now infer Y, then Z

![image-20211109121434007](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109121434007.png)

#### Ask the user

- introduct askable clauses
- Only way to receive new information is from a user/expert
- Can be tedious to have the user inpu tall known info, particularly when it is unclear what will be relevant
- Instead, define some clauses as ‘askable’ meaning info the system can ask the user about
- Modify backward chaining to incorporate the ability to clarify information with the user
- the user and the system now have a symmetric relationship









### Knowledge level debuggining

Four types of non-syntactic errors in rule based systems

- An incorrect answer is produced - a clause that should be false has be interpreted to be true
- An answer was not produced - a clause that should be true that could not be derived
- Stuck in an infinite loop
- System asks irrelivent quetions - reassesment of the KB

#### Debuggin incorrect answers

- suppose some clause was proved false in the intended interpretation
- Must be some rule in the KB that was used to prove that clause
- Either:
  - One of the variables in the rule is false in the intended interpretation
    - Debug by asking the user if each variable should be true
  - All of the variables are true in the intended interpretaion
    - The rule itself is wrong ans hould be reassessed

#### Missing answers

- if a variable is true in the intendedinterpretation, but could not be proved, then either:
  - There is no appropriate rule for the variable
  - There is a rule, and it did not fire when it should have - this means that one of the variables of the ocndition should be true, but is not
  - Can solve this recursively - finding al variables that should be true but do not have a rule

#### Infinite loops

- A Knowledge based system can get stuck in an infinte loop if the rules are cyclical 
  - If you convert the KB into a directed graph, can check for cycles
  - Nodes are the variable
  - sEdges represent that the source node is used to derive the destination node
- Rules should be reassessed to ensure an acycluc KB
- Forward chaining cannot get suckk in infiite loops - only fires a reule if the consequendt set is not already in the working memory

#### Conflict resolution

- the firing of a rule may e
- affectht activation of other rules since it changes the KB
- The method for choosing which rule to fire when more than one can be fired in a given inference cycle is called conflict resolution
  - Can significantly change the behaviour of the system
  - Can affect the runtime of a bakward chaining system
- The set of rules tha can potentialy be fired in a single cycle referred to as the conflic set called agenda in CLIPS

##### Basic approach

- fire the rules in the order of appearance in the knowledge base
  - Stop when goal is reached
  - However,
    - Rule order has strong influence
    - Implicit knowledge
- Use rule priority
  - Make explicit the order in which rules may fire
  - Difficult to define priorities, and makes it harder to add 

**If** infection is meningitis - priority 100

**And** the patient is a child

**Then** drug recommendation is ampicillin

##### Conflict Resolution Mechanisms

- typically use rule-independent conflict resolution mechanisms
- Specificity - Fire the most specific rule
- More conditions = more difficult to satisfy
  - Take more data in working memory into account
- Used to deal with exceptions to more general rules

**Recency**

- fire the rule that used the data most recently entered into the working memory

**Refractoriness**

- A rule is only allowd to fire once on the same data
- Prevents loops 

##### Meta Knowledge

- Knowledge about knowledge
- Concerns the use and control of domain knowledge in an expert system
- Represented in the form of meta rules
- Determine the strategy for the use of task specific rules in the expert system

- May be domain independent, more likely to be deomain dependent
- Elicited from te domain exper in addition to domain knowledge

![image-20211109123035854](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109123035854.png)

#### Efficient Rule Matching

- Observations
  - The working memory is only modified bery slghtly in each recognise act cycle
  - Many rules share conditions
- RETE algorithm
  - Create a network from rule antecedents – offline
    - Two types of nodes 
    - $$\alpha$$ represents simple self contained tests
    - $$\beta$$ variables create contraints between different conditions

![image-20211109123247161](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211109123247161.png)



##### RETE Algorithm

During operation of the production system

- tokens representing new or changed WMEs are passed through the network
- Tokens that make it rhough the network satisfy the rule
  - A new cnflict set is generated from the conflict set of the prvious cycle, incorporating any changed made to WM
  - Only a small part of the WM needs to be matched

- If a tkoen cannot move through the network, it is reassessed when the corresponding WME is modified

#### Pros and cons of rule based systems

- each mapping between expert expressed their knowledge and format of the rules
- Each rule an independent peice of knowledge
  - Make relationship between the rules more opaque
  - Becomes less manageable as the number of rules increase
- Separation of knowledge from the processing/control structures
- Ability to represent and reason with uncertain knowlege
- Exhaustive search through all rules during each inference cycle
- The Knowledge aquisition bottleneck - system does no independent learning
- Brittle

### Assumption Based Reasoning

- often want agents to make assumptions than doing deductions from their knowledge
- **Abduction** - an agent make assumptions to explain observations
  - It hypothesizes what could go wrong with a system to produce the observed assumptions
- **Default Reasoning** - An agent makes asuumptions of normality to make predictions
  - A delivery robot may assume the route is open even when not true

#### Assumption based framework

Defined in terms of two sets of formulae

- A set of xlosed formula $$F$$ called facts
  - Given as true in the world
  - Can include integrity constraints which are formula that evaluate to false
  - Means every variable in the condition cannot be true at the same time
- Set of formulae $$H$$ called the possible hypotheses - assumables



#### Default Reasoning and Abduction

- two strategies for using assumption based framework
  - Default reasoning - where the truth of a clause $$g$$ is unknown and is to be determined
    - An explanation for $$g$$ corresponds to an argment for $$g$$
  - Abduction - where $$g$$ is given and we need to explain
    - $$g$$ could be an observation in a recognition task
    - or a given design goal in a design task
- Given observations - typically abduction
- Default reasoning is then uesd to find the consequences



#### Computing explanations

- need to find a set of assumable that imply the clause -query
- for each var in the query, find a rule that results in the variable being added to working memory
- Replace var with the condition of the rule
- Repeat until all variables in the query are assumable’s
- Must ensure we do not violate integrity constraints

## Planning With Uncertainty

### Uncertainty and AI

- logicism was the earliest approach for knowledge reepresentation
- Problems:
  - Cannot cope with uncertainty
  - Real world does not always have obcious logical implication
  - MCYIN used Certainty Factors to tray and avoid this, however it suffers from sound theoretical backing

### Using Probability

- the process of reasoning about experiments that have a set of distinct outcomes
  - Drawing the top card from a deck
    - Outcome is the face of a card
  - Picking a person from a population and etermining whether they smoke or now
- An experiment is well defined once the set of outcomes has been indentified

Definitions:

- A measure of chance
- The proportion of cases in which an even occurs
- A measure of beleif in the proposition

#### Where Do Probability numbers come from

**Freqentist View**

- From repeatable identical experiments
- The relative frequency of each outcome approaches a limit
- A coin toss or dice roll

**Objectivist View**

- Propensities of object to behave in certaqin ways
- Use frequentist appeoach to calculating probabilities

**Subjectivist View**

- degree of beleif rather than any physicsl signific ance
- The probability of getting a tooth cavity is 1 in 10 - in opinion
- My not translate to number achieved in experiments
- may not find agreement between different people  

#### Probability space

- The sample space $$\Omega$$
  - The finite set of possible outcomes $$s_1,…,s_n$$ - the states of the world
  - Outcomes in $$\Omega$$ must be mutually exclusive and exhaustive - atomic events
  - A probability measure $$(\Omega, P)$$ is obtained by assigned a real number: $$P(s)\in [0,1]$$ to each state $$s_i\in \Omega$$ such that $$\sum_{s_i \in \Omega}P(s_i)$$
  - This is known as a finite probability space
- An Event E is a set of outcomes $$E\subseteq \Omega$$
  - $$\Phi$$ is the impossible event
  - $$\Omega$$ is the certain event
  - For an event $$E\subseteq \Omega:P(E)=\sum_{s_i\in E}P(s_i)$$

#### Random Variables

- given a probability space $$\Omega , P$$ a random variable $$X$$ is a function on $$\Omega$$
- Each element in $$\Omega$$ is assigned a unique value
- The set of possible values $$X$$ can assume is called the domain of $$X$$ 
- A RV with a finite domain is known as a discrete random variable
- We can use random variables to construct propostitions
  - A primitive proposition is either an assignment to a variable, a comparison between a variable and a value, or a comparison between a variable and a variable
  - A proposition can be contructed from a primitive propositions using logical connectives
    - $$X=$$ heads $$\wedge X \neq Y$$ 
- We can express the probability of proposition $$\alpha$$ in relation to each possible states $$s_i$$ of $$\Omega$$ 
  - $$P(\alpha)=\sum_{s_i\in \Omega:\alpha=True \in s_i}P(s_i)$$ 

#### Expected values and variance

- if the domain  of a random variable is a subset of real numbers, we can define its expected value and varianve
- Expected value $$E[X]$$ the average value of the random variable
  - $$E[X]=\sum_{s_i \in \Omega}P(s_i)X(s_i)$$  where $$s_i$$ is an outcome in the probability space. $$P(s_i)$$ is the probability of $$s_i$$ occurring and $$X(s_i)$$ is the value of $$X$$ in $$s_i$$	    
  - What is the average value for the sum of values when rolling two dice
- Variance $$Var[X]$$ measure the spread of the random variable:
  - $$Var[X]=E[(X-E[X])^2]$$

#### Probability distributions

- Discrete - the domain has a finite number of values
- Continuous - the domain consists of a range ov values
- Binary - there are two outcomes

A probability distribution is a function over a random variable, that assigns a probability to each possible outcome in the domain.

- A joint probability distribution is a probability distribution defined over multiple random variable
  - The Cartesian product of the domains of the two random variables

#### Axioms of Probability

- Given two proposition $$\alpha,\beta$$ 
  - $$P(\alpha\wedge\beta)$$ is the probability of  $$\alpha$$ and $$\beta$$ occuring
    - If this value is 0, then they are disjoint
  - $$P(\alpha\or\beta)$$ is the probability of either one or both occuring
  - $$P(¬\alpha)$$ is the probability that $$\alpha$$ does not occur
- If $$(\Omega,P)$$ is a probability space, then:
  - $$P(\Omega)$$ =1
  - $$0\leq P(\Omega)\leq 1 \forall \alpha \subseteq\Omega$$ 
  - For propositions $$\alpha,\beta \subseteq \Omega : \alpha \cap \beta=\emptyset$$
    - $$P(\alpha\or\beta)= P(\alpha) + P(\beta)$$ 
    - In general, when $$\alpha\cup\beta\neq\emptyset :P(\alpha\or\beta)=P(\alpha)+P(\beta)-P(\alpha\wedge\beta)$$ 
  - Negation of a proposition: $$P(¬\alpha)=1- P(\alpha)$$
  - Reasoning by cases: $$P(\alpha)=P(\alpha\wedge\beta)+P(\alpha\wedge¬\beta)$$ 
  - If random variable $$\beta$$ has a domain $$D$$ then: $$P(\alpha)=\sum_{d\in D}P(\alpha\wedge\beta=d)$$

#### Conditional Probability

- if alpha and beta are propositions such that the probability of beta is not 0 then the conditional probability of $$\alpha$$ given $$\beta$$ is deonted as $$P(\alpha|\beta)$$ is defined as
- $$P(\alpha|\beta) = \frac{P(\alpha\wedge\beta)}{P(\beta)}$$
- This is known as the posterior probabililty of $$\alpha$$ being true when $$\beta$$  is true
  - $$P(\alpha)$$ is also known as the prior probability and is equivalent to $$P(\alpha|true)$$
  - Reflects the background knowledge about the chance of $$\alpha$$ being true
- Conditional probability is not a measure of causaility

#### Independence

- not all random variable or events affect the probability of each other
- If we have two random variable $$X,Y$$ then:
  - If $$P(X|Y)=P(X)\wedge P(Y|X)=P(Y)$$ then $$X,Y$$ are independent, since $$Y$$ occurring does not effect the chance of $$X$$ occurring and vice versa

![image-20211116095506725](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116095506725.png)

Conditional independence

- If X,Y are conditionally independent given a random variable $$Z$$ if:

![image-20211116095612670](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116095612670.png)



- Assuming independence is a useful tool since it means we do not need a list of exhaustive conditional probabilities which can often be infeasable to compute

#### Probability theorems

**Total probabiltiy**

- Given a set of disjoint event $$A_i$$ that partition of the Same space $$P(\Omega)=\sum_iP(A_i)$$ 

![image-20211116095802771](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116095802771.png)

**Product Rule**

- $$P(A\wedge B)=P(\alpha|\beta)\cross P(\beta)$$ 

**Chain Rule** - generalisation of the product rule

- If we rearrange the definition of conditional probability, we find that $$P(\alpha\wedge\beta)=P(\alpha|\beta)\cross P(\beta)$$ 
- This means any conjunction of propositions and can be expressed as a product of conditional probabilities

![image-20211116100028048](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116100028048.png)



#### Bayes Rule

Bayes rule is the foundation of much reasoning in a:

- $$P(A|B)=\frac{P(B|A)p(A)}{P(B)}$$ provided neither of the porbabilities are 0.
- The probability of B can be found by enumerating the possible situations in which B can occurr in relation to A

![image-20211116111402359](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116111402359.png)

- This can be extended in we have $$n$$ mutually exclusive and exhaustive events: $$A_1,..,A_n$$ such that $$P(A_i)\neq 0$$, using $$P(B)=\sum^n_{i=1}p(B|A_i)p(A_i)$$ 

![image-20211116112131942](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116112131942.png)



##### Example

- j has a chest X ray
  - Results return positive for lung cancer
  - Should they be worried
  - Satats saying 1 in 1000 have lung cancer
- Depends on the value of $$p(L=true |T=p)$$ 
  - L is a random variable with space true or false representing whether the patient has lung cancer
  - T is a random variable with space $$\{p,n\}$$ where $$p$$ signifies a positive X ray, and $$n$$ signifies negative
- On further investigation into the test
  - A false negative rate  of $$p(T=n|L=true) = 0.4$$ 
  - False positive rate of $$p(T=p|L=false)=0.02$$  
- use bayes rule:

![image-20211116112442868](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116112442868.png)

- T being positive with $$L$$ being true



#### Joint probability and Marginalisation

- Given random variables $$A,B$$ with
  - Mutually exclusive states $$a_i$$, $$b_j$$
  - Probability distributions $$P(x_i)$$, $$P(y_i)$$ 
- the join probability distribution  $$P(A,B)$$ will contain $$m$$ events for which $$A$$ is in the state $$a_i$$
- To calculate $$a_i$$ 

![image-20211116112825658](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116112825658.png)

- Variable $$b$$ is said to be marginalised out of $$p(A,B)$$
- Givena  joint probability distribution, we can use this to find the probabilities for each individual outcome for each probability
  - These can then be used to determine if the variables are independent



##### Example

- j goes for a second X ray that returns a positive result

![image-20211116113127159](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113127159.png)

- Since we do not know this, we assume conditional independence 

![image-20211116113157349](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113157349.png)

- ​	We are assuming that if a patient has lung cancer, the result of one test is independent of the result of the second

![image-20211116113229179](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113229179.png)

#### Continuos probability densities

- Suppose there is a camera filming a conveyor belt of fish
  - Objective is to automatically sort salmon and sea bass for processing and packaging
  - Assume that we have methods for extracting the lightness of the fish from the images
  - The lightness of the fish is an example of a continious randrom variable in the interval [0,1]
- Want to calculate $$P(F|X)$$, where $$F$$ is a discrete random variable with domain Salmon and Seabass

- Bayes rule provides us with the ability to calculate $$p(F|X)$$ given $$p(X|F)$$ and $$p(X)$$ 
  - How do we assign probabilities to each of the infinite set of possible value of $$X$$ 
  - Assigning an equal probability to each of the infinite values of $$X$$ would imply a probability of 0 for each value

- there are an infinte number of values, this implies a p of 0 for each value

![image-20211116113807189](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113807189.png)

- Use a probability density function from reals to non-negative reals that integrates to 1
- The probability that a real-value random variable X has a value between a and b is given by :

![image-20211116113849203](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113849203.png)

- Use a standard statistical distribution such as Gaussian distribution

  - Given a mean $$\mu$$ and a standard deviation $$\delta$$ for $$X$$ 

  ![image-20211116113959297](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116113959297.png)

##### Example

![image-20211116114036102](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116114036102.png)

- given the full disjoint distributio, using marginilzation:

![image-20211116114123126](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116114123126.png)

- Suppose the query $$q$$ is $$p(cavity|toothache)$$ then 

![image-20211116114152266](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116114152266.png)



#### Probabilistic inference

- The computation from observed evidence, of posterior probabilities for query propositions
  - The full join probability distribution is the KB
- The general inference procedure is as follows
  - Let $$X$$ be the query variable
  - Let E be the evidence variables and $$e$$ be the observed values for them 
  - Let Y be the unobserved variables
  - We must calculate $$P(X|e)$$

![image-20211116114526993](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116114526993.png)

### Bayseia Belefi Network

#### Problems with larger instances

- Consider a larger problem with 5 random variable
  - Smoke
  - Bronchitis
  - Lung Cancer
  - Fatigue 
  - Positive X ray
- What is the probability $$p(l_1|c_1)$$

![image-20211116114912375](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116114912375.png)

- Some of the required join t probabilities may not be easily available not all variables directly affect another
- There is an exponential number in terms of the sums
- Need an approach making use of conditional independence.

#### Graphs and Probability Distributions

- Variables often related through an inference chain
  - Smoke affects lung cancer affects fatigue
- Suppose we have a joint probability P of the random variable in some set V and a directed acyclic graph $$G=<V,E>$$ 
  - $$(G,P)$$ is said to satisy the Markov Condition if for each variable $$X\in V$$, $$X$$ is conditionally independent of the set of all its non-descendants given the set of all its parents: $$l_p(\{X\},ND_x|PA_x)$$ 
  - $$ND_X$$ is the set of all non descendants of $$X$$
  - $$PA_X$$ is the set of all parents of X



- If (G,P) satisfies the Markov condition, then $$P$$ is equal to the product of the conditional distributions of all nodes given value of their parent, wheneve4r these condition distributions exist
  - Allows the number of parameters to be determined to be much smaller
  - Only the condition probabilities of $$p(X|PA_x)$$ need to be determined
  - If each node is binary, and has at mose one parent, less than $$2n-1$$ parameters need to be determined as opposed to $$2^n-1$$
- If we need to know P in the first instance to know (G,P) Satisfies the Markov condittion how have we reduce the number of  parameters to detemine?
- Given a DAG G, in which each node is a random variable, and a condition probability distribution of each node given values of its parents in $$G$$
  - The product of the conditional distributions yields a joint probability distribution $$P$$ of the variables and $$(G,P)$$ satisfies the Markov condition

#### The networks

- givena problem domain using a set of $$n$$ random variables $$V=\{X_1,…,X_n\}$$ a joint probability distribution $$P$$ defined on $$V$$ and a DAG $$G=<V,E>$$ 
- GP is a bayesian beleif network if it satisfies the Markov Condition
- A bayesian beleif network
  - a SAG
  - Eacxh variable corresponds to a node in a graph
  - Parents of a node $$x_i$$ are a subset of the variables with a direct influence on the node
- The BBN represents a full joint probability disttributions over these variables
  - DOes this by using local conditional probability distributions $$P(X_i|PArents(X_i))$$ - the conditional probability table
  - Also uses assertions of conditional independencce

![image-20211116120300692](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116120300692.png)



#### The Chain Rule

- when building a BFD, we need an ordering of variables in terms of which ones are dependent on which others
- Typically a node $$i$$ should only depend on nodes $$j<i$$
  - Order is important since ordering may result in different belief networks because the nodes eligible to parents will be different
  - Some orderings will result in less complex networks that others
- We can then use the chain rule and the Parents() function to define a joint probability dist.

![image-20211116120533668](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116120533668.png)

- Like the chain rule, a BF defines a joint factorization of the joint prob. dist. as a product of conditional prob.’s.

#### Networks

- the network of variables which is shown below
- Every one of these variables is dependent on $$X$$ 
- Often no representation for the not true of each case

![image-20211116124432163](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116124432163.png)

##### Simple example: - lungs

- probability that lung cancer is present is 1 in 1000

![image-20211116124557733](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116124557733.png)

##### Not so simple example:

- each variable may have more than one parent and or more than one child

![image-20211116124714888](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116124714888.png)





#### Reducing complexity - Noisy Or

- Local probability distributions can get large $$O(2^n)$$
- We can approximate these distributions by using canonical interaction models that require fewer parameters.
- Noise OR:
  - Describes a set of $$n$$ causes $$(x_i,x_j,…)$$ and their common effect $$(y)$$ 
  - Assumes each $$x_i$$ is sufficient to cause the effect $$y$$ in the absence of all other causes
  - The ability $$x_i$$ to cause $$y$$ is independent of the presence of  other causes
- Only need $$k$$ parameters 

![image-20211116125149559](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116125149559.png)











#### Noisy OR Gate Model

- Consider the BBN representing the relation ship between F, L, B, O
- Causal inhibition - Each cuase has an inhibitor that inhibits the expression of the cause
  - The effect is observed iff the inhibitor is disabled
  - Bronchitis will result in Fatigue iff the mechanism that inhibits B from causeing F is not present
- The inhibiting mechanism of one cause is independent of the mechanism inhibiting the other causes - Exception independence
- The effect can only happen if at least one of its causes is present and not being inhibited - Accountability 

![image-20211116125400286](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116125400286.png)

- Nodes whose value is exaclty specified by the parent nodes are called dereministic nodes 
- An inhibitor has a probability of being observed

![image-20211116125501119](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116125501119.png)



- A BBN does not actually contain the deterministic nodes and inhhibitor nodes
  - For each cauase node $$n_i$$ with inhibitor $$i_i$$ the causal strength $$p_i$$ is defined as $$1-p(i_i)$$ 
  - $$p_i=P(y|n_i,¬n_j)$$ for $$i\neq j$$ 
- Given the set of causes observed $$X_p$$ the probability of $$p(y|X_p)$$ is calculated as:

![image-20211116125821238](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116125821238.png)

#### Constructing a BBN

- Nodes and values
  - What are the variables of interest given the problem being modelled
    - Consider what might be observable, anything that is will likely need to be a var
    - Are there hidden or latent vars that can make the model simpler? Theres vars may account for dependencies, reduce the complexity of the cond probabilities, or better model how the wold is assumed to work 
  - What is the domain of each of these variables
    - Consider the level of detail needed to answer the sorts of queries that will be encountered
    - What must be true in the world for a variable to have a particular value should satisfy the **clarity principle**: an omniscient agent should be able to know the value of a variable

- **Define the structure/topology of the network**
  - Qualitative relationships betwee nodes
  - What directed edges need to be defined betwen the nodes?
    - Nodes are connected by an edge if one affect the otther
    - The direction is from tehcause to the efect
  - Must satisfy the Markov Cond
    - Independencies suggested by the lack of arcs must hold
    - It is not necessary that arcs are actual dependencies
- **Conditional Probability Table**
  - Quantitative relationship between nodes
  - One table for each node $$X_i$$ defining $$P(X_i|PA_{X_i})$$ 
  - CPTs must be defined to nullify any dependency

- What information is required
  - Given a domain with $$n$$ boolean variables, the full prob dist. table has $$2^n$$ entries
  - A BBN consisting of $$n$$ nodes
    - Each node may have up to $$k$$ parents: $$k=max_i|parents(n_i)|$$ 
    - Maximum number of entires in each conditional probability table is $$2^k$$ 
    - Maximum number of entries in full probability distribution is $$2^kn$$ which is linear in $$n$$

#### Pearls Network Construction Algorithm

- Given an ordering of nodes $$\{X_1,..,X_n\}$$ 
- Process each node in order
  - Add it to the existing network
  - Add arcs from a minimal set of parents such that the parent set renders the current node inpenedent of every other node preceding it
  - Define $$PA_{X_i}\subseteq\{X_1,…,X_{i-1}\}$$
- Define the CPT for $$X_i$$

- Notes
  - The resulting network given any node ordering can define the same joint probability distribution
  - Topology may be very different 
  - Some networks will be mroe compact than others
  - Compact networks are desirable as they are more tractable
  - Compact networks are desirable as they are more tractable
  - Dense networks fail to represent independencies or causal dependencies

##### Example

- One on the right is very simiple whereas middle is harder to read

![image-20211116131050862](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116131050862.png)

- There are $$n!$$ orderings, therefore this is difficult
- The arc between

![image-20211116131141315](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116131141315.png)



- The arc between Smoke and Pollution is solely to nullify the dependency between Smoke and Pollution through the common cause node Cancer
- The above DAG does not entail the independencey between Smoker and Pollution, but the underlying joint probability table contains the following independence relation

![image-20211116131255316](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116131255316.png) 



#### Markov Condition and Entailed Conditional Dependence

- Given a DAG $$G=<V,E>$$ and a joint probability distribution $$P$$ defined on the random variables in $$V$$ the Markov Condition
  - Entails only independencies NOT dependencies
    - Absence of an eddge4 between $$X,Y\in V$$ entails that there is no direct dependency between $$X$$ and $$Y$$
    - The presence of an edge does not imply there there is direct dependency
- We say that base on the Markov condition, G entail cond. independence $$I_P(A,B|C)$$ for $$A,B,C\in V$$ if $$I_P(A,B|C)$$ holds for every $$p\in \P $$ where $$\P$$ is the set of all probability dists $$p$$ such that $$(G,P)$$ satisfies the Markov condition

### Exact Inference in a BBN

- given ovserved event e defines as a set of evidence variables $$E$$ with values assigned
- We want to ocmpute posterior probability distribution for a set of query variables $$X$$ $$p(X|e)$$
- $$Y$$ is the set of hidden variables  - non evidence
- Types of evidence
  - Specific evidence - evidence that a variable $$E=e1$$ 
  - Negative evidence - evidence that a variable $$E\neq e1$$ 
  - Virtual (likelihood) evidence - defines a new probability distribution over $$E$$



#### Probabilistic Inference

- Two main types of PI
  - Exact - where the probabilities are computed exactly
    - A simple version of this is enumeration
    - Also use variable elimination which exploits conditional independence
  - Approximate Inference - approximates the probabilities with methods characterized by the different guarantees they provide
    - May produce guaranteed bounds on the probabilties - the exact prob. that they fall between a given range
    - My produce probabilistic bounds on the error - the error is within some range some percent of the time
    - As time increases,  typically the probability estimates will converge to the exact answer

#### Types of Inference

- Diagnostic Reasoning	
  - Reasoning from symptom to cause
  - Occurs in the opposite direction from the arcs
- Predictive reasoning
  - Reasoning from cause to sympton
  - Along the direction of the arcs
- Intercausal Reasoning - explaining away
  - Reasoning about the mutual causes of a common effect
  - If a cause and its immediate effect are observed, reasons about the other possible causes of the observed effect
  - Mutual causes are considered independendtr of each other, unless the mutual effect is observed
- Combined reasoning
  - If the query variable is a parent of some variables, and the descendent of other observed variables

![image-20211116132802813](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116132802813.png)

##### Example

![image-20211116132917537](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116132917537.png)

#### Inference by Enumeration

- Involves enumerating through every world that is consistent with evidence

![image-20211116133054985](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116133054985.png)

##### Example

![image-20211116133401760](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116133401760.png)



- probability of a first fire $$X=F$$
  - A storm took place
  - Lightling $$L$$ was observed
  - No tour bus visited the forest

- Calculate the posterior probability $$p(F|S\and L\and¬b$$)

![image-20211116133550569](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116133550569.png)

- $$P(E)=\sum_{x\in E}p(x)$$

![image-20211116133705768](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116133705768.png)

- which means that the total probability given these conditions are: 

##### Inference in a chain of three nodes

- If flue is observed, we can use the chain rule:

  - $$p(HM|F)=\sum_{HighTemp}p(HM|HT)p(HT|F)$$ = 0,87

- If A,C are conditionally independent:

  $$p(C|A)=p(C|B)p(B|A)+p(C|¬B)p(¬B|A)$$

![image-20211116134115377](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116134115377.png)

- If HighMerc readings are OBserve, we can use Bayes rule and the chain rule:

![image-20211116134259661](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116134259661.png)



#### Variable Elimination

- adapted from the methods for solving CSPs and optimising with soft constraints
- Based on the notion that a BF specifies a factorization of the joint probability distribution
  - VE is more efficient than enumeration
- Recall - a condition probability is a funciton on a variabel $$Y$$ and some set of evidence vairables
- Also: $$\forall x_1,…,\forall x_k\sum_{y\in domain(y)}p(Y=y_1|X_1=x_1,...,X_k=x_k)=1$$ 
  - The probability of the possible outcomes for Y given a specific set of value assignments to the evidence should sum to 1
- We call a function on a set of variables, a factor, and say the scope of a factor is the set of variables in invloves
- Conditional probability $$p(X|Y,Z)$$ can be describes as $$f_0$$ with scope $$(X,Y,Z)$$ 



##### Expressing Factors

- if we have a finite set of variables each with a finite domain, factors can be expressed as arrays
  - If there is an ordering of the variables
  - And the values in the domains are m apped into non-negative integers then
  - There is a unique representation of each factorr as a one domentional array
- For example we can represent the conditional probability table pictures as $$[0.1,0.2,0.3,0.4]$$ 
- we can perform a number of operations
  - Conditioning
  - Summing
  - Multi9pllying

##### Conditioning Factors

- Conditioning on observed variables
  - If we have observerved a variable, and know its value, we can define a new factor with a new domain
  - If we start with $$P(X|Y,Z)$$ and then learn that $$Z=t$$, we have a new factor $$P(X|Y,Z=t)$$  whose scope is now only $$(X,Y)$$ since $$Z$$ is known
- Using a conditional probability table representation, we can see how assigning variable values, the scope of the factor is decreased until only a single probability remains

![image-20211116135522354](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116135522354.png)



##### Multiplying Factors

- if two factors share a variable in their scope, we can multiple them together to make a new factor, which a scope equivalent ot the union of the original two factor’s scope
- For example $$f_0(X,Y)\cross f_1(Y,Z)=f_2(X,Y,Z)$$
  - To find the value of $$f_2(X=t,Y=f,Z=t)$$ we multiply the value of $$f_0(X=t,Y=f)$$ the value of $$f_1(Y=f,Z=t)$$ 
  - Consider the example pictured

![image-20211116135852409](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116135852409.png)



#####  Summing Factors

- we can elminate a chosen variable, in this case Y, by adding together the outcomes for each possible value ofY
- $$F_1(X,Z)=F_0(X,Y=t,Z)+f_0(X,Y=f,Z)$$ 
- As with conditioning, it allows the removal of a variable in order to simplify the conditional table
- Known as summing out a variable

![image-20211116140135397](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116140135397.png)



#### Variable Elimination

- For a given query, we can represent the cond probs of a BBN as a series of factors
- The algorithm for solving is
  - Construct a factor for each non cond probability condition
  - Eliminate each non query variable
    - If the variable is observed its value is set to the observed value in each of the factros in which the variable appears
    - Otherwise the variable is summed out
  - Multiple the remaining factors, then normalise



##### Example

![image-20211116140656106](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116140656106.png)

- nexzt eiminate variable suppose begin with fire
- Multiply together all factors that contain fire then sum fire out

![image-20211116140740059](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116140740059.png)

- repeat this process with factors containing Alarm to eliminate that variable
- Two factors only with tampering
  - $$f_0(Tampering),f_5(Leaving),f_7(Tampering,Leaving)$$ 
- Trying to find tampering, must eliminate leaving
- Again multiply and sum out Leaving
  - $$f_0(Tampering),f_8(Tampering)$$ 
- Multiple these factors together 
  - $$f_9(Tampering)=f_0(Tampering)*f_8(Tampering)$$ 
- These factors have the same scope they will have different values as they are constructed using different evidence

- Finally we can find the posterior distribution over 
- Tampering

​		$$f_9(Tampering)/\sum_{Tampering}f_9(Tampering)$$ 

- Denominator represents the prior probability of the evidence - in this case Smoke = True, Report = True

### Making Decisions with Outcome Probabilities

- BBS provide a mechanism to conduct Bayesian inference on large sets of random variables
  - Deciding on the liklihood of a set of ‘related’ diseases given vertain observations
- The likelihoods of outcomes must have then been used in some way to make a decision, a processs not specifically suppported by a BBN
  - Deciding on a treatment regime for a patient
  - Need to explicitly represent actions under consideration and utitity of the resultant outcomes
- Two tools
  - Decision trees
  - Influence Diagram or Decision Networks 

#### Expected Utility

- Givena  set of outcomes $$\{O_i\}$$ of a particular action, A a utilitity function $$U(O_i|A)$$, assigns a utility (measure of desirability) to each outcome
- Assuming that we have a prob dist over the set of outcomes, the expected utility of taking the action A is defines as
  - $$EU(A)=\sum_iP(O_i|A)\cross U(O_i|A)$$ 
  - This allows us to calc expected utility of an action
- The assumption is that a Bayesian decision maker wants to maximise their expected utility throuhg actions
- Combining Utility Theory with Probability Theory
  - Given evidence E, which action A is expected to deliver the most value
  - $$EU(A|E)=\sum_iP(O_i|E,A)\cross U(O_i|A)$$
- Utility can be a number of things and is often represented by money
  - £100 ticket for a lotter where you had a chance of 0.0001 to win £2,000,000

#### Outcomes

- typically decision is made by an agent, that must choose an action within a defined world
- To inform the agent, we must define relations between outcomes
- Consider two outcomes $$\sigma_1,\sigma_2$$ possible alternatives are:
  - $$\sigma_1\succeq\sigma_2$$: $$\sigma_1$$ is weakly preferred to $$\sigma_2$$, meaning that $$\sigma_1$$ is at least desirable as $$\sigma_2$$
  - $$\sigma_1\sim\sigma_2$$: means that $$\sigma_1\succeq\sigma_2\wedge\sigma_2\succeq\sigma_1$$- the outcomes are equally desirable - indifferent to which we choose
  - $$\sigma_1\succ\sigma_2:\sigma_1$$ is strictly preferred to $$\sigma_2$$ - this means that we are not indifferent, and that we do not weakly prefer $$\sigma_2$$ to 
  - $$\sigma_1$$ 
- These relations should be:
  - Complete - an agent has preferences between all pairs of outcomes
  - Transitive - if $$\sigma_1\succ\sigma_2\and\sigma_2\succ\sigma_3\implies\sigma_1\succ\sigma_3$$

#### Decision Tree

- contains two types of nodes
  - Chance nodes - circle
    - Represent random variables
    - Edges emerging froma  chance node represent the possible outcomes of the random variable and are labelled with the probability of the outcome
  - Decision nodes - square
    - Represent decisions to be made
    - Edges emanating from a decision node can represent a set of mutually exclusive and exhaustive actions that the decision maker can make

- Expected utility of a
  - Chance node is the expected value of the utilities associated with its outcomes
  - Decision alternate (action) is the expected utility of the chance node encountered if the action is takes
  - Decision node is the maximum utility of all its alternatives



##### Example

to buy or not to buy stock

- Suppose you have $$1000
- Decision invest in stocks - 100 shares of company X, or in the bank with guaranteed 0.5% return in a month
- X is a random variable that represents the stock price

![image-20211116145404778](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116145404778.png)



EU(Buy X) = 0.25x500 + 0.25+1000 + 0.5x2000

#### Utility

- the assumption that the agent/user wants to maximise expected utility, is known as normative theory:
  - Assumption that in the long run you will be better off
  - People may prefer a guaranteed \$$1005 rather than only having only \$$500
  - Different users/agents have different levels of risk aversion 
- The idea that humans do no perceive absolute value, but value in context, and thus have different risk tolerances is known as prospect theory
  - £10,000 means a lot more to someone with £10,000 than someone with £100,000 
  - Despite the constant increase

- One way to model an individual’s attitue to risk is with a utility function that maps money to utility
  - Using an exponential utility function, where R is the risk tolerence - the larrger the value of R, the more risk tolerant the user
  - $$U_R(x)=1-e^{-x/R}$$

##### Example

- sue wants 10,000 shares at Y @ 10/Share
  - Her buying this  number of shares will affect the price of Y
- Believes that the overall value of the DOW industrial average will also affect the price of Y
  - Believes that in one month the DOW will either be at 10,000 or 11,000
  - And that Y will be at \$$5 or \$$20 per share 

- Alternatively can buy an option of Y worth $$100,000
  - Allows her to buy 50,000 shares in Y for $$15/Share in one month
- Her beliefs are

![image-20211116150331311](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116150331311.png)

**Belief tree is shown below**

- \$$250,000 = \$$20 x 50,000 - \$$15 x 50,000

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116150453609.png" alt="image-20211116150453609" style="zoom:50%;" />

#### Decision trees with non numerical outcomes

- order outcomes in order of preference
  - Assign 0 utility to the most undesirable outcomes
  - Assign utility of 1 to most desirable outcome
- What utility should be assigned to the inconvenience of carrying an umbrella
  - Choose a value $$p\in [0.1]$$ such that $$U(Inconvenience)\approx p\cross U(SuitNotRuined)+(1-p)\cross U(StuitRuined)$$
  - The utility of a chance node with probability p associated with the most desirable outcome and (1-p) with the least desirable outcome

#### Inference Diagram

Problem with decision trees

- grows exponentialy in size with the number of random vairables
- The probabilities required by the decision trrr are not always ones that are available
- Influence diagrams avoid these issues

Contains three different types of node: chance(circle), decision (square) and utility(diamond)

- Different edges have different meanings
  - Edge to chance node - Value of node is probabilistically dependent on the value of the parent
  - Edge to decision node: Value of the parent is known at the time the decision is made
    - If parent is a decision node, the edge represents a decision sequence
    - If parent is a chance node, reprresents that the evidence about tha parent node will be known before the decision is mare (information link)
  - Edge to utility node - value of node is deteministically dependent on the value of the parent
- Chance nodes satisfy the Markvo Condition
- Influence diagrams are BBNs augmented with utility nodes and ordered (by direction of edges) decision nodes

##### Buying Stock

![image-20211116151517234](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116151517234.png)



#### Evaluating Influence Diagrams

- With a single Decision Node

  - Add any evidence (set the probability of value of random variables observed to 1)

  - For each action value in the decision node
    - Set decision node to that value
    - Calculate posterior probabilities of nodes that are parent of the utility node
    - Calculate the expected utility of the action
  - Return action with the highest utility

##### Sues Dilema

![image-20211116152036793](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116152036793.png)



#### Information Links

- links from chance nodes to decision node
- Indicate that the chance node must be known before a decision is made corresponding to that decision node
  - Can be used to explicitly calculate what decision should be made, given the different values for the chance node
- Evaluating Influence Diagram with Information Links
  - Add any evidence (set the probability of value of random variables observed to 1)
  - For each combination of values of the parents of the decision node, for each action value in the decision node
    - Set decision node to that value
    - Calculate posterior probabilities of nodes that are parents of the utility node
    - Calculate the expected utility of the action
  - Record the resulting expected utility of the action
  - Return the table of actions and associated expected utility value (decision table)

#### Sequential Decision Making

- Typical Example is Test-Action InfluenceDiagram
  - Test Decision node must be evaluated first
  - The cost associated with a test is included as a separate utility node
  - If the decision is to run a test, evidence will be obtained as a result of the test
    - The chance node representing the observation Obs has an information link to the Action decision
    - One value unknownof Obs will represent the decision to not test with CPT
    - $$p(Obs=unknown|Test=no)=1$$ 
    - $$p(Obs=uknown|Test=yes)=0$$
- Evaluating such a diagram
  - Evaluate decision network witth any available evidence
  - Enter test decision as evidence
  - If test decision is not ‘yes’, use value ‘unknown’
  - Evaluate action decision 



![image-20211116153320710](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211116153320710.png)







## Reinforcement Learning

What should an agent do given:

- Priot knowledge - possible staes of world, possible actions
- Observations current state of world, immediate reward/punishment
- Goal - act to maximise accumulated (discounted) reward

Like decision theoretic planning, but do not know the rewards.

### Examples

- Robot - reward task completion, punish dangerous behaviour

### Experiences

- Assume there is a sequence of experiences:
  - state, action, reward, state , action reward
- At any time, the agent must decide whether to
  - Explore - gain more knowledge
  - Exploit knowledge it has already discovered

### Why is it hard

- What actions are responsible for a reward may have occurred a long time before the reward was received
- the long term effect of an action depends what the agent will do in the future
- The explore - exploit dilema: at each time should the agent be greedy or inqusitive

### Main approaches

- learn a model consisting of state transition function $P(s’|a,s)$ and a reward function $R(s,a,s’)$ solve this as an MDP
- Learn $Q^*(s,a)$ use this to guide action

### Agents as processes

Agents carry out actions

- forever - infinite horizon
- Until some stopping criteria is met - indefinite horizon
- Finite and fixed number of steps - finite horizon

### Decision-theoretic planning

What whould agent do when:

- it gets rewards and punishments and tries to maximise its rewards received?
- Actions can be stochastic and the outcome of an action can’t be fully predicted?
- There is a model that specified the probabilistic outcome of actions and the rewards?
- The world is fully observable?

#### Initial Assumptions

- flat 
- Explicit states or features or individuals and relations
- Indefinite stage or infinite stage
- Fully observable
- stochastic dynamics
- complex preferences
- single agent
- knoweldge is given
- Perfect rationality

#### Utility and time

- would you prefer to pay $1000 today or \$1000 next year
- What price would you pay now to have an eternity of happiness
- How can you trae off pleasures today with pleasures in future

![image-20211122102511151](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122102511151.png)

#### Rewards and Values

Suppose the agent receives a sequence of rewards $r_1,…$ in time what utility should be assigned:

![image-20211122102702556](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122102702556.png)

#### Average vs Accumulates

- An infinite sequence of \$1 rewards has the same total as a sequence of \$100 rewards
- 

![image-20211122102750216](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122102750216.png)



- Suppose the agent receives a sequence of rewards in time:

  Discounted retunr $V=r_1 + \gamma r_2 + \gamma^2r_3 + …$ where $\gamma$ is the discount factor between 0 and 1

#### Discounted Rewards

- the discounted rewards are: 

![image-20211122103019461](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122103019461.png)



- If $V_t$ is the value obtained from time step $t$

​	$V_t=r_t+\gamma V_{t+1}$

Infinite future values to immediate rewards

- $1+\gamma + \gamma^2+…=1/(1-\gamma)$ $\implies$ $min$ $reward/(1-\gamma) \leq V_t\leq max$ $reward/(1-\gamma)$
- Value of all he future is at most $1/1-\gamma$ times the maximum reward and $1/1-\gamma$ times the minimum reward
- Thus the eternity of future has a finite value compared to average reward which is dominated by cum remawrd for the eternity of time:

![image-20211122103413853](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122103413853.png)

### World State

- the world state is the information such that if the agent knew the world state, no information about the past is relevant to the future $\rightarrow$ Markovian assumption
- $S_i$ is state at time $i$ and $A_i$ is the action at time $i$

![image-20211122103613774](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122103613774.png)

​	$P(s’|s,a)$ is the probability that the agent will be in state $s’$ immediately after doing action $a$ in state $s$

- The dynamics is stationary if the dist. is the same for each time point.

#### Decision Processes

- A markov decision process augments a Markov chain with actions and values

![image-20211122103804932](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122103804932.png)

- This can be made into a decision process by having rewards and values for actions
- A sequence of actions, states, rewards, 



#### Consitis of:

- a set $S$ of states
- A set $A$ of actions
- $P(S_{t+1}|S_t,A_t)$ specifies the dynamics
- $R(S_t,A_t,S_{t+1})$ specifies the reward at time $t$

​	$R(s,a,s’)$ is the expected reward received when the agent is in state $s$ does action $a$ an ends up in state $s’$

- $\gamma$ is the discount factor

#### Information availability

- Fully observable - the agent gets to observe $S_t$ when decidding on action $A_t$
- Partially observable MDP - POMDP - the agent has some noisy sensor of the state. It needs to remember its sensing and acting history

#### Policies

- A Stationary policy is a function: $\pi:S\rightarrow A$ given state $s, \pi(s)$ specifies what action the agent should do 
- An optimal policy is one wiht maximum expected discounted reward
- For a fully observable MDP with stationary dynamics and rewards with inf or indef horizon, there is always an optimal stationary policy 

#### Value of Policy

Given policy $\pi$

- $Q^{\pi}(s,a)$ where $a$ is an action and $s$ is a state, is the expected value of doing $a$ in state $s$ then following policy $\pi$
- $V^{\pi}(s)$ where $s$ is a state, is  the expected value of following policy $\pi$ in state $s$
- Both can be defined recursively

![image-20211122104640435](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122104640435.png)

#### Optimal Policy

- $Q^*(s,a)$ is the expection value of doing $a$ in state $s$ then following the optimal policy
- $V^*(s)$ is the expected value of following the optimal policy in state $s$
- Defined recursively as:

![image-20211122105242523](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122105242523.png)

#### Value iteration

- Let $V_k$ and $Q_k$ be $k-$ step lookahead value and $Q$ functions
- Given an estimate of the k step lookaheead value function, determine the $k+1$ step lookahead value function
- Set $V_0$ arbitrarily
- Compute $Q_{i+1},V_{i+1}$ from $V_i$
- Converges exponentially fast in k to the optimal value function

​	Error reduces proportionally to $\gamma^k/(1-\gamma)$



#### Asynchronous Value iteration

- Agent does not need to sweep through all states, but can update the value functions for each state individually
- Converges to the optimal value functions, if each state and action is visited infinitely often in the limit
- Either store
-  $V[s]$ 

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122105726739.png" alt="image-20211122105726739" style="zoom:50%;" />

-  $G[s,a]$

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122105741894.png" alt="image-20211122105741894" style="zoom:50%;" />

### Deterministic Case of Reinforcement Learning

- deterministic dynamics,
- Infinite or indefinite stage

#### Experiential Aync Value Iteration for Deterministic RL

Initialise $Q[S,A]$ arbitrarily observe current state $s$

**repeat forever**

​	Select and carry out action $a$

​	Observe reward $r$ and state $s’$

​	$Q[s,a]\leftarrow r+\gamma max_{a’}Q[s’a’]$ 

​	$s\leftarrow s’$ 

### Non Deterministic Case of RL

- stochastic dynamics

### Temporal Differences

- Suppse there are a sequence of values $v_1,…$ and want a running estimate of the average of the first $k$ values:

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122110427383.png" alt="image-20211122110427383" style="zoom:50%;" />



- Often this TD formula to update with $\alpha$ fixed
- We can guarantee convergence to average if:

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122110514404.png" alt="image-20211122110514404" style="zoom:50%;" />

### Q- Learning

- store $Q[State,Action]$ and update this in an sync value iteration, but using experience (empirical probabilities and rewards)
- Suppose the agen has an experience $\langle s,a,r,s’\rangle$ 
- This provides one peice of data to update $Q[s,a]$
- An experience $\langle s,a,r,s’\rangle$ provides a new estiamte for the value of $Q^*(s,a)$:

<img src="C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122110853793.png" alt="image-20211122110853793" style="zoom:50%;" />

Initialise $Q[S,A]$ arbitrary, observe the current state

**repeat forever**

​	Select and carry out an action $a$

​	observe reward $r$ and state $s’$

​	$Q[s,a]\leftarrow Q[s,a]+alpha(r+\gamma max_{a’}Q[s’,a’]-Q[s,a])$

​	$s\leftarrow s’$

#### Properties

- Q learning converges to an optimal policy, no matter what the agen does, as long as it tries each action in each state enough
- What should the agent do?
  - Exploit: when in $s$ select an action that maximises $Q[s,a]$ 
  - Explore: sekect another action

#### Exploration strategies

- The greedy $\epsilon$ - choose random action with probability $\epsilon$ and choose best action with probability $1-\epsilon$
- Softmax action selection: in state $s$ choose with a probability

![image-20211122111255765](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211122111255765.png)

- Where $\tau>0$ is the temperature - good actions are chosen more often than bad attions, and $\tau$ defines how much a difference in $Q$ values maps to a difference in probability
- Optimisim in the face of uncertainty - initialise Q to values that encourage exploration
  - High initial estimates - poor actions will reduce the estimates
- Upper confidence bounds - take into account average + variance

### On Policy learning

- Q does off policy - it leanrs the value of an optimal policy no matter what it does
- Could be bad if the exploration policy is dangerout
- On policy learning learns the value of the policy being followed
  - 80% greedy, 20% randomly
- Estimate the impact of 20% randomness
- if the agent is going to explore, it may be better to optimize the actual policy it is going to do
- SARSA uses the experience $\langle s,a,r,s’,a’\rangle$ to update $Q[s,a]$ - adds the action $a’$ to the Q learning example

#### SARSA

Initialse $Q[S,A]$ 

pbserve state $s$

select action $a$

**repeat forever**

​	carry out action $a$

​	observe reward $r$ and state $s’$

​	select action $a’$ using a policy based on $Q$

​	$Q[s,a]\leftarrow Q[s,a]+\alpha(r+\gamma Q[s’,a’]-Q[s,a])$ 

​	$s\leftarrow s’$

​	$a\leftarrow a’$ 

## Multi - Agent Systems

- many domains are characterized by multiple agents
- Can be cooperative, competitive, or often somewhere in between

- can have its own values, goals, utility function and perceptions
- Select actions autonomously based on their own information
- The outcome can depend on the actions of all, or a subset of the agent
- Each agents value depend on the outcome
- Agents are self interested, acting to maximise their own utility



















