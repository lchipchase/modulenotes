---
layout: default
title: Artificial Intelligence
math: true
---
# CS255 Artificial Intelligence

1. [CS241 - Operating Systems and Computer Networks](./CS241)
2. [CS255 - Artificial Intelligence](./CS255)
3. [CS258 - Databases](./CS258)
4. [CS260 - Algorithms](./CS260)

## Introduction to AI, Rational Agents, and Agent Architectures and Hierarchical Control

<u>Books:</u>

- Foundations of computational agents
- Artificial Intelligence A Modern Approach
- Other Reading (check Moodle) - check for Bayesian AI
- Examinable

----------------------------------------------------------------------------------------

**<u>Learning Outcomes</u>**

- Develop application for knowledge based systems, intelligent agents and architectures.
- Understand a wide variety of knowledge representation and artificial intelligence approaches.

- Understand various methods for search, planning and reinforcement learning.
- Understand various methods for representing and reasoning under uncertainty.

----------------------------------------------------------------------------------------

### Module Introduction

### What is AI

- The automation of activities that we associate with human thinking, such as decision making, problem solving and learning.

Falls into four categories

- Thinking humanly
- Thinking rationally
- Acting humanly
- Acting rationally

Is success a measure of human behaviour or rational behaviour.

----------------------------------------------------------------------------------------

#### Turing Test

- Have a human interrogator
- Have a human and an AI
- Can the human differentiate between the computer and the human
- Suggests the main components
  - Knowledge
  - Reasoning
  - Language
  - Learning
- Is it a good test of intelligence?
- General view is that it isn't
  - Not reproducible
  - Can’t use mathematical analysis
  - Lots of human intelligence factors not considered



**<u>Searle’s Chinese Room</u>** (counter example)

- Consider a room containing a human who understand only English, a rule book in English

----------------------------------------------------------------------------------------

#### Thinking Humanly

- Cognitive modelling
- Need a precise theory of the mind
- Can express this theory as a program
- Cognitive Science draws on AI and psychology

----------------------------------------------------------------------------------------

#### Thinking Rationally

- Derivation of conclusion from particular premises
- Various forms of logic, notation and rules of inference
- Task or problems expressed in logic, then a solution is deduced from here
- Problems:
  - Difficult to express tasks in logic
  - How to cope with uncertainty
  - Computational expense

----------------------------------------------------------------------------------------

#### Acting Rationally

- Doing the right thing
- Acting to maximise goal achievement given the available information and memory 
- Thought not necessarily involved
- Should be in pursuit of rational action.

----------------------------------------------------------------------------------------

#### Specialised AI vs Artificial General Intelligence

- AI designed to perform a specific tasks
- A systems which tries to perform intellectual tasks that a human can do (20 - 100 years)

----------------------------------------------------------------------------------------

#### Rational Agents

- An agent is an entity that perceives and acts
- We are concerned with designing rational agents
- An agent can be viewd as a function from percept histories to actions, $$f:P \rightarrow A$$

- Agents typically required to exhibit autonomy
- For any given class of environments and tasks, we seek the agent(s) with the best performance
- Perfect rationality usually impractical - we must work within out computational restraints
- Design the best thing possible for a set of resources.

----------------------------------------------------------------------------------------

#### What is AI

- They synthesis and analysis of computational agents that act intelligently
- An agent acts intelligently if:
  - Its actions are appropriate for its goals and circumstances
  - it is flexible to changing environments and goals
  - It learns from experience
  - It makes appropriate choices given perceptual and computational limitations

----------------------------------------------------------------------------------------

#### Goals Of AI

- Scientific goal.
- Engineering goal.

----------------------------------------------------------------------------------------

####  Applications of Rational Agents

- Air traffic
- Electricity distribution
- Information management
- Data mining
- Etc.

----------------------------------------------------------------------------------------

### Rational Agents

#### Inputs to an Agent

- Abilities - the set of possible actions
- Goals/preferences - what it wants 
- Prior Knowledge - what background knowledge does it have
- History of stimuli:
  - Current Stimuli - what it receives from the environment now
  - Past experiences - what has it seen before

Example of an agent - rescue robot

- abilities: movement, gripping, speech
- goals: rescue prople, explore
- prior Knowledge: what are important features, categories of object
- stimuli: vision, sonar, speech
- past experiences: effects of steering etc.

----------------------------------------------------------------------------------------

#### Goals in Rational Agents

- WLOG “goals” can be specified by some performance measure defining a numerical value
- Rational action is whichever action maximises the expected value of the performance measure given the percept sequence to date.
- Previous perceptions are typically important.

----------------------------------------------------------------------------------------

#### Rational Agents

- rational $$\neq$$ omniscient
  - A rational agent would know the actual outcome of its actions - agents are very rarly this since unexpected situations occur in dynamic environment. It must only do its best given the current percepts
- rational $$\neq$$ clairvoyant
  - Not expected to foresee future changes in its environment
- rational $$\neq$$ succesful
  - Rational action is defined in terms of expected value, rather than the actual value. A failed actional can still be rational.

----------------------------------------------------------------------------------------

#### Dimensions Complexity

| **Dimension**        | **Possible Values**                                          |
| -------------------- | ------------------------------------------------------------ |
| Modularity           | flat, modular, heirarchical                                  |
| Planning horizon     | non-planning, finite stage, indefinite stage, infinite stage |
| Representation       | states, features, relations                                  |
| Computational Limits | perfect rationality, bounded rationality                     |
| Learning             | knowledge is given, knowledge is learned                     |
| Sensing uncertainty  | fully observable, partially observable                       |
| Effect uncertainty   | deterministic, stochastic                                    |
| Preference           | goals, complex preferences                                   |
| Number of agents     | single or multiple                                           |
| Interaction          | offline, online                                              |

----------------------------------------------------------------------------------------

##### Modularity

- one level of abstraction: flat
- interacting modules that can be understood separately: modular
- modules that are (recursively) decomposed into modules: hierarchical

Flat adequate for simple systems, complex biological systems, computer systems, organizations are all hierarchical

----------------------------------------------------------------------------------------

##### Planning Horizon

- Static - world does not change
- Finite Stage - agent reasons about a fixed, finite number of time steps
- Indefinite stage - agent reasons about a finite, but not predetermined number of time steps
- Infinite stage - the agent plans for going on forever

----------------------------------------------------------------------------------------

##### Representation

Much of AI is finding compact representation and exploiting this for computational gains.

Agent can reason in terms of:

- Explicit states - one way the world can be - chess board
- Features or propositions
  - 30 binary features can represent $$2^{30}$$
- Individuals and relations
  - There is a feature for each relationship on each tuple of individuals
  - Often an agent can reason without knowing the individuals or when there are infinitely many individuals

----------------------------------------------------------------------------------------

##### Computational Limits

- Perfect rationality - the agent can determine the best course of action
- Bounded rationality - must make a good decision based on its computational capabilities

----------------------------------------------------------------------------------------

##### Learning from Experience

- Knowledge is given
- Knowledge is learned from data or past

----------------------------------------------------------------------------------------

##### Uncertainty 

Two main dimensions for this, sensing and effect, in each dimension an agent can have:

- No uncertainity - knows what is true
- Disjunctive uncertainity - set of states that are possible
- Probabilistic uncertainty - a probability distribution over the worlds

----------------------------------------------------------------------------------------

##### Why probability

- Agents must act even if uncertain
- Predictions are needed to decide
  - Definitive - you willl run out of power
  - Disjunctions: Charge for 30 mins or run out of power
  - Point Probabilities: probability you will run is 0.01 if you charge for 30 minutes else 0.8

- Agents who do not use probabilities, it will lose to those who do
- This can from data and prior knowledge

----------------------------------------------------------------------------------------

##### Sensing uncertainty

- Fully observable - can observe the entire state
- Partially observable - there is a number of states that can be perceived

----------------------------------------------------------------------------------------

##### Effect Uncertainty 

If an agent knew the state and its action could it predict the resulting state

- Deterministic: The resulting case is determined from the actions and the state
- Stochastic: there is uncertainty about the resulting state

----------------------------------------------------------------------------------------

##### Preference

What does the agent try to achieve

- Achievement goal - complex logical formula
- Complex preferences
  - May involve tradeoffs between various
  - Ordinal - only order matters
  - Cardinal - absolute values also matter

----------------------------------------------------------------------------------------

##### Number of agents

Are there multiple reasoning agents that need to be taken into account?

- Single agent reasoning: any other agents are part of the environment
- Multiple agent reasoning: an agent reasons strategically about the reasoning of other agents

Each of these can have their own goals or be independent.

----------------------------------------------------------------------------------------

##### Interaction

When does the agent reason to determine what to do?

- offline - before acting
- online - while interacting

----------------------------------------------------------------------------------------

##### How dimensions interact

- Partial observability makes multi-agent and indefinite horizon reasoning more complex

- Modularity interact with uncertainty and succinctness: some levels may be fully observable, and some partially observable
- Three values of dimensions promise to make reasoning simpler
  - Hierarchical
  - Individuals and relations
  - Bounded rationality

----------------------------------------------------------------------------------------

### Representations of Rational Agents:

![image-20211004111258326](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004111258326.png)

#### What do we want in a representation

- Rich enough to express the knowledge
- As close to the problem as possible - compact, natural and maintainable
- Amenable to efficient computation
  - Able to express features of the problem that can be exploited for computational gain	
  - Able to trade off accuracy and computation time and/or space
- Able to be acquired from people, data and past experiences

----------------------------------------------------------------------------------------

#### Defining a Solution

- Given an informal description of a problem, what is a solution
- Typically much is left unspecified, but unspecific parts cannot be filled in arbitrarily
- Much work in AI is motivated by common-sense reasoning - the computer needs to make common-sense conclusions about the unstated assumptions

----------------------------------------------------------------------------------------

#### Quality of solutions

- Does it matter if the answer is wrong or answers are missing?

**Different classes of solutions:**

- An **optimal solution** is a best solution according to some measure of solution quality. Often unachievable 
- A **Satisficing solution** is one that is good enough, according to some description of which solutions are adequate
- An **approximately optimal solution** is one whose measure of quality is close to the best and theoretically possible
- A **probable solution** is likely to be a solution

----------------------------------------------------------------------------------------

#### Decisions and Outcomes

- Good decisions can have bad outcomes; bad decisions can have bad outcomes
- Information can be of value because it leads to better decisions: value of information
- Often can trade off computation time and solution quality. An anytime algorithm can provide a solution at any time; given more time it can produce a better solution

An agent is not just concerned about finding the right answer, but about acquiring the appropriate information, and computing it in a timely manner.

**Solution quality vs computation time**

![image-20211004112559381](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004112559381.png)

A solution sooner could be better than one which is later. The time discount can be a function of elapsed time for example, which leaves a discounted quality to decide when to take the solution.

----------------------------------------------------------------------------------------

#### Choosing a Representation

We need to represent a problem to solve it on a computer

problem

​	$$\rightarrow$$ specification of problem

​			$$\rightarrow$$ appropriate computation

![image-20211004112809012](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004112809012.png)

For this module, mainly high level specification.

----------------------------------------------------------------------------------------

#### Physical symbol system hypothesis

- A symbol is a meaningful physical pattern that can be manipulated
- A symbol system creates, copies, modifies and destroys symbols

Physical symbol system hypothesis:

- A physical symbol system has the necessary and sufficient for general intelligent action.

----------------------------------------------------------------------------------------

#### Knowledge & Symbol Levels

Two levels of abstraction seem to be common among biological and computational entities

- The knowledge level is in terms of what an agens knows and what its goals are
- The symbol level is a level of description of an agent in terms of what reasoning it is doing

The knowledge level is about the external world to the agent.

The symbol level is about what symbols an agent uses to implement the knowledge level

----------------------------------------------------------------------------------------

#### Mapping from Problem to Representation

- What level of abstraction uses?
- What individuals and relations?
- How can the agen represent the knowledge to ensure that the representation is natural, modular, and maintainable?
- How can an agent acquire the information data, sensing, experience or other agents?

----------------------------------------------------------------------------------------

#### Choosing a level of abstraction

- A higher level is easier for a human to speccify and understand
- A low-level description can be more accurate and more predictive;  high-level descriptions abstract away details that may be important for solving the problem
- The lower the leve, the more difficult to reason with
- You may not know all required information for a low-level description

Sometimes it is possible to use multiple levels of abstraction

----------------------------------------------------------------------------------------

#### Reasoning and acting

Reasoning is the computation required to determine what an agent should do

- Design time reasoning and computation - carried out by the designer
- Offline - done by the agent before it has to act, background knowledge and data
- Online computation - computation that is done by an agent between receiving information and acting

----------------------------------------------------------------------------------------

### Agent Architectures and Hierarchical Control

#### Agent Systems

An agent system is made up of a agent and an environment

- An agent receives stimuli from the environment

- An agent carries out actions in the environment

  ![image-20211004114259967](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004114259967.png)

----------------------------------------------------------------------------------------

#### Agent System Architecture

An agent is made of a body and a controller:

- An agent interacts with the environment through its body
- The body is made up of:
  - Sensors - interpret stimuli
  - Actuators - carry out actions
- The controller receives percepts from the body
- The controller send commands to the body
- The body can also have reactions that are not controlled.

![image-20211004114658813](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004114658813.png)

(Controller is the main focus of the module)

The body itself can have some reactions which aren’t controlled (don’t need any reasoning)

----------------------------------------------------------------------------------------

#### Implementing a controller

- Brains of the agent
- Agents are situatiod in time, they receive sensory data in time, and do actions in time
- Controllers have limited memory and computational abilities
- The controller specifies the command every time
- The command at anu time can depen of the current and previous percepts - can’t store all

----------------------------------------------------------------------------------------

#### Agent functions

- Let $$T$$ be a set of time points
- A **percept trace** is a sequence of all past, present and future percepts received by the controller
- A **command trace** is a sequence of all past, present, and future command output by the controller
- A **transduction** is a function from percept traces into command traces
- A transduction is **causal** if the command trace up to time $$t$$ depend only on percepts up to $$t$$
- A **controller** is an implementation of a causal transduction
- An agents **history** at time $$t$$ is a sequence of past and present percepts and past commands
- A causal transduction specifies a function from an agent’s history at time $$t$$ into its action at time $$t$$.

Despite the fact that a casual transduction is a function of an agent’s history, ti cannot be directly implemented, because an agent does not have direct access to its entire history.

----------------------------------------------------------------------------------------

#### Belief States

- An agent does not have access to its entire history, only what is remembered
- The **memory** of **beleif state ** of an agent at time $$t$$ encodes all of the agent’s history that is has access to
- The belief state of an agent encapsulates the information about its past that is can use for current and future actions
- At every time a controller has to decide on:
  - What should it do?
  - What should it remember? - How is memory  updated, most recent decisions or most significant?
  - [As a function of its percepts and its memory].
- An example of a belief state could be: The belief state for an agent that is following a fixed sequence of instructions may be a program counter that records its current position in the sequence.
- A belief state can contain specific facts that are useful (where a delivery robot  leaves a parcel).
  - If is often useful to remember anything that is reasonably stable and cannot be immediately observed.
- The belief state could be a representation of the dynamics of the world and the meaning of its percepts, and the agent could use its perception to determine what is true in the world

**A Belief state transition function** for discrete time is a function:

$$remember : S \cross P \rightarrow S$$ 

Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts. $$s_{t+1} = remember(s_t,p_t)$$ means that $$s_{t+1}$$ is the beleif state following beleif state $$s_t$$ when $$p_t$$ is observed.

**A command function** is a function:

$$do: S \cross P \rightarrow C$$

Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts, and $$C$$ is the set of possible commands. $$c_t=do(s_t,p_t)$$ meant that the controller makes that command when seeing the states and percepts: $$s_t,p_t$$

----------------------------------------------------------------------------------------

#### Controller



![image-20211004115640946](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004115640946.png)

- If there exist a finite number of belief states, the controlelr is called a **finite state controller** or a **finite state machine**
- If there exist a finite number of features and each feature can have a finite number of possible values, the controller is a **factored finite state machine**

#### Functions implemented in a Controller



![image-20211004115713697](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004115713697.png)

For a discrete time, a controller implements:

- **Belief state function:** remember(belief state, percept) returns the next belief state [Blue]

  - **A Belief state transition function** for discrete time is a function:

    $$remember : S \cross P \rightarrow S$$ 

    Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts. $$s_{t+1} = remember(s_t,p_t)$$ means that $$s_{t+1}$$ is the beleif state following beleif state $$s_t$$ when $$p_t$$ is observed.

- **Command Function**: do (memory, percept) returns the command for the agent [Red]

  - **A command function** is a function:

    $$do: S \cross P \rightarrow C$$

    Where $$S$$ is the set of belief states, $$P$$ is the set of possible percepts, and $$C$$ is the set of possible commands. $$c_t=do(s_t,p_t)$$ meant that the controller makes that command when seeing the states and percepts: $$s_t,p_t$$

----------------------------------------------------------------------------------------

#### Agent Architectures

You do not need to implement an agent viewed as:

​															Perception $$\rightarrow$$ Reasoning $$\rightarrow$$ Action

As three independent modules feeding into the next:

- Too slow
- High level strategic reasoning takes more time than the reaction time needed to avoid obstacles
- The output of the perception depends on what you will do with it.

----------------------------------------------------------------------------------------

#### Hierarchical Control

- A better architecture is a **hierarchy of controllers**
- Each controller sees the controllers below at as a **virtual body** from which it get percepts and sends commands
- The lower-level controllers can
  - Run much faster, and hence react to the world more quickly
  - deliver a much simpler view of the world to the higher level controllers

**Hierarchical Robotics System Architecture**

![image-20211004125049935](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125049935.png)

----------------------------------------------------------------------------------------

#### Functions Implemented in a Layer

In general, there can be multiple different features passed from layer to layer and between states. There are three types of **inputs** to each layer at each time:

- The features that come from the belief state, which are referred to as the remembered or previous values of the features
- The features representing the percdepts from the layer below in the hierarchy
- The features representing the commands from the layer above in the hierarchy

There are three types of **outputs** from each layer at each time

- The higher level percepts for the layer above
- The lower level commands for the layer below
- The next values for the belief state features

![image-20211004125224283](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125224283.png)

- memory function: remember (memory, percept, command) - [Blue]
- command function: do(memory, percept, command) - [Red]
- percept function: higher percept(memory, percept command) - [Black]

----------------------------------------------------------------------------------------

#### Qualitative Versus Quantitative Representations:

Much of science an engineering considered **quantitative reasoning** with numerical quantities, using differential and intergral calculus as the main tools. **Qualitative reasoning** is reasoning often using logic about qualitative distinctions rather than numerical values.

**Qualitative reasoning** is important for:

- An agent may not know what the exact values are. Take a coffee pouring robot which may not know what the optimal angle is to pour a coffee, but a control rule may suffice to fill up the cup suitably.
- Reasoning may be applicable regardless of the quantitative values. You may want a kettle to heat up water so long as it is within specific parameters: $$\leq 100 \degree C$$ 
- An agent needs to do qualitative reasoning to determine which quantitative laws are applicable.

**Quantitative reasoning** is important for

- Landmarks - values which make qualitative distrinctions in the individual being modelled. Taking a coffee cup example, some qualitative distinctions include whether the cofee cup is empty or full. Which can then be used to determine what happens if the coffee cup is dropped for example.
- Orders of magnitude reasoning - approximate reasonign that ignores minor distinctions
- Qualitative derivative - indicate whether some value is increasing, descreasing or staying the same.

A flexible agent needs to do qualitative reasoning before it does quantitative reasoning. Sometimes qualitative reasoning is all that is needed. Thus, an agent does not always need to do quantitative reasoning, but sometimes it needs to do both.





#### Example - Delivery Robot

- Three actions: straight, right, left
- Can be given a **plan** consisitng of a set of names locations for the robot to go to in turn
- Robot must avoid obstacles
- It has a single **whisker sensor** pointing forward and to the right.
  - The robot can detect if the whisker hits an object
  - The robot knows where it is
- The object and locations can be moved dynamically - obstacles and new locations can be created dyncamically.

##### **Decomposition of the delivery robot**

![image-20211004125751159](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004125751159.png)

- Highest Level:  follow the plan, takes a plan as an Input from an external source, chooses how to follow the plan. The middle layer acts as an interface between the two outer layers.
- Middle Level: Takes the information - how to avoid obstacles, passes steering input back to the lower level.
- Lowest Level: takes information from the environment, steers the robot, find obstacles and position.

----------------------------------------------------------------------------------------

#### What Should be in an Agent’s Belief State?

- An agent decides what to do based on its belief state and what it observes
- A purely **reactive** agent does not have a belief state, a **dead reckoning** agent does not perceive the world, - neither of these work well in complicated situations - not for the deliver robot example.
- Often it is useful for the agent’s belief state to be a model of the world (itself and the environment which it is present)

----------------------------------------------------------------------------------------

#### Agents Modelling the World

- Belief state definition is often very general and doesn’t constrain what should be remembered.
- A model of a world is a representaion of the state of the world - regardless of how accurate.
- One method is to maintain its beleif about the world and to update this based on its commands.
  - Requires both the state of the world, and the dynamics of the world. Given these, the state at the next time can be predicted.
- When the world is dynamic, or noisy actuators (say a wheel slips), these inconsitencies can accumulate and become very significant.

Another approach is for the agent to use perception to build a model of the relevant part of the world. This tends to be ambiguous and noisy - in a 3D world, it is very difficult to determine the world from one image or one scan. 

A way to improve on this is to:

- If both noise of forward prediction and sensor noise are modelled, the next belief state can be estimated using Bayes’ rule (filtering)
- With more complicated sensors, a model can be used to predict where visual features can be found, then vision can be used to look for these features close to the predicted location - makes the task simpler, and reduces the errors in position arising from forward prediction.

A control problem is separable if the best action can be found by first finding the best model of the world, then using that to determine the best action. (Most world problems are not separable)

-------------------

### Agent Functions

- An agent can be viewed as being specified by the **agent function**  mapping percept sequences to actions $$f:P \rightarrow A$$
- **Ideal rational agent:** does whatever action is expected to maximise performance measure on basis of percept sequence and built-in knowledge
- In principle, there is an **ideal mapping** of percept sequences to actions corresponding to ideal agent
- Simple approach - lookup table:
  - Table too large
  - Time to build
  - No autonomy
- Lookup table suggests an “ideal mapping”
- One agent function (or a small equivalence class) is **rational** can be seen to approximate ideal mapping
- The aim: - find a way to implement the rational agent function
- Need to implement this concisely, and the implementation must be:
  - Efficient
  - Exhibit autonomy [if required]
  - Get as close as possible to the ideal mapping

----------------------------------------------------------------------------------------

#### Embedded and Simulated Agents

There are multiple ways an agent’s controller can be used:

- An **embedded agent** is one that is run on the real world, where actions are carried out in a real domain and the sensing comes from a domain.
- A **simulated agent** is one that is run with a simulated body and environet - wheree a program takes in the commands and returns approximate percepts. This is often used to debug a controller before its deployment.
- An **agent system model** is where there are models of the controller, the body, and the environment which can answer questions about how the agent will behave. This can be used to prove properties of agents before they are build, or to use hypothetical questions about an agent that may be difficult or dangerous to answer with a real agent.

These are useful for different purposes:

- Embedded mode is how the agent must run to be useful.
- A simulated agent is useful to test and deub the controller when many design options must be explored, where building a design is expensive, or then environment is unaccessible.
  - Models must always abstract some information, therefore depending on the sensitivity of the agent, this could mean that as much detail is required as possible to determine the effectiveness of the agent.
- A model of the agent, a model of the set of possible environment, and a specification of the correct behaviour allows us to prove theorems about how the agent will work in such environment. 
- Given a model of the agent and the environment, some aspects of the agent can be left unspecified and can be adjusted to produce the desired optimal behaviour. this is the general idea behind optimization and planning.
- In reinforcement learning, the agent improves its performance while interacting with the real world.

-------------------

#### Acting with Reasoning

- Previously, we assumed than an agent has some belief maintained through time. For intelligent agents, this can become extremely complex.
- An intelligent agent requires some internal representation of its own belief state.
  - **Knowledge** is the information about a domain that is used for problem solving within that domain
  - This can include general knowledge applied to particular situations.
  - Therefore, it is more general than the beliefs about a specific state.
- A knowledge based system is a system that uses knowledge about a domain to act or solve problems.

Knowledge tends to mean general information that is taken to be true.

Belief tends to be information that can be revised based on new information. This often comes with measures of how much they should be believed and models of how the beliefs interact:

- In an AI system, knowledge is typically not necessarily true, and is justified as only being usefly. This can become blurry when one module of an agent reats information as true, but another may be able to revise that information.

- The image below shows a model for a knowledge based agent
  - A **knowledge base** is build offline and is used online to produce actions. This is orthogonal to the layered view of an agent - an intelligent agent requires both hierarchical organisation and knowledge bases



- **Online**, when the agent is acting, it uses its knowledge base, its observations of the world and its goals and abilities to choose what to do, and how to update its knowedge base.
  - This is its long term memory, where it keeps information that is needed to act in the future.
  - This comes from proior knowledge, and is combined what is learned from data and past experiences
  - The **beleife state** is the short term memory, which  maintains the model of the current environment needed between time steps
  - (There is not always a specific distinction between general and specific knowledge)
- **Offline** before the agent must act, it can build the knowledge base that is useful for it online.
  - The role of offline computation is to make online computation more efficient
  - The knowledge is built on prior knowledge, and from datat of past experiences (not what is learned from data) - either its own experience or what it has been given.
  - In most domains, the agent must se whatever information is available, and therefore requires both rich prior knowledge, and lots of data.

The goals and abilities are given online, offline, or both depending on the agent. The online computation can be made more efficient if the knowledge base is tuned for the particular goals and abilities, however this is often not possible when goals are determined at runtime.

![image-20211008110950107](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211008110950107.png)

-----------------

##### Design Time and Offline Computation

The knowledge base required for online computation can be built initially at design time and then augmented offline by the agent.

An **ontology** agent is a specification of the meaning of the symbols used in an information system. 

- This specifies what is being modeled and the vocabulary used in the system
- In simplest form, if an agent is using explicit state-based representation with full observability, the ontology specifies the mapping between the world and state.
  - Without this, the agent may know its state, but without the ontology will now know what to do.
- In other cases, the ontology defines the featrues of the individuals or relationships. This is what is needed to convert raw sense data into something meaningful for the agent 
- Ontologies are built by communities - independently of a particular knowldge base or application. It is this what allows for effective communication and interoperation of the data from multipe sources.

An ontology typically comes before the data and the prior knowledge - we require an ontology to have data or to have knowledge. Without this, data are just sequences of bits. The ontology often evolves as the system is developed.

- It may also specify a level or levels of abstraction. If the ontology changes - the data must change

The knowledge base is typically build offline from a combination of expert knowledge base and data. Usually this is built before the agent knows teh particulars of the environment.

##### Roles of Offline Computation 

- Software engineers - build the inference engine and the UI. They typically know nothing about the contents of the knowledge base. No need to be experts in the use of the system they implement, but must be experts in the language they use.
- Domain experts - the people who have the appropriate prior knowledge about the domain. They know about the domain, but typically know nothing about the particular case that is being considered. 
  - They do not typically know about the internal workings of the AI system. They only have a semantic view of the knowledge and no notion of the algorithms used by the engine. The sytem should interact with them in terms of the domain.
- Knowledge engineers - design, build, and debug the knowledge base in consolation with domain experts. They know the details of the system and about the domain through the domain expert. Nothing is knows about any particular case. Should know useful inference techniques and how the complete system works.

![image-20211008115039198](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211008115039198.png)

##### Roles of Online Computation

Online, the information about particular cases becomes apparent and the agent must act. Online, the following roles are involved.

- User - a person who has need for expertise or has information about individual cases.
  - Typically not experts in the domain of the knowledge base.
  - Often do not know what information is needed by the system
  - Unreasonable to expect them to vlunteer infrmation about a particular case
  - A simple interface must be provided, as users typically do not understand the internal structure of the system.
  - They often must make an informed decision based on the recommendation of the system (along with an explanationas to why)
- Sensors provide information about an environment.
  - Passive sensors - continuously feeds information to the agent. 
  - Active sensors - controlled or asked for information.
  - Sensors that are passive sensors may be seen as active sensors at a higher levels of abstraction
- External knowledge sources - web sites or a database can be asked questions and provide the answer for a limited domain
  - Asking a weather website for the weather
  - The interface between an external knowledge source and an agent is called a wrapper;
  - A wrapper translates between the representation the agent uses, and the queries the external knowledge source is prepared to handle
  - When website and databases can be used together, they can be used together because the same symbols have the same meaning
  - This is called **semantic interoperability**







#### Agent Types

Find fundamental types of agents:

- Simple reflex agents

  $$\rightarrow$$ condition action rules - essentially if, else

- Reflex agents with state

  $$\rightarrow$$ retains knowledge about the world

- Goal based agents

  $$\rightarrow$$ has a representation of which states are desirable

- Utility based agents

  $$\rightarrow$$ able to discern some useful measure (cost or quality for example), between different possible means of achieving a certain state

- Learning agents

  $$\rightarrow$$ able to modify their behaviour based on their performance, or given new information.

----------------------------------------------------------------------------------------

#### Simple Reflex Agents

- Based on a set of condition-action rules
- We can view this as summarising the notional lookup table
- Although simple, reflex agents can still achieve relatively complex behaviour
- Learning of new rules possible
- Very quick and efficient

![image-20211004131536572](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004131536572.png)



- The environment is outside the box,

-The sensors feed information to the agent, and decides based on the current state (no history).

----------------------------------------------------------------------------------------

#### Reflex agents with State / Model based Reflex Agents

- This includes the current state with the memory
- Still takes the current state from the environment
- How you update beliefs changes how the world appears to be

![image-20211004131801447](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004131801447.png)

- Pure reflex agents only work if the “correct” decision of what to do can be made on the current percept
- Typically need some internal state to track the environment
- The update state function uses knowledge about how the world evolves and how actions affect the world, a different way of deciding what to remember changes how the world will be perceived
- Agents can use this information to track unseen parts of the world

----------------------------------------------------------------------------------------

#### Goal Based Agents

- For more complex problems
- Still takes in the state and history

![image-20211004132048605](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132048605.png)

- Knowing current environment state not normally enough to choose action - need to know what is trying  to be achieved
- Can combine information about the environment, goals, and effects of actions to choose what to do
- Relatively simple if goal is achievable in single action
- Normally, this is not the case, and the goal required a sequence of actions - use search or planning
- Much more flexible than a reflex agent

----------------------------------------------------------------------------------------

#### Utility Based Agents

- Initially look similar to goal agents
- Think about what benefit each action has

![image-20211004132411310](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132411310.png)

- Often different ways to achieve a goal, some desirable some less so:
  - Might achieve free space on hard drive
- Can use a **utility function** to judge resulting states of the world
- Enables a choice about which goals to achieve - select the one with the highest utility
- If achievement if uncertain we can measure the importance of the goal against the likelihood of achievement

----------------------------------------------------------------------------------------

#### Learning Agents

- Can be build on the foundations of the previous four
- The previous four fit in the **performance element** of the agent.

![image-20211004132737788](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004132737788.png)

- Adds three components:
  - critic
  - learning element
  - problem generator
- Learning element 0 changed the performance element
  - Takes information about how the performance element is operating and changes that
  - Find information on how it is doing
  - Generates learning goals
- If the agent designer has incomplete information about the environment - an agent needs to be able to learn
- Learning provides an agent with independence or autonomy - behaviour determined by its own experience
- **Idea**: - percepts ca be used to improve agent’s ability to act in the future (as well as for choosing an action now)
- Learning results from interaction between the agent and the world, and observation by agent of its decision making

----------------------------------------------------------------------------------------

##### Learning Agent Components

Typically, this has four conceptual components:

- **Learning element:** responsible for making improvements - takes knowledge of the performance element and feeds back on the agent’s success, then suggests improvements to the performance element

- **Performance Element:** responsible for acting - take percepts and returns actions (in non learning agents this is considered to be the whole agent)
- Design of the learning element dependent on design of the performance element:
  - Learning algorithms for improving logical inference is different to those for improving decision-theoretic agents
- **Critic:** tells the learning agent how well the agent is doing, using a **performance standard**
  - The performance standard should be a fixed measure and external to the agent
- **Problem Generator:** responsible for suggesting actions in pursuit of new (and informative depending on the situation) experiences
  - Without the problem generator, the performance element would always do what it thinks is best. However, performing a “suboptimal” action may lead to discovery of better actions in future (route finding).

----------------------------------------------------------------------------------------

## Problem Solving and Uninformed Search

- Problem Solving agents
- Problem types
- Problem formulation
- State space graph
- Basic tree and graph search

----------------------------------------------------------------------------------------

### Problem Solving

- Often not given an algorithm to solve a problem, but a specification of what a sultion is - a solution must be found
- A typical problem is when the agent is in one state, it has a set of deterministic actions it can carry out, and wants to get to a goal state
- Many AI problems can be abstracted into the problem of finding a path in a directed graph
- Often there is more than one way to represent a problem as a graph.



##### Problem solving as Search

The idea of search is straightoforward: the agent contructs a set of potential partial solution to a problem that can be checked to see if they are truyly solutions, or if they could lead to solutions. This proceeds by repeatedly selecting a partial solution, stopping if it is a path to a goal, and otherwise extending it by one more arc in all possible ways.

- This underlies much of AI: “When an agent is given a problem, it is usually given only a description that lets it recognize a solution, not an algorithm to solve it. It has to search for a solution.” - Book

----------------------------------------------------------------------------------------

#### Problem Solving: Example

- Suppose you are in Cov and goal is getting to London
  - How is this achieved, which method of transport
  - Do you go via somewhere else
- A computer needs to know which of these is bad and which is good
- Finding a sequence of actions that would result in you being in london is **problem solving**

A **problem solving agent** is a goal-based agent that will determine sequences of actions that lead to desirable states

----------------------------------------------------------------------------------------

#### **Four Problem Solving Steps**

- **Goal Formulation:** identify a goal given the current situation
- **Problem Formulation:** identify permissible actions (or operators), and the states to consider
- **Search:** find the sequence of actions to achieve the goal
- **Execution:** perform the actions in the solution

**Two types of problem solving**

- **Offline** - complete knowledge of problem and solution (Coventry example - assuming no external issues)
- **Online** - involves acting without complete knowledge of problem and solution.



#### A simple Problem Solving Agent

----------------------------------------------------------------------------------------

**BEGIN ALGORITHM** 

**function** $$SimpleProblemSolvingAgent(p)$$ - *This is offline* 

​		**returns** $$action$$ 

----------------------------------------------------------------------------------------

​	**Inputs:** $$p$$, as percept

​	**Static**: $$s$$, an action sequence, initially empty

​               $$state$$, a description of the current world state: 

​		    	$$g$$ a goal, initially null

----------------------------------------------------------------------------------------

​	$$state \leftarrow$$ $$UpdateState(state,p$$) 

​	**if** $$s = \emptyset$$ **then**

​		$$g \leftarrow$$ $$FormulateGoal(state)$$

​		$$problem \leftarrow FormulateProblem(state,p)$$

​		$$s \leftarrow Search(problem)$$ 

​	$$action \leftarrow Recommendation(s,state)$$

​	$$s \leftarrow Remainder(s,state)$$

​	**return** $$action$$

**END ALGORITHM**

----------------------------------------------------------------------------------------

##### Dimensions of (typical) State-space Search

| **Dimension**        | **Values**                                                   |
| -------------------- | ------------------------------------------------------------ |
| Modularity           | flat                                                         |
| Representation       | states                                                       |
| Planning Horizon     | indefinite stage - most complex part, finite number of steps, don’t know how many |
| Sensing uncertainty  | fully observable                                             |
| Effect uncertainty   | deterministic                                                |
| Preference           | goals                                                        |
| Number of agents     | single                                                       |
| Computational limits | perfect rationality - assumption                             |
| Interaction          | offline                                                      |

- This is a simplistic problem (almost all are simplest)

-----------------

#### Problem Types	

- Deterministic, fully observable $$\rightarrow$$ **single-state problem**
  - Sensors tell the agent current state and knows exactly what its actions do
  - Means it knows exactly what state it will be in after any action
- Deterministic, partially observable $$\rightarrow$$ **multi-state problem**
  - limited access to state, but knows what actions do
  - can determine a set of states resulting from an action
  - instead of manipulating single states, agent must manipulate sets of states
- Stochastic, partially observable $$\rightarrow$$ **contingency problem**
  - don’t know current state, and don’t know what state will result from action
  - must use sensors during execution
  - solution is a **tree** with branches for contingencies
  - often **interleave** search and execution
- Unknown state space - knwoedlge is leaned $$\rightarrow$$ **exploration problem** (online)
  - agent doesn’t know what its actions will do

---------------

#### Vacuum Cleaner World

- Simple examle environment
- Vacuum cleaner robot, in a world with just two locations
- Each location may or may not contain dirt
- Agent in one of the locations
- Agent can move left or right, or such up dirt
- Example State:![image-20211004141624238](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004141624238.png)

- Single state: (fully observable)  - move to the right and solve

- Multiple state (partially observable) start in any of $$\{1,2,…,8\}$$
  - Right will take you to $$\{2,4,6,8\}$$
- Contingency (partially observable) start in 5:
  - Solution $$ =$$ [right, **if** dirt **then** suck]

----------------

#### Problem Formulation: State-space problems

State space searching **assumes** that:

- The agent has perfect knowledge of the state space and can observe what state it is in
- The agent has a set of actions that have known deterministic effects.
- Some states are goal states, that the agent wants to reach one of these goals, and can recognise a goal
- A solution is a sequence of actions that will get the agent from its current state to a goal state

A **state-space problem** consists of:

- a set of states
- a subset of states called the start states
- a set of actions
- an **action function:** given a state and an action, returns a new state
- a set of goal states, specific as function, $$goal(s)$$ 
- a criterion that specifies the quality of an acceptable solution (possibly), maybe any solution within 10% optimality is acceptable



##### Selecting a State Space

The real world is very complex - state space may need to be **abstracted**

- Abstract state $$\approx$$ set of real states
- Abstract operations $$\approx$$ compbination of real actions e.g. Coventry $$\rightarrow$$ Warwick comprises all possible routes.
- Abstract solution $$\approx$$ a set of paths that exist in the real world.

##### Example: The 8 puzzle

- States? locations of tiles
- Operators? move: up, down, left, right,
  - Another operator is moving the blank tile up and down, is equivalent  (this is better)
- Goal Test? given goal state
- Path cost? 1 per operator

##### State space for Vacuum world

- States? integer dirt and vacuum locations
- Operators? left, right, suck
- Goal test? no dirt
- Path cost? 1 per operator

![image-20211004142938938](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004142938938.png)



-------------------

#### State Space Graphs

A general formulation of a problem solving problem is a  **state space graph**

- A (directed) **graph** consisting of a set $$N$$ of **nodes** and a set $$A$$ of ordered pairs of nodes, called **arcs** or **edges**

- Node $$n_{2}$$ is a **neighbour** of $$n_{1}$$ if there is an arc from  $$n_{1}$$ to  $$n_{2}$$. 

  **If** $$\langle n_{1},n_{2}\rangle \in A$$

- A **path** is a sequence of nodes $$\langle n_{0},n_{1}, ... , n_{k}\rangle : \langle n_{i-1},n_{i}\rangle \in A$$

- A **cycle** is a non-empty path such that the end node is the same as the start node: $$\langle n_{0},n_{1}, ... , n_{k}\rangle : n_0 = n_k, k\neq 0$$

- A directed graph without any cycles is called a Directed Acyclic Graph **(DAG)**

- A tree is a DAG where there is one node with no incoming arcs and every other node has exactly one incoming arc. The node with no incoming arcs is called the root of the tree, and nodes with no outgoing arcs are called leaves

- The **length** of the path  $$\langle n_{0},n_{1}, ... , n_{k}\rangle$$ is $$k$$

- Given a set of **start nodes** and **goal nodes**, a **solution** is a path from a start node to a goal node.

Sometimes, there is a cost associated with arcs. This is written as $$cost(\langle n_i,n_j \rangle)$$, which consequently induces a cost of paths:

- Given a path $$p= \langle n_{0},n_{1}, ... , n_{k}\rangle$$, the cost of that path is the sum of the costs of all the arcs in the path.
- An optimal solution is one of the least cost, such that there is no path $$p’:cost(p')<cost(p)$$  

------------------------

Often, this graph is not given explicitly, it is dynamically constructed as needed. All that is needed for the search algorithms that follow is a way to generate the neighbours of a node and to determine if a node is a goal node.

- **Forward Branching Factor** the number of arcs leaving the node
- **Backward Branching Factor** the number of arcs entering the node

These measure provide complexity of graphs, and when using time complexity, these are intended to be constant. They are very important,as they are key components of the size of the graph

### Basic Tree Search and Graph Search Algorithms

#### Tree Search Algorithm

- Offline simulated exploration of the state space
- Starting with a start state expand one of the explored states by generating its successors to build a search tree.

---------------------

**BEGIN ALGORITHM**

**function** $$TreeSearch(problem,strategy)$$

​		**returns** $$solution | | failiure$$ 

​		Initialise search tree using initial state of $$problem$$

​		**loop do**

​				**if** no candidates for expansion **then return** $$failiure$$ 

​				choose leaf node for expansion according to $$strategy$$

​				**if** node contains a goal state **then**

​						**return** corresponding $$solution$$ 

​				**else** expand node and add resulting nodes to search tree

**END ALGORITHM**

----------------

 ##### Implementing Tree Search

- A State represents a physical configuration
- A node is a data structure comprising part of search tree
- Node: state, parent, children, depth, path cost
- Nodes waiting to be expanded called the **frontier**
- Represent frontier as a queue

(Variable $$g$$ typically used to represent the current **cost**)

------------------

##### Search Strategies

- Strategy defined by **order or node expansion**
- Evaluation along several dimensions
  - **completeness:** always find solution (if exists)?
  - **optimality:** always find the least-cost solution?
  - **time complexity:** number of nodes expanded
    - maximum depth branching factor **$$b$$** 
    - depth of least cost solution $$d$$ 
    - maximum depth of state $$m$$ (could be $$\infty$$)
  - **space complexity:** maximum number of nodes in memory (measured in terms of $$b,d,m$$)

---------------------------

##### Simple Uninformed Tree Search Strategies

**Uninformed** search only uses information from the problem definition

Two Examples are:

- Breadth-first
- Depth-first

-------------------------

###### Breadth-First Tree Search   

- Complete - always finds solution if finite
- Time
  - $$1 + b + b^2 + b^3 + … + b^d = O(b^d)$$  - technically $$O(b^{d+1})$$ because check for goal state occurs when the node is selected for expansion rather than when generates - all nodes at depth $$d$$ are expanded
- Space
  - Each leaf node is kept in memory: $$O(b^{d-1})$$ explored and $$O(b^d)$$ in frontier, so complexity is $$O(b^d)$$
- Optimal
  - If cost is **non-decreasing** function of depth - operators equal cost then yes. Otherwise not
- **Space** is the main problem

**Time and memory requirements:**

($$b=10, 10,000$$ nodes/sec; $$1000$$ bytes /node)

| Depth | Nodes     | Time        | Memory        |
| ----- | --------- | ----------- | ------------- |
| 2     | 11,000    | .11 sec     | 1 megabyte    |
| 4     | 111,100   | 11 sec      | 106 megabytes |
| 6     | $$10^7$$ | 19 min      | 10 gigabytes  |
| 8     | $$10^9$$ | 31 hours    | 1 terabyte    |
| 10    | $$10^{11}$$ | 129 days    | 101 terabytes |
| 12    | $$10^{13}$$ | 35 years    | 10 petabytes  |
| 14    | $$10^{15}$$ | 3,523 years | 1 exabyte     |

- Each depth increases exponentially.
- Can solves the BFS tree in 5 moves.

---------------------------------

###### Depth-first Tree Search

- Expand deepest unexpanded node
- $$QueueingFn = $$ insert successors at front of queue (LIFO)
- Incomplete - fails in infinite depth, spaces with loops
- Change search to avoid loops $$\Rightarrow$$ complete in finite spaces
- Time
  - $$O(b^m$$) terrible if $$m$$ is much larger than $$d$$. If solutions are dense may be much faster than BFS
- Space
  - $$O(bm)$$ linear space, store the path from the root to leaf an unexpanded system of nodes.
- Not Optimal.

--------------------------------

### Graph Search Algorithms

- With tree search, state spaces with loops give rise to repeates states that cause inefficiencies, and complete tree is infinite
- Practical way ot exploring the state space that can account for the repetitions
  - Given a graph, start nodes, and goal nodes, incrementally explore paths from the star nodes
  - Maintain a **frontier** of paths from the start node that have been explored
  - As search proceeds, the frontier of paths from the start node that have been explored
  - The way in which the frontier is expanded defines the **search strategy**

![image-20211004153843019](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211004153843019.png)

- Nodes marked in red are the frontier, gradually expanding outwards

-------------------

#### Graph Search Algorithm

- Which value is selected from the frontier at each stage defines the search strategy
- The neighbours define the graph
- $$goal$$ defines what is a solution
- If more than one answer is required, the search can continue from return call
- We can modify to return all possible solutions

--------------

**BEGIN ALGORITHM**

**Input:** a graph

​			 a set of nodes

​			 Boolean procedure $$goal(n)$$ that test if $$n$$ is a goal node.

-----------

$$frontier := \{\langle s\rangle : s$$ is a start node$$\}$$

**while** $$frontier \neq \emptyset$$

​		**select** and **remove** path $$\langle n_0,…,n_k \rangle$$ from $$frontier$$

​		**if** $$goal(n_k)$$

​				**return **$$\langle n_0,…,n_k \rangle$$

​		**for each** neighbour $$n$$ of $$n_k$$ 

​				**add** $$\langle n_0,…,n_k, n \rangle$$ to $$frontier$$

**end while**

**END ALGORITHM**

--------------

#### Breadth First Graph Search

- Treats the frontier as a queue
- It always selects one of the earliest elements added to the frontier
- If the list of paths on the frontier is $$[p_1,p_2,….,p_r]$$:
  - $$p_1$$ is selected and its neighbours are added to the end of the queue, after $$p_r$$ 
  - $$p_2$$ is selected next



**Complexity of Breadth-first** Graph Search

- If the branching factor for all nodes is finite, BF graph search is garunteed to find a solution if one exists - it is garunteed  to find the path with fewest arcs
- Time complexity is exponential in the path length: $$b^n$$, where $$b$$ is branching factor, $$n$$ is path length
- Space complexity is exponential in path length: $$b^n$$
- The search is unconstrained by the goal

--------------

#### Depth First Graph Search

- Depth-first search treats the frontier as a stack
- It always selects one of the last elements to the frontier
- If the list of paths on the frontier is $$[p_1,p_2,…]$$:$$p_1$$ is selected and the paths that extend $$p_1$$ are added to the front of the stack (in front of $$p_2$$)
- $$p_2$$ is only selected when all paths from $$p_1$$ have been explored

**Complexity of DFS**

- DF graph search is not guaranteed to halt on infinite graphs or on graphs with cycles

- The space complexity is linear in the number of arcs from the start to the current node

  (Supposing $$\langle n_0,…,n_k \rangle$$ is selected from the frontier. In DFS every other path on the frontier has the form: $$\langle n_0,…,n_k, n \rangle$$ for some index $$i < k$$ and some node $$m$$ that is a neighbour of $$n_i$$ . Therefore it follows the selected path for a number of arcs and has one extra node. So, the frontier has the current path and paths to neighbours of the nodes on this path. There can be at mode $$k(b-1)$$ paths on the frontier, and so space is linear in the number of arcs from the start to the current node)

- If the graph is a finite tree, with a forward branching factor $$\leq b$$, and all paths from the start have at most $$k$$ arcs, worst case time complexity is $$O(b^k)$$ 
- The search is unconstrained by the goal

--------------

#### **Lowest-Cost-First Search**

- Sometimes there are costs associated with arcs
- The cost of a path is the sum of the costs of its arcs

$$
cost(\langle n_0,...,n_k\rangle) = \sum_{i=1}^kcost(\langle n_{i-1},n_i\rangle)
$$

​	An optimal solution is one with minimum cost

- At each stage, LCFS selects a path on the frontier with lowest cost
- The frontier is a PQ ordered by path cost
- The first path to a goal is a least-cost path to a goal node
- When arc costs are equal $$\Longrightarrow$$ BFS

------------

#### Summary of Graph Search Strategies (as of yet)

| **Strategy**      | Frontier Selection | Complete | Halts | Space  |
| ----------------- | ------------------ | -------- | ----- | ------ |
| Breadth-First     | First node added   | Yes      | No    | Exp    |
| Depth-First       | Last node added    | No       | No    | Linear |
| Lowest-Cost-First | Minimal $$cost(p)$$ | Yes      | No    | Exp    |

**Complete** - guaranteed to find a solution if there is one (for graphs with finite number of neighbours, even on infinite graphs)

**Halts** - on finite graph (perhaps with cycles)

**Space** - as a function of the length of current path

---------------

------------------------

## Informed Search

### Overview

Uninformed search is generally very inefficient; if we have extra information about the problem then it is good to use it

- Improve the search using specific knowledge - **informed search**

- This is still offline search

-------------

### Heuristic Search

- Idea: don’t ignore the goal when selecting paths
- Often there is extra knowledge that can be used to guide the search: heuristics
- $$h(n)$$ is an estimate of the cost of the shortes path from node $$n$$ to a goal node
- $$h(n)$$ needs to be efficient to compute
- $$h$$ can be extended to paths: $$h(\langle n_0,…,n_k \rangle)=h(n_k)$$ 
- $$h(n)$$ is an **underestimate** if there is no path from $$n$$ to a goal with cost strictly less than $$h(n)$$  - typically want there to be an underestimate
- An **admissible heuristic** is a nonnegative heuristic function that is an underestimate of the actual cost of a path to a goal.

---------------

#### Example Heuristic Function

- If nodes are points on a Euclidean plane, and the cost is the distance $$h(n)$$ can be the straight line distance from $$n$$ to the closest goal
- A heuristic function can be found by solving a simpler version of the problem

----------------

### Best-First Search

- We can use the heuristic function to determine the order of the stack representing the frontier
- **Idea** select the path or node that is closest to a goal according to the heuristic function
- Heuristic DFS select a neighbour so that the best neighbour is selected first
- Greedy best first search selects a path on the fronteir with the lowest heuristic value
- Best-First search treats the frontier as a PQ ordered by $$h$$

![image-20211011084452241](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011084452241.png)

- The graph above is bad for simple BFS: heurist DFS will select the node below $$s$$ and never terminate. Greedy best-first seach will cycle between the node below $$s$$ never finding an alternative route

-------------------

#### Complexity

- Space is exponential in path length: $$b^n$$, where $$b$$ is the branching factor, and $$n$$ is the path length
- Time complexity is exponential in $$b^n$$
- Not guaranteed to fund a solution
- Does not always find the shortest path.

-------------

### A* Search

- uses path cost and heuristic value
- $$cost(p)$$ is the cost of path $$p$$ - referred to as $$g(p)$$
- $$h(p)$$ estimates the cost from the end of $$p$$ to a goal



Let $$f(p) = g(p) + h(p)$$

- $$f(p)$$ estimates the total path cost of going from a start node to a goal via $$p$$

![image-20211011085000129](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011085000129.png)

-----------------

#### Algorithm

- Is a mix of lowest cost-first and best-first search
- It treats the frontier as a PQ ordered by $$f(p)$$
- It always selects the node on the frontier with the lowest estimated distance from the start to a goal node constrained to go via that node

--------

#### Admissibility of A*

- A searhc algorithm is admissible, if whenever a solution exists, it returns an optimal solution
- A* is admissible if:
  - The branching factor is finite
  - Arc costs are bounded above 0 - some $$\epsilon$$
  - $$h(n)$$ is nonnegative and an underestimate of the cost of the shortest path from $$n$$ to a goal node

Why is A* admissible

- If a path $$p$$ to a goal is selected from a frontier, can there be a shorter path to a goal? 
- Suppose $$p’$$ is on the frontier, Because $$p$$ was chosen before $$p’$$ and $$h(p) = 0 ~$$: $$g(p) \leq g(p’) + h(p’)$$
- Because $$h$$ is an underestimate: $$g(p’) + h(p’) \leq g(p’’)$$ for any path $$p’’$$ that extends $$p’$$ 
- So $$g(p) \leq g(p’’)$$ for any other path $$p’’$$ to the goal

A* can always find a solution if there is one:

- The frontier always constrains the initial part of a path to a goal, before that goal is selected
- A* halts, as the costs of the paths on the frontier keeps increasing, and will eventually exceed any finite number
- Admissibility **does not** guarantee that every node selected from the frontier is on an optimal path.
- Admissibility ensure that the first solution found will be optimal, even in graphs with cycles.

-----------------

#### Good Heuristics

Selecting the correct heuristic is very important:

Suppose $$c$$ is the cost of an optimal solution. Consider what happens to a path $$p$$ where

- $$cost(p) + h(p)  < c$$
- $$cost(p) + h(p) = c$$
- $$cost(p) + h(p) > c$$ 

A better function can help because:

- A* expands all paths from the start in the set $$\{p: cost(p) + h(p) <c\}$$
- As well as some paths from the set $$\{p: cost(p) + h(p) =c\}$$
- Increasing $$h$$ while keeping it admissible reduces the size of the sets.
- If the second is large, there can be a large amount of variability in the space and time complexity of A*

--------------

#### Complexity of A*

Time:

- Exponential in relative error in $$h\cross$$length of solution

Space:

- Exponential: keeps all nodes in memory - the main problem with the algorithm

#### Optimality of the A* algorithm

An algorithm is optimal if no other search algorithm uses less time or expands fewer nodes with a guarantee of solution quality.

- The optimal solution would be the algorithm which picks the correct node at every choice, this is not effective as we can not directly implement it,



**Optimality of A*** - among search algorithms that only use arc costs and a heuristic estimate of the cost from a node toa goal, no algorithm expands fewer nodes than A* and guarantees to find a lWowest-cost path:

**“Proof” Sketch**

Given only information about the arc costs and heuristic information, unless the algorithm has expanded each path $$p$$, where $$f(p)$$ is less than the cost of an optimal path, it does not know whether $$p$$ leads to a lower cost path.

- Suppose an algorithm $$A’$$ found a path for problem $$P$$ where some path $$p$$ was not expanded s.t. $$f(p)$$ was less than the solution found.
- Suppose there was another problem $$P’$$ which was the same as $$P$$ except that there was a path via $$p$$ with cost $$f(p)$$. The algorithm cannot tell $$ P’$$ from $$P$$, because it did not expand the path $$p$$, so would report the same solution for both $$P,P’$$ but the solution found for $$P$$ would not be optimal for $$P’$$ as the solution found has a higher cost than the path via $$p$$. Therefore, an algorithm is not guaranteed

##### Counter Example

Consider an algorithm that does a forward $$A^*$$ like search, and a backward dynamic programming search, where steps are interleaved (swapping forward and backward steps). The backward dynamic programming search builds a table of the $$costToGoal(n)$$ values of the actual discovered cost from $$n$$ to a goal, and maintains a bound $$b$$ where it has explored all paths of cost less than $$b$$ to a goal. The forward search uses a PQ on $$cost(p) + c(n)$$, where $$n$$ is the node at the end of the path $$p$$ and $$c(n)$$ is $$costToGoal(n)$$ if it has been computed, otherwise $$c(n)$$ is $$max(h(n), b)$$. The intuition is that if a path exists from the end of path $$p$$ to a goal node, it either uses a path that has been discovered by the backward search, or a path that costs at least $$b$$  This algorithm is guaranteed to find a lowest-cost path and often expands fewer nodes than A∗

##### Conclusion

Having a counterexample seems to mean the optimality of A* is false, however the proof seems correct for algorithms only doing forward search.

##### Dynamic programming and A*

Dynamic programming can be used to construct heuristics for A* and branch-and-bound searches.

- A way to build a heuristic function is to simplifying a problem until that simplified problem has a small enough state space.
- Dynamic programming can be used to find an optimal path length in a simplified problem, which can then be used as a heuristic function for the original problem.

----------------

### Summary of Search Strategies

| **Strategy**          | Frontier Selection | Complete | Halts | Space  |
| --------------------- | ------------------ | -------- | ----- | ------ |
| Breadth-First         | First node added   | Yes      | No    | Exp    |
| Depth-First           | Last node added    | No       | No    | Linear |
| Lowest-Cost-First     | Minimal $$cost(p)$$ | Yes      | No    | Exp    |
| Heuristic depth first | Local min $$h(p)$$ | No       | No    | Linear |
| Best-first            | Global min $$h(p)$$ | No       | No    | Exp    |
| A*                    | Minimal $$f(p)$$ | Yes      | No    | Exp    |

-----------------

### Cycle Checking, Path Pruning

The simplest way of pruning the search tree, whilst guaranteeing that a solution will be found in a finite graph, is to ensure that the algorithm does not consider neighbours what are already on the path from the start.

- The search can prune a path that ends in  a node already on the path, without removing an optimal solution
- In depth-first method, checking for cycles can be done in a constant time in path length - hash function, however this varies depending on implementation. Where the space complexity is exponential, a cycle check takes linear time.
- Other methods, this can be done in linear time in path length

---------

#### Multiple-Path Pruning

In staring in $$s$$, there are two paths, one is better than the other, so only one needs be considered

- Prune a path to node $$n$$ if the seach has already found a path to $$n$$
- Implemented by maintaining an explored set, the closed list, of nodes that at the end of expanded paths
- The closed list is initially empty

- When a path $$\langle n_0,…,n_k\rangle$$ is selected, if $$n_k$$ in closed list the path is discarded, otherwise $$n_k$$ is added to the closed list and algorithm continues as before

![image-20211011091417126](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011091417126.png)

-----------

#### Multiple-Path Pruning and Optimal Solutions

What if the subsequent path is shorter than the first?

- ensure this does not happen, the shortest path is always found first
  - This is hard to do
- Remove all paths from the frontier that use the longer path - if there is a path $$p=\langle s,…,n,…,m\rangle$$ on the frontier, and a path $$p’$$ is found with a lower cost than the portion of $$p$$ from $$s$$ to $$n$$, then $$p$$ can be removed from the frontier.
- Change the initial segment of the paths on the frontier to use the shorter path:  if there is a path $$p=\langle s,…,n,…,m\rangle$$ on the frontier, and a path $$p’$$ is found with a lower cost than the portion of $$p$$ from $$s$$ to $$n$$, then $$p’$$ can replace the initial path of $$p$$ to $$n$$

-----------------

### Multiple Path Pruning and A*

A* does not guarantee that when a path to a node is selected for the first time it is the lowest cost path to that node - the admissibility theorem guarantees the minimum path cost for the goal node, but not for every path.

- suppose path $$p’$$ to $$n’$$ was selected, but there is a lower cost path to $$n’$$. Suppose the lower cost path is via path $$p$$ on the frontier
- Suppose path $$p$$ ends at  node $$n$$
- $$p’$$ was selected before $$p$$ $$f(p') \leq f(p)$$, so $$cost(p’) + h(p’) \leq cost(p) + h(p)$$ 
- Suppose $$cost(n,n’)$$ is the actual cost of a path from $$n$$ to $$n’$$. The path to $$n’$$ via $$p$$ is lower cost than via $$p’$$ so: $$cost(p) + cost(n,n’) < cost(p’)$$
- Resulting in : $$cost(n,n’)<cost(p’) - cost(p) \leq h(p) - h(p’) = h(n) -h(n’)$$
- We can ensure this does not happen if: $$|h(n) -h(n’)| \leq cost (n,n’)$$ 

Multiple path pruning includes a cycle check, because a cycle is another path to a node, and is therefore pruned.

- Can be done in constant time if the graph is explicit ally stored, by setting a bit on each node to which a path has been found.
- Can be done in logarithmic time (in the number of expanded nodes if indexed properly), if the graph is dynamically generated, by storing the closed list of all the expanded nodes.

This is always preferred in breadth-first search where virtually all nodes must be stored anyways, however for DFS strategies, the algorithm does not otherwise have to store all of the nodes already considered - this would make it exponential in space. Therefore, cycle checking is preferred.

---------------

#### Monotone Restriction

- Heuristic function $$h$$ satisfies the monotone restriction if $$|h(m) -h(n)| \leq cost(m,n)$$ for every arc: $$\langle m,n\rangle$$
- If $$h$$ satisfies the monotone restriction it is consistent meaning $$h(n) \leq cost(n,n’) + h(n’)$$ for any two nodes $$n,n’$$
- A* with a consistent heuristic and multiple path pruning always finds the shortes path of a goal

These factors increase the strength of the admissibility criterion. 

----------------

### Direction of Search

- The definition of searching is symmetric: find a path from start nodes to goal node or from goal node to start nodes:
- **Forward branching factor** number of arcs out of a node 
- **Backward branching factor** number of arcs into a node
- Search complexity is $$b^n$$: should use the forward search if the forward branching factor is less than the backward branching factor and vice versa
- When the graph is dynamically constructed, the backwards graph may not be available

------------------

#### Bidirectional Search

- Search from the goal and forward and start simultaneously:
- This is good, because: $$2b^{k/2} << b^k$$
- Main problem is ensuring they meet.
  - Depth first in both directions is unlikely to meet as the small search frontiers are likely to pass one another, however, breadth-first search in both directions would guarantee a meeting
  - A combination of depth-first search in one direction and breadth-first in the other would guarantee the required intersection of the frontiers, but which one to choose in which direction can be difficult.
- This is often used with one breadth first method that builds a set of locations that can lead to the goal and in the other direction a method can be used to find a path to these interesting locations

------------

#### Island Driven Search

- **Idea** find a set of islands between $$s$$ and $$g$$

  $$s\rightarrow i_1 \rightarrow i_2 \rightarrow … \rightarrow i_m \rightarrow g$$ 

- Can be effective as $$mb^{k/m} << b^k$$ 

- The problem is to identify the islands that the path must pass through - difficult to ensure optimality

- The subproblems can be solved using islands $$\Longrightarrow$$ hierarchy of abstractions

  - Incorrect choice of island could make the problem harder or impossible to solve

---------------

### Dynamic Programming

**Idea** for statically stored graphs, build a table of $$dist(n)$$ the actual distance of the shortest path from node $$n$$ to a goal

This can be build backwards from the goal:

- $$dist(n) = 0$$ if $$isGoal(n)$$,
- otherwise $$dist(n) = min_{\langle n,m \rangle \in A}(|\langle n,m \rangle | +dist (m))$$ 
- Following this policy will take the agent from any node to a goal along a lowest cost path. Given a $$costToGoal$$, determining which arc is optimal takes constant time with respect to the size of the graph - assuming a bounded number of neighbours for each node.
- Dynamic programming take time and space linear in the size of the graph to build the table for the $$costToGoal$$ or $$dist(n)$$ table



When building a cost to goal function, the searcher implicitly determined which neighbour leads to the goal, instead of determining at runtime which neighbours is on an optimal path, it can store this information

#### Useful When:

- The goal nodes are explicit 
- A lowest cost path
- The graph is finite and small enough to store the cost to goal values for each node
- The goal does not often change
- The policy is used a number of times for each goal, so that the cost of generating the cost of goal valued can be amortized over many instances

#### Problematic When

- it requires enough space to store the graph
- The dist function must be recomputed for each goal
- the time and space required is linear in the size of the graph, where the graph size for finite graphs is typically exponential in the path length.


-----------------

### Bounded Depth-First Search

- A bounded depth-first search takes a bound (depth or cost) and does not expand paths that exceed that bound
  - Explores part of the search path
  - Uses linear space in the depth of the search

----------------

### Iterative-deepening search

This is used to combine the benefits of the space complexity of DFS with the optimality of BFS.

- Start with a bound $$b=0$$
- Do a bounded depth-first search with bound $$b$$
- If a solution is found return that solution
- Otherwise increment $$b$$ and repeat
- You must distinguish:
  - Failure because the depth bound was reached - **Failing unnaturally**
    - Here the search needs to be retried with a larger depth bound
  - Failure that does not involve reaching the depth bound - **Failing naturally**
  - - Here it is a waste of time to try again with a larger depth bound, because no path exists no matter what the depth.



This finds the same first solution as a breadth-first search, since using a depth-first search iterative-deepening uses linear space.

- Iterative deepening has an asymptotic overhead of $$(\frac{b}{b-1})$$ times the cost of expanding the nodes at depth $$k$$ using breadth-first search
- When $$b=2$$, there is an overhead factor of 2, when $$b=3$$ there is an overhead factor of 1.5.
  - As $$b$$ increases, the overhead factor reduces

A clear problem is the wasted computation that occurs at each step, however this is not as prevalent when the branching factor is high.

Assume a branching factor $$b>1$$

- Consider the search where the bound is $$k$$. At this depth, there are $$b^k$$ nodes, and each of these has been generated once.
- Those at $$k-1$$ have been generated twice, $$k-2$$ have been generated three times …
- The total generated nodes is:

$$
b^k +2b^{k-1} + 3b^{k-2}+...+kb = b^k(1+2b^{-1} + 3b^{-2}+kb^{1-k})\\
\leq b^k\bigg(\sum^{\infty}_{i-1}ib^{1-i} \bigg) \\
=b^k\bigg(\frac{b}{b-1}\bigg)^2
$$

- This means there is a constant overhead pf $$(b/(b-1))^2$$ times the cost of generating the nodes at depth $$n$$.
  - When $$b=2$$, there is an overhead factor of 4, when $$b=3$$, there is an overhead factor of 2.25.

#### Iterative Deepening Pseudocode - from book

![image-20211013112439463](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211013112439463.png)

#### Iterative Deepening and A* (IDA*)

This can be applied to A*, and performs repeated depth-bounded depth-first searches. Instead of a bound on the number of arcs, it is a bound on the value of $$f(n)$$. This threshold starts at the value of $$f(s)$$, where $$s$$ is the starting mode with the minimal $$h$$ value. This then carries out depth-first depth-bounded search but never expands a node with a higher $$f$$ value than the current bound.

- If it fails unaturally, the next bound is the minimum of the $$f$$ value that exceed the previous bound.
- Checks the same nodes as $$A*$$, but recomputes them with depth-first search instead of storing.

------------

### Depth-first Branch-and-Bound 

The goal is to maintain the lowest-cost path to a goal found so far, particularly when many paths exist, and we want an optimal path - A* search for example

- Combined depth-first search with heuristic information
- Find optimal solution
- Most useful when there are multiple solution and an optimal one is sought after
- Uses the space of a depth-first search.

Suppose we want to find a single optimal solution

- Suppose bound is the cost of the lowest-cost path found to a goal so far

- What if the search encounters a path $$p$$ such that $$cost(p)+h(p) \geq bound$$

  $$\Rightarrow p$$ can be pruned

- What can we do if a non-pruned path to a goal is found?

  $$\Rightarrow bound$$ can be set to the cost of $$p$$ and $$p$$ can be remembered as the best solution so far.

- Uses linear space

- Guarantees an optimal solution

-----------

#### How should bound be initialised

- The bound can be initialised to $$\infty$$
- The bound can be set to estimate the optimal path cost
- After a depth-first search terimantes, either:
  - A solution was found
  - No solution was found, no path pruned
  - No solution was found, path was pruned
- This can be combined with iterative deepening to increase the bound until a solution is found, or there is no solution.
- Cycle pruning works well with depth-first branch-and-bound
- Multiple-path pruning is not appropriate as storing explored set defeats space saving of depth-first search.

#### Bound and a Solution

The algorithm will return an optimal solution - a least-cost path from the start node to a goal node - if there is a solution with cost less thatn the initial bound.

- If this is slightly above the cost of a lowest-cost path, the algorithm can find an optimal path expanding no more arcs than A* search. This happens when the initial bound is such that the algorithm prunes any path that has a higher cost than a lowest-cost path. Once a path is found, it only explores path whose $$f$$ value is lower than the one found.
- If no solution is returned:
  - When $$bound = \infty$$ then no solution exists
  - When $$bound$$ is finite, then none exist with costs less than that
- This can be combined with iterative deepening to increase the bound until a solution is found or it can be shown there is no solution.

--------------

### Heuristics 

A way to determine which node seems the most promising.

- A function $$h(n)$$ take a node $$n$$ and returns a non-negative real number that is an estimate of the path cost from node $$n$$ to the goal node.

- It must only use information that can be readily obtained about a node

  - Often a trade off between the qeuality of the heuristic and the work required to determine it.

  $$\Rightarrow$$ Often, a way to derive a heuristic function is to solve a simpler problem and to use the actual cost in the simplified problem.

- The heuristic function can be extended to applicable paths, and the heuristic value of the path is the value of the node at the end of the path:

  - $$h(\langle n_0,…,n_k\rangle)=h(n_k)$$ 



A use of this is ordering neighbouring nodes that are added to the stack representing the frontier in depth-first search - the best neighbour will be selected first.

- This is **heuristic depth-first search**
- Chooses the locally best path
- Explores all paths from the selected path before selecting another.
- Often used, suffers from the problems of DFS.



Another use of this is to always select a path on the frontier with the lowest heuristic value:

- Best-first search
- Usually does not work well - can follow paths that look promising, however their costs may keep increasing

#### Admissible Heuristics example

- 8 puzzle
- Branching factor around 3 - typical solution around 20 steps
- Exhaustive search: $$3^{20}$$ 
- Need a decent heuristic

#### Possible heuristics

- $$h_1(n) = $$ number of misplaced tiles
  - Admissible since each misplaced tile must have been moved at least once.
- $$h_2(n) =$$ city block, or Manhattan distance: number of squares from desired location
  - Admissible since a single move can only move one tile step closer to goal

#### Characterising Heuristics

**Definition** if A* tree-search expands $$N$$ nodes and a solution is $$d$$, the effective branching factor $$b^*$$ is the branching factor a uniform tree of depth $$d$$ would have to contain $$N$$ nodes

- $$N = 1 + b^* + (b^*)^2 + … + (b^*)^d$$ 
- Closer this gets to 1, the larger the problem that can be solved.

This can be estimated experimentally: 

![image-20211011122723216](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211011122723216.png)

- If $$h_2(n) \geq h_1(n)$$ then $$h_2$$ dominates $$h_1$$ 



On average A* tree-search using $$h_2(n)$$ expands less nodes than using  $$h_1(n)$$ 

- A* expands all nodes with $$f(n) <f^*$$ where $$f^*$$ is the cost of optimal path
- All nodes with $$h(n) <f^* - g(n)$$ are expanded

**Conclusion** a higher value is better, provided it is admissible

- need to make sure that the required computation required to determine the heuristic does not outweigh the savings.

#### Deriving heuristics - Relaxed problems

- Can derive admissible heuristics from the exact solution cost of a relaxed version of the problem
- A problem with less restrictions on operators is a relaxed problem
- If relaxed rules of the 8-puzzle so that a tile can move anywhere, then $$h_1(n)$$ gives the shortest solution
- If relax rules so that a tile can move to any adjacent square,m then $$h_2(n)$$ gives the shortest solution

We would like to generate heuristics automatically

- If the problem is described in a formal language, relaxed problems can be generated.





**Example** The 8-puzzle can be described as: a tile can move from square A to square B if A is adjacent to B and B is blank

- Generate relaxed problems
  - A tile can move from square A to square B if A is adjacent to B
  - A tile can move from square A to square B if B is blank
  - A tile can move from square A to square B
- Each of these problems give a heuristic measue



If one dominate choose that one, however typically there may be several admissible heurisitics

- If none dominate, then use $$h(n) = max(h_1(n),…,h_m(n))$$ 
- Since all are admissible, then $$h$$ is admissible
- $$h$$ dominates all of its constituent heuristics

#### Deriving heuristics - Subproblems

- Derive admissible heuristic from solution cost of a subproblem of a given problem
- Cost of subproblem= lower bound on cost

#### Deriving heuristics - Pattern Databases

- Store exact solution costs for each possible subproblem instance
- Heuristic $$h_{DB}=$$ cost of solution to corresponding subproblem
- Construct the database by searching backwards from the goal
- Combine pattern databases as before $$h(n) = max(h_1(n),…,h_m(n))$$ 

#### Deriving heuristics: Disjoint Pattern Databases

- Choice of tiles 1,2,3,4 was arbitrary, could have done the same for 5,6,7,8
- Cannot add the solution together, as each subproblem is likely to share moves
- If shared moves are discounted, then we can add the heuristics $$\rightarrow$$ disjoint pattern database
- Need to be able to divide up the problem so moves only affect a single subproblem

#### Deriving Heuristics - Other

- Statistical approach - run over training problems and gather stats
- Select features of state that contribute
  - In chess, number of pieces left
  - Determine the weightings of factors 





## Constrain Satisfaction Problems

A CSP is characterized by 

- A set of variables: $$(V_1,V_2,…,V_n)$$
- Each variable has an associated domain $$D_{v_i}$$ of possible values.
- There are hard constraints on various subsets of the variables which specify legal combinations of values for these variables
- A solution to the CSP is an assignment of a value to each variable that satisfies all the constraints.

CSPs as optimization problems:

- For optimization problems there is a function which gives the cost for each assignment of a value to every variable
- A solution is an assignment of values to the variables that minimizes the cost function

### States VS features

Typically, there are too many constraints for an agent to reason with. Also, most problems do not come with an explicit set of constraints, they need to be derived or thought about. Therefore, the states are typically described implicitly in terms of features.

- When describing a real state space, it is more natural to describe the features that make up the state space rather than explicitly enumerating the states.

The definition of states and features are intertwined - they can be described in terms of each other.

**States** - can be defied in terms of features: features can be  primitive and state corresponds to an assignment a value to each feature

**Features** - can be defined n terms of state: the states can be primitive and a feature is a function of the sates. Given a state, the function returns the value of the feature on that state.

#### Using features vs states

Every feature has a domain that is the set of values that it can take on. The domain of the features is the range of the function of the states.

​	One of the advantages of reasoning in terms of features is the computational savings - a binary features the domain has two values, therefore many states can be described by only a few features.

- Reasoning in terms of features may be easier than reasoning in terms of a billion states, - Reasoning in terms of 100 features is not that many, however reasoning in terms of 2^100^ states is not possible
- Often, the states are not independent, therefore there are sets of combination of features that are possible to happen to each other.

### Types of CSP

CSP’s are described in terms of features, without explicitly considering time - CPSs will be described in terms of possible worlds.

- **Possible worlds** - possible way the world could be - when representing a crossworl puzzle, the possible worlds could correspond to the ways the crossword could be filled out.
  - In an electrical environment, a possible world specifies the position of every switch and component

#### Algebraic variables

- a symbol used to denote the features of possible worlds - will be wreitten starting with an upper case letter. Every variable has a given domain $$dom(V)$$, which is the set of values which it can take on.

Discrete variables:

- Finite domains: n variables of domain of domain size: $$d\Longrightarrow O(d^n)$$ complete assignments 
  - Boolean CSPs
- Infinite domains - integers, strings
  - Lecture and seminar scheduling variables are start/end times
  - Cant enumerate all assignments - need a constraint language
  - linear constraints are solvable, nonlinear undecideable

Continuous variable - time
- Domain is continuous
- Linear constraints solvable in polynomial time by linear programming methods



Possible worlds can be defined in terms of variables or variables can be define d in terms of possible worlds.

- Variables can be primitive and a possible world coressponds to a total assignment of a value to each vairable
- Worlds can be primitive and a variable isa  function from possible worlds into the domain of the variable in that possible world.

#### Types of Constraints

- **Unary constraints** - involve a single variable
- **Binary constraints** - involve pairs of variables
- **Higher-order constraints** involve 3 or more variables
- **Preferences / soft constraints** a 1005 is better than 0805 for a lecture to start - represented by a cost for each variable

A world satisfies a set of constrains if for every constraint, the value assigned in the world to the variables in scope of the constraint satisfy the constraint

#### Cryptarithmetic Example

![image-20211018120208478](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018120208478.png)

- letters which you must assign digits to in such a way that the sum works
  - O + O = R, W + W  = U, T + T = O
- Represent with the contraint graph above
  - Circles are variables
  - Circles as possible remainders
- Boxes represent contraint
  - A constraint with, O, R $$X_1$$ 

Variables:

- $$F,T,U,W,R,O,X_1,X_2,X_3$$ 

Domains:

- $$\{0,1,2,3,4,5,6,7,8,9,\}$$ 

Constraints:

- $$allDiff(F,T,U,W,R,O)$$  - letters = different digits

  $$O+O = R + 10\cdot X_1$$ 

  $$X_1 + W + W = U + 10 \cdot X_2$$

  $$X_1 + U + U = O + 10 \cdot X_3$$

  $$X_3 = F$$

  

#### N-Queens Problem

Find the config that places $$n$$ queens on an $$n \cross n$$ board with no pairs of queens attacking.

#### Scheduling Problem

- Variables $$A,B,C,D,E$$ that represent the starting times of activities.
- Domains: $$D_A = \{1,2,3,4\}, D_B = \{1,2,3,4\}, D_C = \{1,2,3,4\}, D_D = \{1,2,3,4\}, D_E = \{1,2,3,4\}$$ 

-  Constraints: $$(B\neq 3) \and (C\neq 2)$$ for example. 

#### Real-world CSPs

- Assignment problems
- Timetabling
- Hardware configuration
- Spreadsheets

### Solving CSPs

#### Generate-and-Test Algorithm

- Generate the assignment space - the cartesian product of all domain - the set of total assignments for example
- Test each of these with the contraints
- There need to be $$ d^n $$ assignments.

- Incredibly inefficient.

#### Backtracking Algorithms

- Systematically explore $$D$$ by instantiating the variables one at a time
- Evaluate the constraint predicate as soon as all its variables are bound
- Any partial assignment that does not satisfy the constraint can be pruned

Properties:

- Every solution appears at depth $$n$$ - this means that we can use depth first search

- Path is irrelevant, so can use complete-state formulation

- Branching factor $$b(n-l)d$$ at depth $$l$$ - $$n!\cdot d^n$$ leaves

  Top level branching factor is $$nd$$ since any of $$d$$ values can be assigned to any of $$n$$ variables - the next level branching factor is $$(n-1)d$$ etc

- Variables are commutative

- Only need to consider assignments to a single variable at each node

- Backtracking search is the basic uninformed algorithm for CSPs

- Can solver n-queens for $$n\approx 25$$

**Algorithm**

![image-20211018122443859](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018122443859.png)

- Is a recursive algorithm



#### Improving Search Efficiency

General-purpose methods can give huge gains in speed

- Which variable should be assigned next - **MRV & degree heuristic**
- In what order should its values be tried - **LCV**
- Can we detect inevitable failure early - **Consistency Algorithms**
- Can we take advantage of problem structure - **Cutset Conditioning & variable elimination**

Can avoid the need to use domain specific knowledge.

#### CSP Backtracking Search Heuristics

##### MRV 

- Choose the variable with the fewest legal values
  - Also called “fail first” - will pick variable most likely to cause failure - if exists a variable with 0 possible assignments will pick and fail immediately

##### Degree Heuristic

- Tie breaker among MRV variables
  - Choose the variable with the most constraints on remaining variables
  - Attempts to reduce branching factor of future choices

##### Least Constraining Value

- Given a variable, choose the LCV - the one that rules out the fewest values in the remaining variables.

The combination of these can solve n-queens for $$n \approx 1000$$ 

### CSP as Graph Searching

A CSP can be solved by graph -seraching

- A node is an assignment of values to some of the variables

- Suppose node $$N$$ is the assignment of $$X_1 = v_1,…,X_k = v_k$$ that is not assigned in $$N$$

  For each value $$y_i \in dom(Y)$$ 

   $$X_1 = v_1,…,X_k = v_k, Y=y_i$$ is a neighbour of $$N$$ if it is consistent with the constraints

- The start node is an empty assignment

- Goal node is a total assignment which satisfies the constraint

#### Consistency Algorithms

- Prune the domains as much as possible before selecting values from them
- A variable is **domain consistent** if there is no value of the domain of the node is reeled impossible by the  constraints

#### Constraint Network

- circular node for each variable
- Rectangle node for each constraint
- There is a domain of values associated with each variable node
- There is an arc from variable $$X$$ to each constraint that involves $$X$$

#### Arc Consistency

- An arc $$\langle X, r(X, \overline{Y})\rangle$$ if for each value $$x \in dom(x)$$, there is some value $$\overline{y} \in dom(\overline{Y})$$ such that $$r(x,\overline{y})$$ is satisfied
- A network is arc consistent if all its arcs are arc consistent
- What if the arc is not consistent?

  - All values of $$X$$ in $$dom(X)$$ for which there is no corresponding value in $$dom(\overline{Y})$$ can be deleted from $$dom(X)$$ to make the arc $$\langle X, r(X, \overline{Y})\rangle$$ 

##### Arc consistency algorithm

- the arcs can be considered in turn making each arc consistent
- When an arc has been made arc consistent, it needs to be revisited if the domain of one of the $$Y$$‘s is reduced.
- Three possible outcomes when all arcs are made arc consistent
  - One domain is empty $$\Longrightarrow$$ no solution
  - Each domain has a single value $$\Longrightarrow$$ unique solution.
  - Some domains have more than a single value.$$\Longrightarrow$$ there may or may not be a solution.



##### Arc consistency algorithm example

- Variables A,B,C,D,E represent the starting times of various activities

- Domains:

  $$D_A = \{1,2,3,4\}, D_B = \{1,2,3,4\}, D_C = \{1,2,3,4\}, D_D = \{1,2,3,4\}, D_E = \{1,2,3,4\}$$ 

- Constraints:

  $$(A>D)\and (D>E) \and (C\neq A) \and (C>E)\and(C\neq D) \and (B \geq A) \and (B\neq C)\and (C\neq D + 1)$$

![image-20211018153610838](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018153610838.png)

- boxes have been omitted for contraints.
  - Constraints in this only are binary





| Arc                    | Relation | Value(s) Removed |
| ---------------------- | -------- | ---------------- |
| $$\langle D, E \rangle$$ | $$D>E$$ | $$D=1$$        |
| $$\langle E,D \rangle$$ | $$D>E$$ | $$E=4$$        |
| $$\langle C,E \rangle$$ | $$C>E$$ | $$C=1$$        |
| $$\langle D,A \rangle$$ | $$A>D$$ | $$D=4$$        |
| $$\langle A,D \rangle$$ | $$A>D$$ | $$A=1,A=2$$    |
| $$\langle B,A \rangle$$ | $$B>A$$ | $$B=1,B=2$$    |
| $$\langle E,D \rangle$$ | $$D>E$$ | $$E=3$$        |

##### Arc Consistent Graph

![image-20211018154346808](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018154346808.png)

##### Finding solutions when AC finished

- If some domains have more than one element - search
- Split a domain, then recursively solve each half $$\Longrightarrow$$ domain splitting or case analysys
  - Idea is to split a problem into a number of disjoint cases and solve each case separately
  - The set of all solutions to the initial problem is the union of the solution to each case
- Often best to split in half
- No need to restart AC, just consider arcs that are possibly no longer consistent as a result of the split

![image-20211018154923257](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018154923257.png)

Splitting the domain of $$D$$ - there are two cases, $$D=2,3$$ there are constraint graphs correspond to the only two solutions

#### Hard and Soft Constraints

given a set of variables assign a value to each variable that either

- Satisfies a set of constraints - satisfiability problems
- Minimizes some cost function where each assignment of values to variables has some cost - optimization problems.

#### Problem Structure

- suppose that each subproblem has c out of n variables
- There are $$n/c$$ subproblems each of which takes at most $$d^c$$ to solve. Worst-case solution cost therefore, $$n/c \cdot d^c$$

### Tree-Structured CSPs

**Theorem** - if the constraint graph has no loops, the CSP can be solved in $$O(n\cdot d^2)$$ time.

#### Algorithm for Tree-Structured CSPs

- Choose a variable as a root, order variables from root to leaves such that every node’s parent precedes it in the ordering

- For j from n down to 2, remove inconsistent domain elements for $$\langle Parent(X_j), X_j\rangle$$

  At this point, CSP is directionally arc consistent, so no backtracking in 3, reverse order checks ensure deleted values do not endanger consistency of processed arcs - $$O(n\cdot d^2)$$ 

- For j from 1 to n, assign $$X_j$$ consistently with $$Parent(X_j)$$

#### Nearly Tree-Structured CSPs

- Conditioning - instantiate a variable, prune its neighbours’ domains - assign variable so remainder is a tree
- Cutset Conditioning - instantiate in all ways a set of variables such that the remaining constraint graph is a tree
- Cutset size $$c \Rightarrow$$ runtime $$O(d^c (n-c)d^2)$$ - very fast for small $$c$$



#### Variable elimination

- eliminate the variables one-by-one passing their constraints to their neighbours

**Algorithm**

- If there is only one variable return the intersection of the constraints that contain it
- Select a variable $$X$$ 
- Join the constraints in which $$X$$ appears forming constraint $$R_1$$
- Project $$R_1$$ onto its variables other than $$X$$ forming $$R_2$$
- Replace all of the contraints in which $$X$$ appears by $$R_2$$ 
- Recursively solve the simplified problem forming $$R_3$$
- Return $$R_1$$ joined with $$R_3$$



When there is a single variable remaining, if it has no values, the network was inconsitent

- The variables are eliminated according to some elimination ordering
- Different elimination orderings result in different size intermediate constraints

##### Variable elimination example:

![image-20211018161517912](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161517912.png)

- First of all, make the network arc-consitent

![image-20211018161543250](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161543250.png)

- Eliminating $$C$$ 

![image-20211018161601860](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018161601860.png)

- Resulting network without $$C$$ 
  - There is now another relation showing the relation $$r_4$$

![image-20211018162205614](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211018162205614.png)

## Local Search

### Iterative Improvement Algorithm

- Just the goal state we are looking for - goal state is the solution
- Local Search uses a single current state - not multiple paths and typically moved to neighbours of state
- Not systematic
  - Low memory useage
  - Can find reasonable solution in continuous spaces
- Useful for optimization problems, including CPSs – finding the best state4 according to some objective function

#### The State Space

- The set of complete configurations
- A goal is a particular configuration
  - The travelling salesman - the shortest route visiting each city exactly once
  - N-Queens problem
- Can use iterative imporvement
  - Keeps track of current state, trying to improve it
- Constant space typically

Consider all states on a surface of a landscape

- Height of any point - evaluation of that point
- Move around the landscape trying to find the highest peaks

#### Hill Climbing

- always try to improve state - reduce cost for example
- Each iteration move in direction of increasing value - or decreasing if cost
- No search tree, just keep current state and cost
- If several have equal choose randomly



**function** $$HillClimbing(problem)$$ **returns** solution state

​	**inputs** - problem

​	**state** - current - a node, next - a node

​	$$current\leftarrow MakeNode(InitialState[problem])$$ 

​	**loop do**

​		$$next \leftarrow$$ highest values neighbour if current

​		**if** $$Value[next]<Value[current]$$ 

​			**then** **return** current

​		$$current \leftarrow next$$

​	**end**



##### Problem of Hill Climbing

- Local maxima - local peak, lower than highest peak, the algorithm will halt witha  suboptimal solution
- Ridges - steep sides, top with gentle slope - search may oscillate from side to side, not making progress
- Plateaux  - flat area will conduct a random walk
- Plateaux - may be a local maximum - no uphill or a shoulder - possible to progress 
  - If this is reached, allow sideways moves to try and get off the shoulder
  - Limit the number of sideways moves - otherwise can conduct infinite random walk on plateaux

![image-20211026093426746](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026093426746.png)

### Greedy Descent - local search for CSPs

- Maintain an assignment of a value to each variable
- Repeat:
  - Select a variable to change
  - Select a new value for that variable
- Until a satisfying assignment is found



**Aim** - find an assignment with zero unsatisfied constraints

- Given an assignment of a value to each variable, a conflict is a violated constraint
- The goal is an assignment with zero conflicts
- Heuristic function to be minimized - as the number of conflicts

#### Greedy Descent Variants

- Find a variable-value pair that minimizes the number of conflict
- Select a variable that participates in  the most conflicts, select a value that minimizes the number of conflicts
- Select a variable that appears in any conflict, select a value what minimizes the number of conflicts
- Select a variable at random, select a value that minimizes the number of conflicts
- Select a variable and a value at random; accept the change if it does not increase the number of conflicts.

#### Complex Domains

- When domains are small or unordered, the neghbours of an assignment can sorrespond to choosing another value for one of the variables
- When the domains are large and ordered, the neighbours of an assignment are the adjacent value for one of the variables
- If the domains are continuous, gradient descent changes each variable proportionally to the gradient of the heuristic function in that direction:
  - The value of variable $$X_i$$ goes from $$v_i$$ to $$v_i - \eta \frac{\delta h}{\delta x_i}$$ where $$\eta$$ is the step size

### Randomized Algorithms

As well as downward steps, we can allow for - prevent getting stuck in local minima

- Random steps - move to a random neighbour
- Random restarts - reassign random values to all variables

1- dimensional Ordered Examppe

![image-20211026150648764](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026150648764.png)

- Greedy descent with random restart should find the optimal quickly, and random walk would not work well, since so many random steps needed to escape local minima **a**
- Random restart quickly gets stuck on a peak and does not work very well, but eandom walk and greedy descent can escape local minima - **b**

### Stochastic Local Search



- Greedy d3scent - move to lowest neighbour
- Random walk - taking some more random steps
- Random restart - reassigning values to all variables



#### Random Walk Variants

- When choosing the best variable-value pair randomly choose a raondom variable value pair
- When selecting a value:
  - Sometimes choose any variable that participated in the most conflicts
  - Sometimes choose any variable that participates in any conflict
  - Sometime choose any variable
- Sometimes choose the best value and sometimes a random value

#### Comparing Stochastic Algorithms

How can you compare when:

- One solves 30% of time very quickly but doesn’t halt for other 70%
- One solves 60% of the cases resonably quickly but doesn’t solve the rest
- One solves the problem in 100% of cases but slowly

Mean, median and mode runtime do not make much sense



![image-20211026152553166](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026152553166.png)

- Plot the runtime or number of steps, and the proportion of the runs that are solved within that runtime
- Which algorithm is best depends on how much time is available or how important it is to find a solution

### Simulated Annealing

- Pick at random and a new value at random
- If it is an approvement adopt it
- If it is not an improvement adopt it proabilistically depending on a temperature parameter $$T$$
  -  With current assignement $$n$$ and proposed assignment $$n’$$ we move to $$n’$$ with probability $$e^{(h(n’)-h(n)))/T}$$ 
- Temperature can be reduced over time

Probability of accepting a change:

![image-20211026153033427](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026153033427.png)

### Tabu Lists

- To prevent cycling, we maintain a tabu list of the $$k$$ last assignment
- Do not allow an assignment that is already in that list
- If k=1, we do not allow an assignment of the same value to the variable chosen
- We can implement it more efficiently thatn as a list of complete assignent - using a hashmap list, or including a list of steps
- Can be expensive



### Parallel search

A total assignment is called an individual

- **Idea** maintain a population of $$k$$ individuals instead of one
- At every state, update each individual in the population
- Whenever an individual is a solution, it can be reported
- Like $$k$$ restarts, but uses $$k$$ times the minimum number of steps.



### Beam Search

- Like a parallel search with $$k$$ individuals, but choose the best $$k$$ out of all of the neigbhours
- When $$k=1$$ it is greedy descent
- When $$k=\infty$$ it is BFS
- The value lets us limit space and  parallelism

### Stochastic Beam Search

- Like beam search, but probabilistically chooses the $$k$$ individulas as the next generation
- The probability that a neigbhbour is chosen is proportional to its heuristic value
- This maintain diversity amongst the individuals
- The heuristic value refelext the fitness of the individual
- Like asexual reproduction - each individual mutates and the fittest ones survive

### Genetic Algorithms

- Related to stochastic beam search
- Successor states obtained from two parents
- Starts with population of $$k$$ randomly generated individuals
- Each individual represented as a string over a finite alphabet
- Each individual in the population is evaluated by a fitness function
- Fitness function should return higher values for better states
- Fitnesss function determines probability for being chosen for reproduction
- Pairs of individuals chosen according to these probabilities - individuals below a certain point being discarded

- Fir each chosen pair, a random crossover point is chosen 
- Offspring generated by crossing over parent strings at chosen crossover point
- First child gets first part of string from parent and remainder from the second parent - vice versa for second child
- Parents: “12341234, 56785678” with crossover point at position three will give:
  - “12385678” and “5674123”

- Finally, each location in the newly  created children is subject to random mutation with a small probability

- For 8 queens, there is a small probability of taking a random queen and moving it to some random position

![image-20211026154535342](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026154535342.png)

- Only four individuals in this case. 
  - The probabilities determine the likely hood of choosing the individuals for reproduction
  - Populations would typically be hundreds of thousands of indivuduals



If two parents are quite different, crossover can produce state that is very different from both parents

- GAs take large steps early in the search, smaller steps as population converges
- Primary advantage is thea bility to crossover large blocks that have evolved independently to perform useful functions
- Choice of representation is fundamental - GAs work best if representation corresponds to meaningful components of solution
- Useful for optimisation problems

## Adversarial Search

- Consider an agent in a xompetitive multi-agent environment, where goals are in conflict
  - Gives rise to adversarial search
- Other agents, opponents, introduce uncertainty
- An adversarial search must deal with contingencies
- Complexity of game is high and time usually important - best guess based on expe4rience available
- Chess has a branching factor of around 35, and games are often 100 moves long 
- - Search tree has 35^100^ nodes
- Have to make the best move given the situation

### Uncertainty

May rise from:

- Opponents trying to make the best move for themselves
- Randomness  - dice
- Insufficient time to determine exact consequences

Makes this interesting because of the uncertainty, they are easy to represent, usually fully observable, and more like the real world

- Have to make some decision even if optimal decision is infeasible
- Inefficiency penalised severely, inefficient search takes longer, an inefficient chess program loses.

#### Perfect Decisions

Start considering perfect decision - even though they are not generally practical

- Utility function gives numeric value to outcome of game
- Many games fit this model - chess, go etc
- More abstract games need other approaches 

#### Formal View of Game

- Initial State - the board position and the player to move
- Set of operators - a successor function - defines the legal moves and resulting states
- A terminal test - determines when the game is over
- A utility function - gives a numeric value for terminal states

Can build a game tree based on initial state and operators.

**Note** - even for a simple game, the search tree is very complex 

##### Example

Consider a (zero sum) game with two players, Max and Win

Max moves first, then take turns, and points awarded at the end of the game

- For a normal search, Max could just find a sequence of actions to achieve a winning state
- Max must form a strategy that will win whatever Min does

- Strategy should include correct move for Max for each possible move for Min

##### Two ply game tree

Game is one move deep 

- Contains two half moves or ply
- Possible moves for Max at root are $$A_1, A_2, A_3$$ 
- Possible replies for $$A_1$$ for Min: $$A_{11},A_{12}, A_{13}$$



### Minimax

- Minimax gives an optimal strategy for the Maximising player
- Choose the move with the highest minimax value
- Definition - maximum value of a state is the utility fo(for max) of being in that state assuming both players play optimally from that state until end of the game
- Minimax value of terminal state = utility
- Max prefers maximum values, Min prefers minimal:

When: 

- $$n$$ is a terminal, $$MinimaxValue(n)=Utility(n)$$
- $$n$$ is a Max node, $$MinimaxValue(s) =max_{s\in Successors(n)}MinimaxValue(s)$$
- $$n$$ is a Min node, $$MinimaxValue(s) =min_{s\in Successors(n)}MinimaxValue(s)$$

#### Algorithm

- Obtains best possible playoff against best play

Algorithm:

- Generate complete game tree
- Use utility function to rate terminal states
- Use utility of termial states to give utility of nodes one level up
- Continue backing up tree until root
- Max should choose move that leads to the highest utility 
  - Maximises utility assuming the opponent wil play to minimise it





![image-20211026183129394](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026183129394.png)

- First rate terminals, then back up minimal values - Min’s move, then back up maximum values (Max’s move)
  - In this case A1 would be chosen

#### Properties of Minimax

- Complete - if finite
- Optimal if against optimal opponent
- Space $$O(bd)$$ 
- Time - $$O(b^d)$$ - very bad with real games
- Minimax requires complete search tree - not normally practical
- Forms a basis for more realistic algorithms



#### Multi Player Minimax

Can extend to multiple player games using vectors of utilities

Backed up value of n is the utility vector of whichever successor is best for the player choosing at n

#### Alpha Beta Pruning

- Complete search tree impractical - alternative is to prune branches that will not influence decision
- Idea - consider node $$n$$ that player might move to. If player has a better choice $$m$$ either at the parent of $$n$$ or further up the tree. Then n will never be reaches in actual play and it can be pruned
- As soon as we discover there is a better choice than $$n$$ - by looking at descendants, we prune it

![image-20211026183819620](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026183819620.png)

Considering $$n$$, but there is a better choice $$m$$, then the path to $$n$$ is pruned.

Minimax is a DFS and Alpha-beta pruning gets its name from the parametes backed up the path:

- $$\alpha =$$ value of best choice along the path for Max - highest value
- $$\beta =$$ value of best choice along the path for Min - lowest value

It updates $$\alpha,\beta$$ as it searches, prining as soon as value of current node is known to be worse than current $$\alpha,\beta$$ for Max or Min respectively

- Done by terminating the recursive call

##### Effectiveness

This is dependent on the order of examining successors

- **Solution** - try to examine best successors first
- If this is possible, alpha-beta looks at $$O(b^{d/2})$$ instead pf $$O(b^d)$$ - twice the lookahead
- For random order of successors, alpha-beta looks at $$O(b^{3d/4})$$ nodes
  - In practice, a simple ordering function can give significant advantage
- In chess, captures, threats, forward moves, then backward moves

#### Imperfect Decisions

- Alpha-Beta prunes much of the search tree, while minimax needs the complete tree
- Alpha-best still needs to search to the terminal states for some of the tree
- Where a move must be timely, this is impractical
- An alternative is to cut off the tree earlier
  - A Heuristic function to get the value for states
  - A cut off test to determine when to stop going down a tree

##### Evaluation Functions

An evaluation function gives an estimate of expected utility for a given position

- Cutting off trees turns nonterminal nodes into terminal leaves
- Evaluation function should:
  - Order terminal states as per utility function 
  - Approximate actual utility of state
- Uncertainty is unavoidable - incomplete tree

Mostly, evaluation functions calculate features of a state:

- For chess, number of pawns, rooks, knights, king safety
- These determine equivalence class of states - each class leads to a win , draw or loss with some probability
  - Can evaluate expected value of class
- Can combine features with a weighted linear function: $$w_{1}f_1 + w_{2} f_{2} +…+w_nf_n$$, where the $$w$$‘s are weights and the $$f$$’s features
  - Assume all are independent, otherwise a nonlinear function is needed

#### Cutting off Search

Simplest approach is to set a fixed depth - cutoff test succeeds at depth $$d$$ 

- More robust approach is to use iterative deepening - continue until out of time, then return best move found so far
- Both unreliable - due to appx in evaluation function 

##### Solution

- Only apply evaluation function to quiescent position - those whose value is unlikely to change in the near future
- Non quiescent position expanded until quiescent positions reached
- This extra search is called quiescent search
- Quiescent search restricted to certain types of move to quickly resolve uncertainties

**Horizon Problem**

- Faced with an unavoidable damaging move from opponent, a fixed depth search is fooled into viewing stalling moves as avoidance

- Singular extension search as a means of avoiding horizon problem
  - Singular extension - a move that is clearly better than all others
- Forward pruning - immediately prune some moves from a node with no further consideration
- Only in safe special cases
  - If two moves are symmetric or equivalent only consider one of them
  - nodes very deep in search tree

#### Games with Chance

Legal moves dependent on the roll of a dice - a complete game tree cannot be included

- Can include chance nodes
- Labelled with a result and probability
- Can calculate expected value taken over possible results of chance node

Generalise the minimax function to use expected values: $$ExpectIminimax(n)=$$

![image-20211026190557904](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026190557904.png)

- Considers all outcomes of chance node $$O(b^mn^m)$$ where $$n$$ is the number of distinct outcomes
- Can prune chance node without looking at children if we put bounds on the utility function - a chance the node will be an average, we know the bounds within which it lies



### Monte Carlo Tree Search 

- Estimate the value of state from the average utility over simulation - playouts, of complete games starting from the state
  - For some games (go) can learn from self play using neural networks
  - For some games can use specific heuristic - capture moves in chess

Pure MCTS - do $$N$$ simulations starting from current state, and track which moves from current position has highest win percentage

- as $$N$$ increases, this converges to optimal play
- Typically to expensive, therefore a selection policy is needed to focus search on important parts of the game tree. Need to balance:
  - Exploration of states having few playouts
  - Exploitation of states having done will in past playouts to increase accuracy of estimate

#### MCTS steps

- Maintains a search tree and growing it on each iteration using:
  - Selection - starting at root,, choose a move (using a selection policy), leading to successor, and repeat moving to a leaf
  - Expansion - grow tree by generating new child of selected node
  - Simulation - perform playout from the newly generated child node - determine outcome but do not record these moves in the tree
  - Back propagation - use the result of playout to update search tree going up to root

- Repeat this for a fixed number of iterations, or until out of time, then return move with the highest number of playouts - 65/100 is better than 2/3 - less uncertainty with more tries

#### UCTS - upper confidence bounds applied to trees

An effective selection policy based on an upper confidence bound formula called UBC1:

![image-20211026191738681](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211026191738681.png)

Where $$U(n)$$ is total utility of playouts through $$n$$, $$N(n)$$ is number of playouts through $$n$$, $$Parent(n)$$ is $$n$$‘s parent tree. and $$C$$ is a constant which balances exploitation and exploration

- The time to compute a playour is linear in the depth of the tree since only once move taken at each point



## Planning with Certainty

### Knowledge bases

- Knowledge base - Domain specific content
  - A database of facts/beliefs
  - A set of senences in a formal knowledge representation language
  - Implementation vould be anything - linked lists, arrays SQL db
- Inference enginge - domain independent algorithm
  - A mechanism for reasoning about those beleifs



#### Declarative approach to building an agent or system

- Agent can ask itself what to do, answers should follow from the KB through inference
- TELL and ASK are standard names for adding sentences and querying the KB
- Result of ASK must follow from the previous sells.

#### Knowledge based agent

- Can reason using inference and their knowledge
  - Can accept new tasks in the form of goals
  - Can adapt to environmental change by updating knowledge
  - Are able to infer unseen properties of the world from perceptions
- Can often find better solutions than timple search
- More flexibile with respect to
  - Adopting new goals
  - Partially observable envirnomants
  - Dynamic environments



##### 3 Levels

- Knowledge level - what is know, regardless of implementation - allows us to work at abstract level using ASK and TELL - declarative approach
- Logical level- knowledge encoded in formal sentences
- Implementation level - data structures in KB and algorithms that manipulate them



##### Simple knowledge based agent

- represent states, actions
- Incorporate  new percepts
- Update internal representations of the world

- Deduce hidden properties of the world
- Deduce appropriate actions

![image-20211101111306223](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101111306223.png)

- Knowledge base may contain initial background knowledge
- Each iteration, TELL knowledge base of perceptions, ASK what actions to perform
- Tell and ASK refer to KB - internal
- Representation details hidden by MakePerceptSentence and MakeActionQuery - lets us work at knowledge level
- Inference details - logical level, hidden in Tell, Ask

#### Wumpus world

##### Environment - percepts

- Squares adjacent are smelly
- Squares adjacent to pits are breezy
- Glitter iff gold is in the same square
- Bump if walk into a wall
- Scream if Wumpus is killed

##### Environment actions

- shooting kills if facing
- Shooting uses only arrow
- Grabbing picks up the gold if in the same square
- Releasing drops the gold in the same square
- Climb leaves the cave only from the start square

1000 points for leaving cave, 100000 penalty for death, 1 point penalty per action

##### Notes

- No perception of current location
- Location of Wumpus and gold random uniform except square
- Any square except start may be a pit, probability 0.2
- In most environments agent can get gold, sometimes agent must choose between chance to get gold or leaving empty handed
- Sometimes no solution - gold in pit or surrounded

##### Properties

- Deterministic
- Not fully observable
- World is static
- Discrete

##### Cautiousness

- Only moves if it is OK
- Using the combination of different smells and senses, can calculate where the different threats are.
- After each action, the knowledge base is told. - can backtrack again

Only were able to do this due to the inferences given from the perceotions

- Combined knowledge at different times
- Used lack of perception
- Relied on persistence of knowledge

Are possible scenarios where it is not possible to know exactly where it is

- If things are uniformly distributes, can take a probabilistic decision here

- Could go in the start state, shoot the arrow, use inference from that - **coercion**

Given agent has fairly complex reasoning, beyond capabilities of most animals

- Done through logical reasoning

### Reasoning with planning on inference

- logics are formal languages for representing information such that conclusions can be drawn
- Syntax defines the sentences in the language
- Semantics define the ‘meaning’ of sentences - the truth of a sentence in a world

#### Entailments

$KB = \alpha$ 

- Knowledge base KB entails sentence $\alpha$ iss $\alpha$ is true in all worlds where KB is true
- If the KB is ‘kilo is black’ and ‘kilo is a dog’ then this entails ‘either kilo is a dog or kilo is black’
- This is important since it provides a strong way of showing that if certain propositions are true, then some other porisition must be true - finding the wumpus

##### Levels

- Representation level - sentences entail sentences
- World - facts folllow facts
- Semantics give a mapping of sentences to facts
- Logical inference generates sentences that are entailed by existing sentences and and should ensure relationship mirrored in real world
- Inference procedure that generates only entailed sentences is sound or truth preserving
- By considering the semantics of a language we can extract the proof theory of the language - what reasoning steps are sound

#### Inference

- $KB \vdash_i \alpha = $ sentence $\alpha$ can be derived from KB by procedure $i$
  - **Soundness** $i$ is sound if whenever $KB \vdash_i \alpha$ it is also true that $KB \models \alpha$
  - Completeness $i$ is complete if whenever $KB \models \alpha$ it is also true that $KB \vdash_i \alpha$

- We need logic which is expressive enough to say almost anything of interest and for which there exists a sound and complete inference procedure - the procedure will answer any question whose answer follows from what is know by the KB

### Propositional logic  

#### Syntax

- The proposition symbols $P_1,P_2$ are sentences
- Negation - if $S$ is a sentence $¬S$
- Conjunction - if $S_1, S_2$ is a sentence, $S_1\and S_2$ is a sentence
- Disjunction - if $S_1, S_2$ is a sentence, $S_1\or S_2$ is a sentence
- Implication - if $S_1, S_2$ is a sentence, $S_1\implies S_2$ is a sentence
- Equivalence - if $S_1, S_2$ is a sentence, $S_1\Leftrightarrow S_2$ is a sentence

#### Semantics

- Each model specifies true/false for each proposition symbol
- Rules for evaluating with respect to model $m$:

![image-20211101115058572](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101115058572.png)

#### Inference by enumeration

- Let $\alpha = A \or B$ and $KB = (A\or C)\and (B\or ¬C)$
- - check all possible models
  - is alpha true whenever knowledge base is true
  - Create a truth table and check

#### Wumpus reasoning

- percepts as sentences put in KB
- Constraints sentences in the form $S_{2,1}$ representing stench in $[2,1]$ 
- Agent given som eknowledge of the world
- If stench in $[2,1]$, then wumpus is ina neigbhouring square:
  - $S_{2,1} \implies W_{3,1} \or W_{2,2} \or W_{1,1} \or W_{2,1}$ 
- Conclusions can then be drawn with standard inference
- Have some action rules

#### Problems

- Too many propositions 
  - 64 rules (16 suquares x 4 orienteations)
- If the world is larger, the problem gets worse, need thousands of rules for a competent agents
- Writing rules is a problem, as is space, inference also slows
- World may change - need a different symbol for each time step

Planning provides a solution

### Search vs Planning

- Consider a task, get milk, bananas, cordless drill - home with nothing
- Standard search could fail miserably

![image-20211101120342709](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101120342709.png)

- For search we must specify initial state, operations and possible a heuristic function
- Branching factor may be huge depending on how operators are defined
- Length may be dozens
- Too many actions and states to consider
- Agent is forced to construct full sequence of actions
- Must decide what to do in initial state first



After the fact heuristic goal test is inadequate

- heuristics can only choose which state is closer to goal - cannot eliminate actions from considerations
- Evaluation function ranks these guesses, but all must be considrered
- need to work on appropriate part of sequence
- Principal difficulty - unconstrained branching - hard to apply heuristics

#### Planning systems

- open up action, state and goal representations to allow selection - represent in first order logic
  - States and goals = set of setnences
  - Actions = description of preconditions and effects
- Allows planner to make direct connections between states and actions

- Divide and conquer by subgoaling
- Planner can consider several smaller subproblems and combine
- Works as there is little interaction between subplans - otherwise cost of combining solutions outweighs the gain
- Relax requirement for sequential constructions of solutions
- Allows planner to add  actions where needed, so can make ‘obvious’ or important decisions first, reducing branching factor

- No connection between order of planning and execution
- This can only be done because of logic - at (supermarket) represents a class of states, search requires a complete state description and not possible
- In real world, planning tends to do better than search

#### Comparing the two

|         | Search              | Planning                       |
| ------- | ------------------- | ------------------------------ |
| States  | Data structures     | Logical sentences              |
| Actions | code                | preconditions and outcomes     |
| Goal    | code                | logical sentence - conjunction |
| Plan    | sequence from $S_0$ | constraints on actions         |

#### Simple planning agent

![image-20211101121258861](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101121258861.png)

**Algorithm**

- update knowledge base
- If not already executing a plan, generate a goal and construct a plan to achieve it
- Agent must be able to cope if the goal is infeasible or achieved - set action to NoOp
- Once agent has a plan it will execute to completion
- Minimal interaction with the environment - perceive to determine initial state, but then just execute plan - no relevance checks

### Partial order planning

#### Situation Calculus

- way of describng change in first order logic
- world viewed as a sequence of situations - snapshots of the state of the world
- Situations generated from previous situations by actions

- Functions and predicates that change - called fluents with time given a situation argument. Those that do not change are called eternal or atemporal
- change represented by function Result(action, situation) which denotes the result of performing action in situation

##### Axioms

- Possibility axioms - describes when it is possible to execute an action - Precondition $\implies$ POSS(a,s)
- Effect axioms - changes due to that action - POSS(a,x) $\implies$ changes

##### Planning in Situation Calculus

- can be seen as logical inference problem using siuation calculus
- Logical sentences to describe initial state, goal and operators
- Initial state - sentence about a situation $S_0$
- Goal state - logical query for suitable situations

![ ](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101122831378.png)

- Goal state is $\exists seq, At(G,[1,1], Result[seq, S_0])$ 
- From first possibility axiom - $Poss(Go(x,y), s)$ 
- From first effect Axiom - $At(Agent, y, Result(Go(x,y),s))$ 
- Agent cannot grab gold
  - Truth value of $At(Agent, y, Result(Go(x,y),s))$ is unknown
  - Nothing in the KB says the location of the gold remains unchanged

#### Frame Axioms

- Need to deescribe how the world stays the same
- Non changes due to an action
- If there are $F$ fluents and $A$ actions it requires $O(AF)$ fram axioms
- the frame problem is a long standing problem in AI
  - Representational - proliferation of frame axioms 
    - Representational problem is largely solves
  - Inferential - having to carry properties through inference steps even if remaining unchanged
    - Avoided by planning - we do not address it for inference systems

#### Successor state axioms

- Solve the representational frame action
- Each axiom is about a predicate, not an action
- General form - $P$ true afterwards $\equiv$ (an action made $P$ true $\or$ $P$ already true and no action made $P$ false)
- Need a successor state axiom for each predicate that can change over time
- Axiom must list all ways the predicate can become true or false

For example:

$Poss(a,s) \implies At(o,y,Result(a,s)) \Longleftrightarrow (a = Go(x,y)\and(o=Agent \or Holding(o,s)) $ $\or (At(o,y,s))\and ¬(\exists z:y\neq z \and a = Go(y,z)\and (o=Agent\or Holding(o,s))))$ - if the action is for the agent to move, then objects other than the Agent, that are not held by the agent remain where they are

#### Restricted language

- Theoretically this is all that is required - unpractical - time, space, semi-decidability
- To make practical we use a restricted language
- Reduces the possible solutions to search through
- Actions represented n a restricted language - allows creation of efficient planning algorithms
- Need a language and a planning algorithm for that

#### STRIPS

- Most planners use the STRIPS language or extensions
- NB use a planner rather than a general purpose theorem prover
- Stanford Research Institute Problem Solver - a planner not a problem solver
- States = conjunctions of function -free ground literals - predicates applied to constants
- State descriptions may be incomplete
- Closed-world assumption - most planners assume that if state description does not mention a positive literal, can assume to be false (can be dangerous)
- Goals = conjunctions of literals - may contain variables
- Planner - asks for a sequence of actions that make the goal true if executed

##### Operators

- comprise three components
  - Action
  - Precondition
  - Effect
- Preconditions are conjunctions of positive literals
- Effects are conjunctions of function free literals
- No explicit situation information - preconditions implicitly refer to the situation immediately before action, and the effects to the result of the action
- STRIPS divided effects into add list and delete list according to positive or negative



- Can be represented graphically:

  - OP (Action:Buy(X))
  - Precond: At(p) $\and$ Sells(p,x)
  - Effect: Have(x)

  

![image-20211101132720556](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101132720556.png)

An operator with variablres in an operator scehma - a family of actions requiring instantiation

An operator is applicable in state $s$ if we can instantiate each variable to that the precondition is true in $s$



#### State space vs plan space

- standard seach - node = concrete world state

- Planning search -  node = partial plan

  Seach through the space of plans

- An open preconidition is a precondition not yet fulfilled

- Operates on partial plans 

  - Add a link from an existitng action to an open condition
  - Add a step to fufill an open condition
  - Order one step with respect to another

- Gradually move from incomplete/vague plans to complete correct plans





#### Planning Terminology

- progression - start from initial siutation and search forward to the goal - large branching factor and search space
- Regression - search back from the goal to the initial situation - reduces branching factor, but complicated in conjunctions in goals and ensuring all conjunctions achieved
- Partial plan - an incomplete plan, with some steps not instantiated
- Partial order planning - some steps are ordered with respect to others

- Total order - all steps ordered

#### Partial ordered plans

- Principal of least commitment - leves choices as long as possible
- Comprised of
  - Set of steps - corresponding to operators
  - Set of ordering constraints on steps $S_i \prec S_j$
  - Set pf variable bindings
  - Set of casual links $S_i\rightarrow^{c} S_j$ - $S_i$ achieves c for $S_j$
  - Once all variables are bound - fully instantiated plan
  - A plan is complete iff every precondition is achieved
  - A precondition is achieved iff it is the effect of an earlier step and no possibility intervening step undoes it
  - A plan is consistent iff there are no contradiction in ordering or binding constraints



A problem defined by a partial plan containing just start and finish

- Initial state is the effect of start
- Goal is the prcondition of finish
- Ordering constraints added as arrows between actions

##### OVERVIEW

- POP
- Regression planner to search through plan space
- Each iter add a step to pachieve preconditions backtrack if inconsistent
- Only add steps to achieve causal links - without breaking other links - links are pprotected
- POP is sound, complete and systematic

###### Initial state

- Defined by operator
  - A special step with no preconditions, but has the effects which describe the start state
    - Op(Action, Effect)

###### Goal state

- Defined by operator: 
  - Op(Action : Finish, precond:) - the state of the world we want to achieve, no effect

###### Actions

Operators describes in strips - they have a name, an action, a precondition and an effect.

The variables need to be bound to constants for he plan to be fully instantiated and executed



#### Planning Algorithm

- start with a minimal partial plan
- Each iteration find a step to achieve the precondition  $c$ of $S_{need}$ 
- Do this by choosing an operator to achieve the precondition
- Record casual link to the newly achieve precondition
- Resolve any threats to causal links
- If fail to find operator or resolve threat to causal link then backtrack

![image-20211101134832299](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101134832299.png)

![image-20211101172101457](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101172101457.png)

![image-20211101172158147](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101172158147.png)

#### Clobbering

- A threat, or clobber is a potentially intervening step that destroys the condition achieved by a causal link

- Causal links are protected
- Protect them, by ensuring that threats - steps which might delete the link are ordered before or after

#### Block world example

Start state:

- On(c,a) On(A,table), Clear(b), On(B,Table), Clear(C)

- Now must find a step with open precondition:
  - Only the finish step has this. Either need On(A,B) or On(B,C)

- Pick On(B,C)
  - Clear(b), N(B,z), Clear(C)
  - First establishes a single causal link - On(B,C)
- Clear(c)
  - Establish causal link from start step - clear(c)
  - Assign table $z$ to the table
  - Clear(b) is also 

- On(A,B)
  - Cant be achieved from exisitng
  - PutOn(A,B) - this clobbers clear(b)
  - Adding an ordering link, keep iterating and backtracking are all fufilled

Finish:

- On(A,B), On(B, C)

![image-20211101173340922](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211101173340922.png)

**Each iteration represents a single causal link **



### Problems for STRIPS and POP

#### hierarchical plans

- Want to specify plans at different levels of detail
- have several levels before reaching executeable action - several iterations of solution
- Makes computation manageable and resulting plan understandable
- Allows human specification of abstract and partial plans to guide the planner
- Often want to give the planner guidance - especially on safety critical applications

#### Complex conditions

- although oeprators contain variables, there is no quantification
- STRIPS use of variables is limited - cannot express that Pickup(carrierbag) causes all to be lifte
  - operators are unconditional - cannot express an action of having different effects according to conditions
  - STRIPS cannot express that Pickup(carrierbag) pics up objects if bag not overloaded otherwise dumps them on the floor

#### Time

- In situational calculus time is discreet and action occur instantaneously
- Need to represent thata actions take time, may only be applicable at certain times, and the goal may have a deadline

#### Resources

- real problems have limited resources - money, time, quantitiy, machiner
- Actions have a cost - need to represent cst
- Action descriptions need to represent the resource requirements of performing the action

- planning must handle constraints on the resources

#### Hierarchical decomposition

- POP allows production of solutions at a high level
- Such solutions are removed from an agents effectors
- In order to execute a low level solution is needed
- Length of low level solution means that the space of plans is sufficieantly large for standard POP to struggle



Need to extend STRIPS To include non primitive operators and modify planner to replace nonprimitives with decomposition

- Solution is to intriduce abstract operators that can be decomposed into steps to implement them
- Decompositions are predetermined and stores in a library of plans - works best when ther are several possible decompositions

#### Extending the language

A  plan $p$ correctly implements a non primitive operator o if it is a complete and consistent plan for achieving the effects of the  o given the preconditions of o 

- p must be consistent
- Each effect of o must be asserted by a step of p 
- each precondition of steps in p must be achieved by a previous step or be a precondition of o

Guarantees that a nonprimitive operator can be replaces by its decomposition in the plan

- Still need to check threats when introducing new steps from the decomposition

#### Modifying the planner

- Every iteration - try too add a step and a resolve the on primitive
- Solution check - Must check that all operators are primitive

![image-20211102092301266](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102092301266.png)





- Steps: add steps of the method to the plan, remove $S{nonprim}$
- Bindings -   add bindings of the method to the plan, fail and back track if there is a contradiction
- orderings - following the least commitment - replace $S_a \prec S_{nonprim}$ by ordering $S_a$ before latest steps of the method - replace $S_{nonprim} \prec S_z$ by ordering $S_z$ after eatlist steps of the method. Then resolve threats - could invlove adding more ordering constraints
- Links - replace links to the non primitive with links to the steps of the method thata achieve the precondition

#### Broadening Operator descreiptions

- make operators more expressive to widen the application
- Conditional effect - avoids premature commitment - have effect condition
- Conditional effects have the form - EFFECT:/…$\wedge ¬Clear(y)$ when $y\neq Table$ 
- In SelectSubgoal must consider preconditions of conditional effects if the effec supplies a protected causal link
- In ResolveThreats any steps having effect $¬c$ $when$ $p$  is a possible threat to link: $S_i \rightarrow^c S_k$ - resolve threat by ensuring that $p$ is not true - confrontation
- Allow negated goals - ability to call ChooseOperator with goal not p
- Must treat initial state different - avoid representing all conditions - consider not p matched by explicit effect or initial state if it does not contain p
- Disjunctive preconditions - allow SelectSubgoal to make a nondeterministic choice between disjuncts- use principle of least commitment
- Disjunctive effects - introduces nondeterminism - may be able to address with coercion - shooting the Wumpus for example 
- Universally quentified preconditions - instead of Clear(b), we can use $\forall$ for example
- Allow universally quantified effects
- Restricted form of first order logic - worl is finite, static and objects have types. The initial state must give all objects a type
- Preconditions and effects of form: $\forall x \cdot T(x) \implies C(x)$ - $T$ is type, and $C$ is condition
- since the world is finite, static and types, can always expand universal quantification into a conjunction

#### Resource constraints

- Most problems have resources, and limit on how much can be consumed
- Introduce numeric values - measures 
- In situation calculus have $FuelLevel(S_n)$ but came make situation implicit - FuelLevel. Called measure fluent
- Some measure the planner has no control over
- Others are resources such as FuelLevel that can be produced and consumed 

- Introduce inequality test in operator preconditions
- Allow assignments of measures in the operators effects

- General idea - plan for scarce resources first, delaying choice of causal links where possible 
- Pick the steps of the plan first, then do a rough check to see if resource requirements are satisfiable
- If so, continue by resolving threats
- Allows us to check for failure without a finished plan

#### Temporal contraints

- Time can generally be treated as s resource
- Initial state must specify that time
- For each operator, specify how much time is takes in its effects
- Two main differences
  - Parallel operators cost the maximum of the respective times, not the sum
  - Time never goes backwards - if a deadline and a partial plan that goes over the deadline, there is no point in continuing with the plan - more cannot be produces

### Real World

- so far assumed that the world is observable and deterministic
- Also assumed that action descriptions are correct and complete
- Real world is not like this
- Often have to deal with incomplete and incorrect information

#### Fixing a flat tire:

- there is a flat tire, and a spare tire which is not flat

Things can go wrong here:

- we do not know whether tire 1 is intact
- Disjunctive effects - inflating the tire can either inflate, or the pump can break, or the tire can burst
- Incorrect information is not correct - the spare tire is flat,  missing/ incorrect postconditions in the operators
- Qualification problem - never finish listing all preconditions and possible outcomes of actions

![image-20211102094446961](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102094446961.png)

#### Solutions

- Conditional planning
  - Plan to obtain information - sensing actions
  - Subplan for each contingency
  - Expensive, as produces many unlikely cases
- Monitoring/replanning
  - Assume notmal states and outcomes
  - Check the progress during execution replan if necessary
  - Unanticipated outcomes may lead to failiure
- Typical a combination is needed

##### Conditional planning

- Execution - check p against KB, execute then or else
- Conditional planning - like POP but, if an open condition can be established by an observation action
  - Add the action to the plan
  - Complete the plan for each possible observation outcome
  - Insert a conditional step with these subplans

The key difference is that steps have a context, at execution the agent must know the state of the condition.

- To ensure the plan is executable, insert actions to find information
- But sensing actions may have other effects - checking tire by putting it in water makes it wet

###### Example

- Initial state 

![image-20211102095502730](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095502730.png)

- Teo open conditions to stisfy
- On(x) satisfied by link from start tteps with $\{x/Tire1\}$
- Inflated(x) satisfied by adding inflate(Tire1) which has preconditions Flat(Tire1)and intact(Tire1) 

![image-20211102095628260](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095628260.png)

- The plan to inflate Tire1 will on work if Tire12 is intact
- Add sensing action Che3dck(Tire1)
- Context(Intact(Tire1)) added to the finish step, since it assumes Tire1 is intact
  - Alls teps are annotated with the context that Tire1 is intact
  - There is another outcome where Tire 1 is not intact

![image-20211102095723669](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095723669.png)

- Need to be able to cope with Tire 1 not being intact 
  - Add a second step to the contact

![image-20211102095821355](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095821355.png)

- Finally satisfy On(Spare) by adding Remove(Tire1) and PutOn(Spare)
- make conditional link froe check(Tire 1) to remove(Tire1) threatening causal link protecting On(Tire1) in the first step

![image-20211102095944557](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211102095944557.png)

Here we needed to add finish steps for each possible observation

##### Parameterised Plans

- A sensing action may have any number of outcomes - checking the colour of an object zz













k
