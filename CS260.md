# CS260 - Algorithms



## Greedy Algorithms For Scheduling Problems

------------------

### Interval Scheduling Problems

#### Interval Scheduling Problem

- Job  $$j $$ starts at $$ s_{j} $$ and finishes at $$ f_{j} $$ 
- Two jobs are compatible if they don’t overlap
- Goal: find the maximum subset of mutually compatible jobs#

**Inputs:** Sequences of jobs 

**Outputs:**  maximum subset of mutually compatible jobs.

------------------

##### Earliest-Finish-Time-First Algorithm

$$ Sort $$ jobs by finish times a renumber so that $$ f_{1} \leq f_{2} \leq ... \leq f_{n}  $$

$$ S \leftarrow \emptyset $$

$$ FOR $$ $$ j = 1$$  $$TO$$  $$  n $$

​		$$ IF$$ (job $$j$$  is compatible with $$S$$)

​				$$S \leftarrow S \cup \{j\}$$

$$RETURN$$ $$S$$

-----------------

**Proposition** Can implement earliest finish time first in $$O(n log n)$$

- Keep track of job $$j*$$ that was added last to $$S$$
- Job $$j$$ is compatible with $$S$$ iff $$s_{j} \leq f_{j*}$$

- Sorting by finish times takes $$O(n log n)$$

- The body of the for loop  can be completed in constant time



Algorithm creates a set $$S$$ that are non-overlapping

- When we are considering a new item $$j$$, we know its start and finish time
- We know that the finish time is after the finish time of $$j*$$, all we need to do here to verify if the start time of $$j$$ is larger or equal to the start time of job $$j*$$

- By the use of a variable to store the last finish time, this allows the check to be done in $$O(1)$$ time.

-------------------------------

##### Proof of correctness of the earliest time first algorithm

Theorem: The earliest finish time first algorithm is an optimal compatible set of jobs.

Proof - by contradiction.





--------------------------

#### Interval Partitioning Problem

- lecture $$j$$ starts at $$s_{j}$$ and finishes at $$f_{j}$$.
- Goal: find the minimum number of classrooms to schedule all lectures so that no two lectures occur at the same time in the same room.



-----------------

##### Interval Partitioning Algorithm 

This is a greedy algorithm because for every lecture based on a simple rule, we allocate to one of the classrooms allocated so far, and never change the decision.

**Inputs:** Sequences of pairs $$(s_{j}, f_{j})$$ 

**Outputs: ** A minimum number of lists with sequences of pairs: $(s_{j}, f_{j})$

$EarliestStartTimeFirst(n,s_1,s_2,…,s_n,f_n,f_2,…,f_n)$ 

$Sort$ lectures by start times and renumber

$d \leftarrow 0$ - Number of allocated classrooms

**for** $j=1$ **to** $n$

​	**if** (lecture $j$ is compatible with some classroom)

​		Schedule lecture $j$ in any such classroom $k$ 

​	**else** 

​		Allocate a new classroom $d+1$ 

​		Schedule lecture $j$ in classroom $d+1$ 

​		$d\leftarrow d+1$ 

**return**  $schedule$ 

-------------------------------------

##### Proposition

- “To maintain a PQ in which we store all the classrooms that have been considered so far, in which the keys are the finish times of the last jobs scheduled in each of those classrooms respectively. The PQ find min operation will allow us to find the classroom and lecture $(i*)$ that has the smallest finish time $(f_{i*})$ of all lectures that have been allocated to any of the classrooms so far. Assume that $(i*)$ is the lecture allocated to one of the classrooms which is the last lecture in the classroom chronologically speaking, and whose finish time is the smallest of all other lectures that are last in the other classrooms.”
  - In order to compare if the next lecture is compatible, it is sufficient to compare $f_{i*}<s_j$, to check compatibility.
  - When $s_j <f_{i*}$, then it is possible to determine that lecture $j$ is incompatible with all classrooms $d$, therefore must be allocated to $d+1$
  - The start time of $s_i \leq s_j < f_{i*} $ 
  - There is a time soon after $s_j$ denoted as: $s_j + \varepsilon : s_j < s_j + \varepsilon < f_{i*}$. this is a time where $j$ is taking place, and all the lectures in classrooms $1$ to $d$ are taking place at this time, as well as lecture $j$.
- The time complexity depends on the allocation
- If  a suitable data structure is used (a PQ) this can be implemented and maintained efficiently with each decision and iteration.

-------------------------------

##### Proof of correctness

**Interval partitioning:** lower bound on optimal solution:

- DEF The **depth** of a set of open intervals is the maximum number of intervals that contain any given point
  - KOBV Number of classrooms $\geq$ depth

![image-20211005112726393](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211005112726393.png)

- As shown above, the depth above is 3.



Question: is the depth also an upper bound?

- Yes, Moreover, ESTF algorithm always finds a schedule whose number of classrooms equals the depth.

-------------------------------

**Begin Proof**

- Let $d =$ number of classrooms that the algorithm allocates
- Classroom $d$ is opened because we need to schedule a lecture, say $j$, that is incompatible with a lecture in each of $d-1$ other classrooms.
- Therefore, these $d$ lectures each end after $s_j$.
- Since we sorted by start time, each of the incompatible lectures start not later than $s_r$.
- Thus, we have $d$ lecture overlapping at time $s_j + \varepsilon$.
- Key Observation $\Rightarrow$ all schedule use $\geq d$ classrooms. 

![image-20211005113509305](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211005113509305.png)

- Whenever the algorithm generates a new classroom, it exhibits why the depth is at least d.

--------------------

### Scheduling to minimizing lateness

- Single resource processes one job at a time
- Job $j$ requires $t_j$ units of processing time and is due at time $d_j$.
- If $j$ starts at time $s_j$, it finishes at time $f_j=s_j+t_j$
- Lateness: $l_j=max\{0,f_j-d_j\}$ - if positive, this is the amount of time that the job misses the schedule
- Goal: schedule all jobs to minimize **maximum** lateness $L = max_jl_j$ 

- **Output:** sequence of numbers. 

--------------------------

#### Minimizing lateness: earliest deadline first

##### Algorithm

$EarliestDeadlineFirst(n, t_1, t_2, ..., t_n, d_1,d_2,...,d_n)$

$Sort$ jobs by due times and renumber so that $d_1 \leq d_2 \leq … \leq d_n$ 

$t\leftarrow 0$

**for** $j = 1$ **to** $n$ 

​	Assign job $j$ to interval $[t, t+t_j]$ 

​	$s_j \leftarrow t; f_j \leftarrow t + t_j$

​	$t\leftarrow t + t_j$ 

**return** intervals $[s_1,f_1],[s_2,f_2],…,[s_n,f_n]$ 

----------------------

##### Minimizing lateness

**No Idle Time**

- **Observation 1** - there exists and optimal schedule with no idle time
- **Observation 2** - the EDF schedule has no idle time.

**Inversions**

- **Definition** Given a shcedule $S$, an **inversion** is a pair of jobs $i,j : i < j$, but $j$ is scheduled before $i$.

  - We assume the jobs are numbered so that $d_1,\leq d_2, \leq … \leq d_n$

- **Observation 3** - the EDG schedule is the unique idle-free schedule with no inversions. 

  - If there is any other schedule, there may be an inversion. 

- **Observation 4** - if an idle-free schedule has an inversion, then it has an adjacent inversion.

  - **Proof**

    - Left $i-j$ be a a closest inversion.
    - Let $k$ be element immediately to the right of $j$.
    - Case 1. $[j >k]$, Then $j-k$ is an adjacent inversion.
    - Case 2. $[j<k$] Then $i-k$ is a closer inversion since $i<j<k$

     $\square$ 

- **Key Claim** Exchanging two adjacent, inverted jobs $i,j$ reduces the number of inversions by 1 and does not increase the max lateness. - the lateness of the new $j$ is upper bounded by the lateness of $i$ in the old schedule

  - Proof
    - $l^{'}_k = l_k \forall k \neq i,j$
    - $ l^{'}_i  \leq l_i$
    - If job $j$ is late:  - the lateness 
      - $l^{'}_j = f^{'}_j - d_j$                    $\longleftarrow$ Definition
      - $  = f_i-d_j$                         $\longleftarrow$ $j$ now finishes at time $f_i$ 
      - $\leq f_i - d_i$                         $\longleftarrow$ $i<j \Rightarrow d_i \leq d_j$
      - $\leq l_i$                                   $\longleftarrow$ Definition

------------------

##### Proof 

**Theorem** The earliest-deadline-first schedule $S$ is optimal.

Proof - by contradiction.

Define $S^*$ to be an optimal schedule with the fewest inverions.

- Can assume $S^*$ has no idle time. $\longleftarrow$ Observation 1
- Case 1. $[S^*$ has no inversions$]$ Then $S = S^*$ $\longleftarrow$ Observation 3
- Case 2. $[S^*$ has an inversion$]$
  - let $i-j$ be an adjacent inversion $\longleftarrow$ Observation 4
  - exchanging jobs $i,j$ decreases the number of inversions by $1$ without increasing the max lateness. $\longleftarrow$ key claim
  - contradicts “fewest inversions” as part of the definition of $S^* $            $\square$ 

-------------------

### Proof Techniques For Correctness Of Greedy Algorithms

#### Greedy analysis strategies

**Greedy algorithm stays ahead** Show that after each step of the greedy algorithm, its solution is at least as good as any other algorithm’s.

**Structural** Discover a simple “structural” bound asserting that every possible solution must have a certain value. Then show that the algorithm always achieves this bound.

**Exchange Argument** Gradually transform any solution to the one found by the greedy algorithm without hurting its quality

---------------------------

**Other greedy algorithms**. Gale-Shapley, Kruskal, Prim, Dijkstra, Huffman, …

### Stable Matching Problem

- Given a set of preferences among employers and applicants, we assign applicants to employers so that for every employer, and every applicant  who is not scheduled to work for  at least one of the following is true.
  -  prefers every on eof its accepted applicants to  or
  -  prefers their current situation over working for  
- If this is true, the outcome is stable (no one will want to change).

#### **Formulating the problem** - whenever trying to get to the essence of a problem, making it as “clean” as possible helps.

In this case:

- Every applicant is looking for 1 company
- Every company looking for many applicants
- May be more or less applicants than spaces
- Applicants may or may not apply to every company

Therefore, to eliminate this, each of the  applicants applies to each of the  companies - each company wants only a single applicant. 

- This in effect, leaves us with two genders (Men and Women)



Therefore, consider a set  and  

-  is the set of all ordered pairs of the form  where  
- The set  is a set of ordered pairs from  where each member of  and each member of  appears in at most one pair in .
- A perfect matching set  is a matching with the property that each member of  and each member of  appear in exactly one pair in  
- After this, the notion of preference can be added. Each man  ranks all women, and vice versa.
- Given a perfect matching set , and there are two pairs: , but  each prefer each other then there is nothing from stopping them from changing
  - This is an instability.
- This creates a goal of making a set of stable pairings.
- A set  is stable if:
  - It is perfect
  - There is no instability
- Two Questions:
  - Does there exist a stable matching for every set of preference lists?
  - Given a set of preference lists, can we efficiently construct a stable matching (if it exists)?

#### **Designing the Algorithm**

Some basic ideas motivating the algorithm:

- Initially everyone is unmarried. If an unmarried man  chooses a woman , who ranks highest on his preference list and proposes. They should not instantly pair, as in future someone who ranks higher on ‘s list may propose. And they should not instantly reject. This may be the highest ranked proposal that  gets. 
  - Add an engagement state.
- Now there is a state where some men and women are engaged and some are not. The next step could be as follows.
  - An arbitrary free man  chooses the highest ranked woman  to whom he has not yet proposed. If  is not engaged, they get engaged. Else,  determines which of the men  or  rank higher.
- Finally, the algorithm terminates when no-one is free - the perfect matching is returned.

##### **PSEUDOCODE**

Initially all  are free

**while**  who is free and hasn’t proposed to every woman

​	Choose a man  

​	Let  be the highest ranked woman in the  preference list to whom they have not yet proposed

​	**If**  is free 

​		**then**  get engaged

​	**else**  is engaged to  

​		**if**  prefers 

​			**then** remains free

​		**else** 

​			 become engaged

​			 becomes free

**return** the set  of engaged pairs

**Note** This algorithm returns a set that is a stable matching.

-----------------------

## Greedy Algorithms for Shortest Paths and Minimum Spanning Trees

### Shortest Path Problems - Extremely Wide Range of Applications 

- Single Pair Shortest Path Problem
  - Given a digraph $G=(V,E)$, edge lengths $l_e\geq 0$, source $s\in V$ and destination $t\in V$, find the shortest path from $s$ to $t$

- Single Source Shortest Paths Problem
  - Given a digraph $G=(V,E)$, edge lengths $l_e\geq 0$, source $s\in V$, find the shortest path from $s$ to every node.
  - Forms a tree routed at $s$

--------------

### Dijkstra's Algorithm

#### The problem

 given nodes $u,v$ what is the shortest $u-v$ path, or given a node $s$, what is the shortest path from $s$ to any other point in the graph.

The setup of the graph is as follows: 

- Given a directed graph $G=(V,E)$ with a designated start node $s$.
  - We assume that $s$ has a path to every other node in $G$. Each edge $e$ has length $l_e \geq 0$, indicating time or cost to taverse that edge.
- For a path $P$, the length of $P$ is denoted as $l(P)$ and is the sum of the length of all the edges in that path..
- **Our Goal:** the goal is to find the shortest length from $s$ to every other node in the graph $G$.
- **Note** - this is meant for directed graphs, but also works with undirected ones, where each edge $e=(u,v)$ must be replaces with the edges $(u,v),(v,u)$ with the same length.

#### Designing the Algorithm

- We begin by describing an algorithm that determines the length of the shortest path from $s$ ti each ither nide un the graph. (This makes it easier to produce other paths as well).
- The algorithm maintains a set $S$ of vertices $u$ for whivh we have determined a shortest path distance $d(u)$ from $s$ 
  - This is the explored part of the graph.
  - Initially $S = \{s\}$, and $d(s) = 0$ 
- For each node $v\in V-S$, we determine the shortest path that can be constructed by travelling along a path through the explored part $S$ to some $u\in S$ , followed by the single edge $(u,v)$.
  - Essentially we consider the quantity: $\pi ’(v) = min_{e=(u,v):u\in S} d(u) + l_e$ 

![image-20211010120251834](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010120251834.png)

-----------------

It is simple to produce the $s-u$ paths corresponding distances found by Dijkstra’s algorithm.

- As each node $v$ is added to the set, we recored the edge $(u,v)$ on which it achieved the value:  $d’(v) = min_{e=(u,v):u\in S} d(u) + l_e$ 
- The path $P_v$ is implicitly represented by these edges:
  - If $(u,v)$ is the edge stored for $v$, then $P_v$ is (recursively) the path $P_u$ followed by the single edge $(u,v)$.
  - Essentially: to construct $P_v$, we start at $v$, follow the edge stored for $v$ in the reverse direction to $u$; then follow the edge we have stored for $u$ in the reverse direction to its predecessor and so on, until $s$ is reache. 
    - **Note** $s$ must be reached, as it is a backward walk from a path that originated from $s$.

- Essentially, the next path taken will be at the smallest total distance from the origin node $s$.



#### Analysing the algorithm 

Dijkstra’s Algorithm is greedy in the sense that we always form the shortest new $s-v$ path we can make from the path $S$ followed by a single edge. To prove its correctness, we show that it:

- Stays ahead - each time it selects a path to a node, that path is shorter than every other possible path.
- Consider the set $S$ at any point un the algorithms execution. For each $u\in S$, the path $P_u$ is a shortest path.

This established correctness, since this can be applied at the algorithm’s termination, where $S$ includes all nodes.

----------------

##### Proof - From Lecture Notes

Invariant: For each node $u \in S:d[u] =$ length of a shortest path $s\rightarrow u$ path.

**Induction on** $|S|$

**Base Case**: $|S| = 1$ is trivial since $S=\{s\}$ and $d[s]=0$ 

**Inductive Hypothesis**: assume true for $|S| \geq 1$

- Let $v$ be next node added to $S$, and let $(u,v)$ be the final edge
- A shortest path $s \rightarrow u$ path plus $(u,v)$ is an $s\rightarrow v$ path of length $\pi(v)$
- Consider any other  $s\rightarrow v$ path $P$. We show that it is no shorter than  $\pi(v)$
- Let $e=(x,y)$ be the first edge in $P$ that leaves $S$ and let $P’$ be the fsubpath from $s$ to $x$
- The length of $P$ is already $\geq \pi(v)$ as soon as it reaches $y$

  $l(p) \geq l(P) + l_e \geq d[x] + l_e \geq \pi(y) \geq \pi(v)$

1. Non negative lengths, 2. inductive hypothesis, 3. definition of $\pi(y)$ 4. Dijkstra chose $v$ instead of $y$

![image-20211010121720419](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010121720419.png)

-----------

##### **Proof** - From Book

Induction on the size of $S$

The case $|S| = 1$ is easy:

- $S = \{s\}$ and $d(s)= 0$

Suppose that the claim holds when $|S| = k$ for some value of $k \geq 1$, we now grow S to size $k+1$ by adding the node $v$

- Let $(u,v)$ be the final edge on our $s-v$ path$P_v$. 

- By the induction hypothesis, $P_u$ is the shortest $s-u$ path for each $u\in S$
- Now consider any other $s-v$ path $P$, this path $P$ must leave the set $S$ somewhere.
- Let $y$ be the first node on $P$ that is not in $S$, and let $x\in S$ be the node just before $y$

- $P$ cannot be shorter than $P_u$ because it is already at least as long as $P_v$ by the time it has left the set $S$.
- In iteration $k+1$ the algorithm must have considered adding node $y$ to the set $S$ via the edge $(x,y)$ and rejecting this in favour of adding $v$
  - This means that there is no path fron $s$ to $y$ through $x$ that is shorter than $P_v$, but the subpath of $P$ up to $y$ is such a path, therefore, this subpath is at least as long as $P_v$.
  - Lengths are non-negative, therefore the full path $P$ is at least as long as $P_v$                 $\square$ 

----------------

**Notes on the proof**

- The algorithm does not find shortes paths if some of the edges can have negative lengths.
- Dijkstra’s Algorithm is essentially a wave front reaching the shortest path to each node. (See page 140 Algorithm Design)

----------

#### Implementation and Running Time

When considering it’s running time, there are $n-1$ iterations of the **while** loop for a graph with $n$ nodes. Each iteration adds a new node $v$ to $S$. Selecting the correct node $v$ efficiently is the more difficult issue.

- Initially, it appears that one must consider every node $v\notin S$ and go through all edges between $S,v$ to determine the minimum $min_{e=(u,v):u\in S} d(u) + l_e$ to select the node $v$ for which this minimum is smallest.
- For a graph with $m$ edges, this takes $O(m)$ time, meaning that the algorithm has a complexity of $O(mn)$ 

However, using the correct data structure can drastically improve this.

- 1st, we explicitly maintain the values of the minima $d’(v) = min_{e=(u,v):u\in S} d(u) + l_e$ for each node $v\in V-S$, rather than recalculating them in every iteration.
- 2nd, the nodes $V-S$ can be kept in a PQ with $d’(v)$ as their keys. (PQs discussed in Chapter 2 of this book and CS126 last year)
- A PQ can efficiently insert and delete elements, change an element’s key and extract the minimum element.
- For this, we require $ChangeKey$ and $ExtractMin$ 

There

-------------

##### **Implementation In a Priority Queue**

- Put the nodes $V$ in a PQ with $d’(v)$ as the key for $v\in V$ 
- To select which node should be added to $S$, we use $ExtractMin$ 
- To update the keys, consider an iteration where:
  -  node $v$ is added to $S$
  -  Let $w\notin S$ be a node that remains in the PQ
  -  To update the value of $d’(w)$:
     - If $(v,w)$ is not an edge, nothing needs be done: the set of edges is considered in the **minimum** $min_{e=(u,v):u\in S} d(u) + l_e$,, is exactly the same as before and after adding $v$ to $S$.
     - If $e’ = (v,w) \in E$, then the new value for the key is  $min(d’(w), d(v) + l_e)$. If $d’(w)>d(v) + l_e$ then the $ChangeKey$ operation is needed to decrease the key of node $w$ appropriately.
     - This can happen at most once per edge when the tail of $e’$ is added to $S$ 

This leaves us with:

- Using a PQ, the algorithm can be implemented on a graph with $n$ nodes and $m$ edges to run in $O(m)$ time, plus the time for $n$ $ExtractMin$ and $m$ $ChangeKey$ 
- Using the heap based PQ, each PQ operation can be run in $O(log (n))$ time. Therefore, the overall running time is: $O(m log (n))$ 

-----------------

##### Pseudocode

$Dijkstra(V,E,l,s)$

**foreach** $v \neq s: \pi[v] \leftarrow \infty, pred[v] \leftarrow null; \pi[s] \leftarrow 0 $

**Create** an empty priority queue $pq$

**foreach** $v\in V: Insert(pq,v,\pi[v])$ 

**while** ($isNotEmpty(pq)$)

​		$u \leftarrow RemoveMin(pq)$ 

​		**foreach** edge $e = (u,v) \in E$ leaving $u$

​				**if** $(\pi[v] > \pi[u] + l_e)$ 

​						$DecreaseKey(pq,v,\pi[u] + l_e)$ 

​						$\pi[v] \leftarrow \pi[u] + l_e ; pred[v] \leftarrow e$ 

------------------

##### Which Priority Queue

Performance: Depends on PQ: $n Insert(), n DeleteMin(), \leq DecreaseKey()$

- Array implementation optimal for dense graphs: $\Theta(n^2)$ edges
- Binary Heap much faster for sparse graphs: $\Theta(n)$ edges
- 4-way heap worth the trouble in performance crititcal situations.

| **Priority Queue**                              | **Insert**            | **DeleteMin()**       | **DecreaseKey()** | **Total**                    |
| ----------------------------------------------- | --------------------- | --------------------- | ----------------- | ---------------------------- |
| node-indexed array: ($A[i] = $ priority of $i$) | $O(1)$                | $O(n)$                | $O(1)$            | $O(n^2)$                     |
| binary heap                                     | $O(log(n))$           | $O(log(n))$           | $O(log(n))$       | $O(m \cdot log(n))$          |
| d-way heap                                      | $O(d \cdot log_d(n))$ | $O(d \cdot log_d(n))$ | $O(log_d(n))$     | $O(m\cdot log_{m/n}(n))$     |
| Fibonacci heap                                  | $O(1)$                | $O(log(n))$           | $O(1)$            | $O(m + n\cdot log(n))$       |
| Integer PQ                                      | $O(1)$                | $O(log(log(n)))$      | $O(1)$            | $O(m + n \cdot log(log(n)))$ |



------------------

### Minimum Spanning Tree Problem

#### Cycles and cutsets

##### Cycles

A cycle is a path with no repeated nodes or edges other than the starting and ending nodes.

##### Cuts

A **cut** is a partition of the nodes into two nonempty subset $S$ and $S-V$

A **cutset** of a cut $S$ is the set of edges with exactly one endpoint in $S$

![image-20211010140013693](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010140013693.png)

##### Cycle-cut insertion

**Proposition**  a cycle and a cutset intersect in an even number of edges:

![image-20211010140131997](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010140131997.png)

- This is for the cut $\{4,5,8\}$



Consider the cycle $C$, if the cycle is fully within or entirely outside of the cut $S$, then the intersection of the cycle and the cutset is empty.

If however, if there is some edge in the cycle, that goes out of the cut, then there must be an edge which goes back into the cut, as the cycle must come back to the original vertex, without repeating any other edges or vertices but the start vertex.

![image-20211010140227248](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010140227248.png)

#### Minimum Spanning Trees

##### Spanning tree definition

Let $H=(V,T)$ be a subgraph of an undirected $G=(V,E)$. $H$ is a spanning tree of $G$ if $H$ is both acyclic and connected.

##### Spanning Tree Properties

**Proposition** Let $H=(V,T)$ be a subgraph of an undirected graph $G=(V,E)$.

Then the following are equivalent:

- $H$ is a spanning tree of $G$ 
- $H$ is acyclic and connected
- $H$ is connected and has $|V| -1$ edges
- $H$ is acyclic and has $|V| -1$ edges
- $H$ is minimally connected: removal of any edge disconnects it
- $H$ is maximally acyclic: addition of any edge creates a cycle

##### Minimum Spanning Tree Definition

**Definition** Given a connected, undirected graph $G=(V,E)$ with edge costs $c_e$, a minimum spanning tree $(V,T)$ is a spanning tree of $G$ such that the sum of the edge costs in $T$ is minimized

**Cayley’s theorem** The complete graph on $n$ nodes has $n^{n-2}$ spanning trees (Brute force not applicable)

**Very useful for approximation algorithms for NP-hard problems**

##### Fundamental Cycle

**Property** Fundamental cycle: Let $H =(V,T) $be a spanning tree of $G=(V,E)$

- For any non tree edge $e\in E : T\cup \{e\}$ contains a unique cycle say $C$
- For any edge $f\in C:(V,T\cup \{e\}-\{f\})$ is a spanning tree

![image-20211010142153938](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010142153938.png)

**Observation:** If $c_e < c_f$, then $(V,T)$ is not an MST - the new tree has a smaller cost.

##### Fundamental Cutset

Let $H=(V,T)$ be a spanning tree of $G=(V,E)$

- For any tree edge $f\in T: (V,T-\{f\})$ has two connected components
- Let $D$ denote corresponding cutest.
- For any edge $e\in D : (V,T-\{f\}\cup \{e\})$ is a spanning tree

![image-20211010142900539](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010142900539.png)

**Observation** If $c_e < c_f$, then $(V,T)$ is not an MST

--------------------

##### The Greedy Algorithm

**Red Rule** 

- Let $C$ by a cycle with no red edges
- Select an uncolored edge of $C$ of max cost and colour it red.

**Blue Rule**

- Let $D$ be a cutset with no blue edges
- Select an uncoloured edge in $D$ of min cost and colour it blue

**Greedy Algorithm**

- Apply the red and blue ruled (nondeterministic ally) until all edged are coloured
  - The blue edges form an MST
- **Note** can stop once $n-1$ edges are coloured blue.

-----------------------

##### Proof of correctness for the Greedy Algorithm

**Colour Invariant** There exist an $MST(V,T*)$ containing every blue edge and no red edge.

**Proof** induction on the number of iterations

**Base Case** No Edged coloured $\Rightarrow$ every MST satisfies the invariant

**Induction step - blue rule**  Suppose colour invariant true before blue rule

- Let $D$ be chosen cutset, and let edge $f$ be edge coloured blue
- If $f\in T^*$, then $T^*$ still satisfies invariant
- Otherwise, consider fundamentay cycle $C$ by adding $f$ to $T^*$
- Let $e\in C$ be another edge in $D$
- $e$ is uncoloured and $c_e \geq c_f$ since
  - $e\in T^* \Rightarrow e$ not red
  - Blue rule $\Rightarrow e$ not blue and $c_e \geq c_f$
- Thus, $T^*\cup \{f\} -\{e\}$ satisfies invariant

![image-20211010144412303](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010144412303.png)

**Induction Step - red rule** Suppose colour invariant true before red rule

- Let $C$ be chosen cycl,e and let edge $e$ be edge coloured red.
- If $e\notin T^*$ then $T^*$ still satisfies invariant
- Otherwise, consider funamental cutest $D$ by deleting $e$ from $T^*$
- Let $f\in D$ be another edge in $C$
- $f$ is uncoloured and $c_e\geq c_f$ since
  -  $f\notin T^* \Rightarrow f$ not blue
  - red rule $\Rightarrow$ $f$ not red and $c_e \geq c_f$ 
- Thus, $T^*\cup \{f\}-\{e\}$ satisfies invariant $\blacksquare$ 

**Theorem** The greedy algorithm terminates. Blue edges form an MST

**Proof** We need to show that either the red or blue rule (or both) applies

- Suppose $e$ is uncoloured

- Blue edges form a forest

- Case 1: both endpoints of $e$ are in the same blue tree

  $\Rightarrow$ apply red rule to cycle formed by adding $e$ to the blue forest

![image-20211010145547948](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010145547948.png)

- Case 2: oth endpoints of $e$ are in different blue trees.

  $\Rightarrow$ apply blue rule to cutset induced by either of two blue trees. $\blacksquare$

---------------

##### Prim’s algorithm

Initialise $S=\{s\}$ for any node $s$, $T\neq \emptyset$ 

Repeat $n-1$ times:

- Add to $T$ a min-cost edge with exactly one endpoint in $S$
- Add the other endpoint to $S$

**Theorem** Prim’s algorithm computes an MST

**Proof** Special case of greedy algorithm (blue rule repeatedly applied to $S$) - by construction, edges in cutset are uncoloured

---------------

###### **Implementation**

$PRIM(V,E,c)$ 

$S\leftarrow \empty, T\leftarrow \empty$ 

$s\leftarrow $ any node in $V$

**foreach** $v\neq s: \pi[v] \leftarrow \infty, pred[v] \leftarrow null; \pi[s] \leftarrow 0$

**create** an empty priority queue $pq$ 

**foreach** $v\in V:Insert(pq,v,\pi[v])$  - $\pi[v]$ is the cost of the cheapest known edge between $v$ and $S$

**while** ($IsNotEmpty(pq)$)

​		$u\leftarrow RemoveMin(pq)$ 

​		$S\leftarrow S \cup \{u\}, T \leftarrow T \cup \{pred[u]\}$ 

​		**foreach** edge $e=(u,v) \in E$ with $v\notin S$ 

​				**if** ($c_e < \pi[v]$) 

​						$DecreaseKey(pq,v,c_e)$ 

​						$\pi[v] \leftarrow c_e: pred[v] \leftarrow e$ 



##### Kruskal’s Algorithm

Consider edges in ascending order of cost:

- Add to tree unless it would create a cycle.

Theorem: Kruskal’s algorithm computes an MST

Proof: Special case of greedy algorithm:

- Case 1: both endpoints of $e$ in the same blue tree.	

  $\Rightarrow$ color $e$ red by applying red rule to unique cycle (all other edges in the cycle are blue)

- Case 2: both endpoints of $e$ in different blue trees

  $\Rightarrow$ colour $e$ blue by applying blue rule to cutset defined by either tree. $\blacksquare$ 

![image-20211010163144694](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211010163144694.png)

###### Pseudocode

**Theorem** Kruskal’s algorithm can be implemented in $O(m \cdot log(m))$ time

- Sort edges by cost
- Use union-find data structure to dynamically maintain connected components



$Kruskal(V,E,c)$ 

$Sort$ $m$ edges by cost and renumber so that $c(e_1) \leq c(e_2) \leq … \leq c(e_m)$

$T\leftarrow \empty$ 

**foreach** $v\in V:MakeSet(v)$ 

**for** $i=1$ **to** $m$

​		$(u,v) \leftarrow e_i$ 

​		**if** $(FindSet(u) \neq FindSet(v))$    - are $u,v$ in the same component

​				$T\leftarrow T \cup \{e_i\}$

​				$Union(u,v)$   - make $u,v$ in the same component

**return** $T$



##### Reverse Delete Algorithm

Start with all edges in $T$ and consider them in descending order of cost:

- Delete edge from $T$ unless it would disconnect $T$

**Theorem** the reverse delete algorithm computes an MST

**Proof** Special case of greedy algorithm

- Case 1: (Deleting edge $e$ does not disconnect $T$)

  $\Rightarrow$ apply red rule to cycle $C$ formed by adding $e$ to another path in $T$ between its two endpoints

- Case 2: (Deleting edge $e$ disconnects $T$)

  $\Rightarrow$ apply blue rule to cutset $D$ included by either component $\blacksquare$ 



**Fact:** Can be implemented in $O(m \cdot log(n \cdot log(log(n))^3))$ time - almost as fast as the others. Useful for clustering



-----------------

#### Minimum Spanning Tree Problem - Additional reading

##### The Problem

- Suppose we have a set of location $V - (v_1, v_2,..,v_n)$, and we want to build a communication network on top of them. The network should be connected - there should be a path between every pair of nodes - however, this should be done as cheaply as possible.
- For some pairs $(v_i,v_j)$, we may connect an edge directly between them, for a certain cost: $c(v_i,v_j)  > 0$.
  - Therefore, we can represent the set of possible links that may be built using a graph $G=(V,E)$ with a positive cost associated with each edge.
  - The problem is to find a subset of the edges $T\subseteq E$ so the graph $(V,T)$ is connected, and the total cost of all edges is as small as possible.
    - **Assumption** - That the graph $G$ is connected or no possible solution
- **Observation** Let $T$ be the minimum-cost solution to the network design problem, then $(V,T$)$ is a tree.
- **Proof** : By definition, $(V,T)$ must be connected; we show it also contains no cycles.
  - If it contained a cycle $C$, and let $e$ be any edge on $C$.
  - $(V,T-\{e\})$ is still connected, as it can go the “long” way round.
  - Therefore, it follows that $(V,T -\{e\})$ is a solution.
  - $\square$
- If there are any edges with cost 0, then the solution may have extra edges, as it can still be minimum cost. Despite this, there still is a minimum solution that is a tree
  - Starting from any minimum cost solution, we  could keep deleting edges on cycles until there is a tree.
- We call a subset $T\subseteq E$ a spanning tree of $G$ if $(V,T)$ is a tree. The statement “Let $T$ be a minimum cost solution to the network design problem, Then $(V,T)$ is a tree.”
  - This says the goal of the network design problem can be rephased that of finding the cheapest spanning tree of the graph - called the minimum spanning tree problem.
  - Unless G is very simple, it will have exponentially many different spanning trees, making the initial approach very unclear.

--------------------

##### Designing the Algorithm

Once again, many greedy algorithms potentially could be solutions to the problem. Here are three examples:

- This algorithm stars without any edges at all and builds a spanning tree by successively inserting edges from $E$ in order of increasing cost. As we move through the edges in this order, we insert each edge $e$ as long as it does not create a cycle when added to the edges already inserted. If it would create a cycle, $e$ is discarded and continue. - **Kruskal’s Algorithm**
- Another can be designed by analogy with Dijkstra’s algorithm for paths.
  - Start with a root node $s$ and try to greedily grow a tree from $s$ outward. At each step, we imply add the node that can be attached as cheaply as possible to the partial tree which we already have:
  - We maintain a set $S\subseteq V$ on which a spanning tree has been constructed so far. Initially $S=\{s\}$. In each iteration we grow $S$ by one node, adding the node $v$ that minimises the attachment cost $min_{e=(u,v):u\in S} c_e$ and including the edge $e=(u,v)$ that achieves this minimum spanning tree: - **Prim’s Algorithm**
- “Backward Version of Kruskal’s” we start with a full graph $(V,E)$ and begin deleting edges in order of decreasing cost. As we get to each edge $e$ - starting with the most expensive, we delete it, so long as this does not disconnect the graph. - **Reverse-Delete Algorithm**

-----------------

##### Analysing the Algorithms

- We assume that all edge costs are distinct from each other.

**When is it safe to include an edge in the minimum spanning tree?**

**Statement (4.17)** Assume that all edge costs are distinct, Let $S$ be any subset of nodes that is neither empty nor equal to all of $V$, and let edge $e=(v,w)$ be the minimum cost edge with one end in S and the other in $V-S$, then every minimum spanning tree contains the edge $e$

------------

**Proof** Let $T$ be the spanning tree that does not contain $e$; we need to show that $T$ does not have the minimum possible cost.

- The ends of $e$ are $v,w$. $T$ is a spanning tree, so there must be a path $P$ in $T$ from $v$ to $w$. Starting at $v$, suppose we follow the nodes of $P$ in a sequence. There is a first node $w’$ on $P$ that is in $V-S$.
- Let $v’\in S$ be be the node just before $w’$ on $P$, and let $e’=(v’,w’)$ be the edge joining them.
- Therefore, $e’$ is an edge of $T$ with one end in $S$ and the other in $V-S$
- If we exchange the edges $e$ for $e’$, we get a set of edges $T’=T-\{e'\}\cup \{e\}$, we claim that $T’$ is a spanning tree. Clearly $(V,T’)$ is connected, since $(V,T)$ is connected, and any path in $(V,T)$ that used edge  $e’=(v’,w’)$ can now be rerouted in $(V,T’)$ to follow the portionof $P$  from $v’$ to $v$, then the edge $e$, and then the portion of $P$ from $w$ to $w’$. To see that $(V,T’)$ is also acyclic, note that the only cycle in $(V, T’\cup \{e’\})$ is the one composed of $e$ and the path $P$, and this cycle is not present in $(V,T’)$ due to the deletion of $e’$.
- We noted above that the edge $e’$ has one end in $S$ and the other in $V-S$. But $e$ is the cheapest edge with this property, and so $c_e < c_\{e’\}$ (inequality is strict as no two edges have the same cost). Thus the total cost of $T’$ is less than that of $T$. $\blacksquare$

--------------

**Kruskal's Algorithm produces a minimum spanning tree of G**

**Proof:**

- Consider any edge $e=(v,w)$ added by KA, and let $S$ be the set of all nodes to which $v$ has a path at the moment just before $e$ is added. Clearly $v\in S$, but $w\notin S$ since adding $e$ does not create a cycle. Moreover, no edge from $S$ to $V- S$ has been encountered yet, since any such edge could have been added without creating a cycle, and hence would have been added by KAs algorithm. Therefore $e$ is the cheapest edge with one end in $S$ and the other in $V-S$, and by (4.17), it belongs to every minimum spanning tree.
- Clearly $(V,T)$ contains no cycles, since the algorithm is explicitly designed to avoid creating cycles. Further, if $(V,T)$ were not connected, then there would exist a nonempty subset of nodes $S$ (not all equal to $V$) such that there is no edge from $S$ to $V-S$, but this contradicts the algorithm’s behaviour: we know that since $G$ is connected, there is at least one edge between $S$ and $V-S$ and the algorithm will add the first of these that it encounters. $\blacksquare$ 

----------------

**Prims Algorithm produces a minimum spanning tree of G**

**Proof**

- In each iteration there is a set $ S\subseteq V$ on which a partial spanning tree has been constructed, and a nore $v$ and edge $e$ are added that minimize the quantity  $min_{e=(u,v):u\in S} c_e$. By definition, $e$ is the cheapest edge with one end i $S$ and the other end in $V-S$, and so by the cut property (4.17) it is in every minimum spanning tree.
- It is also straightforward to show that Prims Algorithm produces a spanning tree of G, and hence it produces a minimum spanning tree. $\blacksquare$

## Divide and Conquer Algorithms and Recurrences

**Divide-and-Conquer**

- divide up problem into several subproblems of the same kind
- Solve each subprovblem recursively
- Combine soluitions to subproblems into overall solution

**Most Common Use**

- Divide a problem of size $n$ into subproblems of size $n/2$ - $O(n)$ time
- Solve two subproblems recursively 
- Combine two solutions into overall solution - $O(n)$ time

**Consequence**

- Brute Force: $\Theta(n^2)$ 
- Divide-and-conquer: $O(n log n )$ 

### Merge Sort

- Given a list of $n$ elements from a totally ordered universe, rearrange them in ascending order.

#### Sorting applications

- Google page rank results
- Identify statistical outliers
- Binary search in a DB
- Remove duplicates in a mailing list
- Convex hull
- Closest pair of points
- Interval scheduling
- Minimizing lateness
- Minimum spanning trees.

#### Mergesort Process

- Recursively sort left half
- Recursively sort right half
- Merge two halves to make the sorted whole

#### Merging

- Scan $A$ and $B$ from left to right
- Compare $a_i, b_j$
- If $a_i \leq b_j$ append $a_i$ to $C$ - no larger than any remaining element in $B$ 
- If $a_i > b_j$ append $b_j$ to $C$ - smaller than every remaining element in $A$ 

![image-20211019091229995](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019091229995.png)

- Moving the pointer from left to right shows that there can be maximum of $n$ comparisons

#### Mergesort Implementation

**Input** List $L$ of $n$ elements from a totally ordered universe

**Output** The $n$ elements in ascending order

$MergeSort(L)$

**If**(list $L$ has one element) 

​	**Return** $L$ 

Divide the list into two halves $A,B$ 

$A\leftarrow MergeSort(A)$ - $T(n/2)$

$B\leftarrow MergeSort(B)$ -$T(n/2)$

$L \leftarrow Merge(A,B)$ - $\Theta(n)$ 

**Return** $L$ 

#### Mergesort Time Complexity

- $T(n) =$ max number of compares to mergesort of a list of length $n$

Recurrence

![image-20211021111302939](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211021111302939.png)

- The solution is $O(n$ $log$ $n)$

Proofs

- Describe several ways to solve the recurrence, and initially assume $n$ is a power of 2 and replace $\leq$ with $=$ in the reccurence.





##### Divide and conquer recurrence - recursion tree

- If $n =1, T(n) = 0$
- If $n>1, T(n) = 2T(n/2)+n$

The tree below can be grouped into levels, where in each level there are 2^level^ nodes, where level is 0 indexed

- Level 0 $=1\cdot n$ - the denominators cancel,

- Level 1$=2\cdot n/2$ 

- Level 2 = $2^2\cdot n/4$

  The number of levels are $log$ $n$, therefore, the total running time is $O(n$ $log$ $n)$

![image-20211021111648717](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211021111648717.png)



##### Proof of Theorem by Induction

If the following is satisfied, then $T(n) = n$ $log$ $n$

- If $n =1, T(n) = 0$
- If $n>1, T(n) = 2T(n/2)+n$







### Closest Pair Of Points Problem

Given $n$ points in the pane, find the pair of points with the smallest Euclidean distance between them

**Fundamental geometric primitive**

- Graphic, computer vision, geographic information systems, molecular modelling, air traffic control
- Special case of nearest neighbour, Euclidean MST, Voronoi

**Brute force** - check all pairs with the $\Theta(n)$ distance calculations

**1D version** - Easy $O(nlogn)$ algorithm if points are on a line

**Non degeneracy assumption** No two points have the same $x$ coordinate - technically true if you consider real real numbers

#### First attempt

- Sort by $x$ coordinate and consider nearby points
- Sort by $y$ coordinate and consider nearby points.

This does not work

![image-20211019093359153](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019093359153.png)

- In this case, the closest two points are far apart in comparison to the rest, therefore will not be established to be the closest point

#### Second Attempt

- Divide the into 4 parts - ideally with similar number of points, however this may often be impossible.

#### Third Attempt

**Divide** - draw vertical line $L$ so that $n/2$ points on each side

**Conquer** - find the closest pair in each side recursively

**Combine** - find the closest pair with one point in each side

**Return** - three best solutions

![image-20211019093912042](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019093912042.png)

##### How to find the closest pair with one point on each side.

Find the closest pair with one point in each side, assuming that the distance $< \delta$ 

- Observation - suffices to consider only those points within $\delta$ of the line $L$ 

- $\delta = min(12,21)$  - only consider points within that dividing line
  - Any points that are further than $\delta$, mean that they will not be the closest pair of points as there are a pair of points that are distance $zdelta$ 
- Sort points in 2 $\delta$ - strip by their $y$ coordinate
- Check distances of only those points within 7 positions in the sorted list
  - Note the pair of points that were closest to each other
  - Repeat this step with the other points, record the smallest of those distances and update the smallest distance found so far.
  - The worst case can be linear, however we are only going to consider 7 of the points each times, and is going of be linear in the number of all points.
  - 

![image-20211019094135751](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019094135751.png)

##### Proof of correctness

- Let $s_i$ be the points in the $2\delta$ strip, with the $i^{th}$ smallest y coordinate
- Claim - I f$|j-i|>7$ then the distance between the two points is at least $\delta$



- Consider a $2\delta$ by $\delta$ rectangle  $R$ in the strip whose min $y$ coordinate is $y$ coordinate of $s_j$
- Distance between $s_i$ and any point $s_j$ above $R$ is $\geq \delta$ 
- Subdivide $R$ into 8 squares
- At most 1 per points per square - the largest distance between two points in one of the squares is the diagonal is $\delta / \sqrt{2} <\delta$  
  - Each of those squares is either to the left or the right of the dividing line $L$ cannot be as far as delta apart, 
- At most 7 other points can be in $R$ - this can be refined with a geometric packing algorithm
  - Any point whose index is greater than 8, the height will be at least $\delta$, therefore there is no way that it is smaller than delta

![image-20211019111616293](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019111616293.png)







##### Algorithm

$ClostestPair(p_1,p_2,…,p_n)$

compute vertical line $L$ such that half the points are on each side of the line - $O(n)$

$\delta_{1} \leftarrow$ $ClosestPair($points in left half$)$ - $T(n/2)$ 

$\delta_{2} \leftarrow$ $ClosestPair($points in right half$)$ - $T(n/2)$

$\delta \leftarrow min\{\delta_{1}, \delta_2\}$ 

Delete all points further than $\delta$ from line $L$ $O(n)$

Sort remaining by y - coordinate - $O(n \cdot log (n))$

Scan points in $y$-order and compare distances between each point and the next 7 neighbours if any one of these distances is less than $\delta$,

 then update $\delta$ - $O(n)$

**return** $\delta$





### The Master Theorem

#### Divide and Conquer Recurrences

- Recipe for solving common DC recurrences

$T(n) = aT\big(\frac{n}{b}\big) + f(n)$

With $T(0) = 0 $ and $T(1) = \Theta (1)$ 



**Terms**

- $a\geq 1$ is the number of subproblems
- $b\geq 2$ is the factor by which the subproblem size decreases

- $f(n) \geq 0 $ is the work to divide and combine subproblems

##### **Recursion Tree**

- $a=$ branching factor
- $a^i$ = number of subproblems at level $i$ 
- $1+log_b n$ levels
- $n/b^i=$ size of subproblems at level $i$



![image-20211019115906293](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019115906293.png)

Suppose that $T(n)$ satisifies $T(n) = aT(n/b) + n^c$ with $T(1) = 1)$ for $n$ a power of $b$ 



![image-20211019120609645](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019120609645.png)



- $a^{log^bn} = (b^{log_b a})^{log_b n}= b^{log_ba\cdot log_b n }= (b^{lob_b n})^{log_ba} = n^{log_ba}$



Let $r=a/b^c$. $r< 1$ iff $c>log_ba$ 



![image-20211019133659949](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019133659949.png)

- Geometric series
  - If $0<r<1$, then $1+r+r^2+...+r^k \leq 1/(1-r)$
  - If $r=1$          then $1+r+r^2+...+r^k = k+1$
  - If $r>1$,         then  $1+r+r^2+...+r^k = (r^{k+1}-1)/(r-1)$



- $\frac{r^{k+1} -1}{r-1} = O(r^k)$ as $r$ is a constant

Therefore

$T(n)= n^c\cdot \sum\limits_{i=0}^{log_bn}= n^c \cdot O((\frac{a}{b})^{log^bn}) = n^c \cdot O(\frac{n^{log_ba}}{n^c})=O(n^{log_ba})$



#### The Master Theorem

- Recipe for solving common DC recurrences

$T(n) = aT\big(\frac{n}{b}\big) + \Theta(n^c)$

With $T(0) = 0 $ and $T(1) = \Theta (1)$, where $n/b$ means either $\lfloor{n/b}\rfloor$ or $\lceil n/b \rceil$ 

3 Cases:

1. If $c<log^ba$ then $T(n) = \Theta(n^{log_ba})$ - the solution is the bigger of the two numbers
2. If $c=log^ba$ then $T(n) = \Theta(n^c\cdot log$ $n)$ 
3. If $c>log^ba$ then $T(n) = \Theta(n^c)$ - the solution is the bigger of the two numbers



**Extensions**

- Can replace $\Theta$ with $O$ everywhere
- Can replace  $\Theta$ with $\Omega$ everywhere
- Can replace initial conditions with $T(n) = \Theta(1) \forall n\leq n_0$ and require recurrence to hold only $\forall n \in \Z :n > n_0$ 



##### Example 1

- $T(n) = 3T(\lfloor n/2\rfloor) +5n$
- $a=3,b=2,c=1,log_ba<1.58$
- $T(n) = \Theta (n^{log_23})=O(n^{1.58})$

##### Example 2

- $T(n) = T(\lfloor n/2\rfloor) + T(\lceil n/2 \rceil) +17n$
- $a=2,b=2,c=1,log_ba=1$
- $T(n) = \Theta(n$ $log$ $n)$

##### Example 3

- $T(n) = 48T(\lfloor n/4\rfloor) +n^3$
- $a=48,b=4,c=3, log_ba>2.79$
- $T(n) = \Theta(n^3)$

##### Gaps in the Master Theorem

- Number of subproblems is not a constant
  - $T(n) = n\cdot T(n/2) + n^2$
- Number of subproblems is less than $1$
  - $T(n) = \frac{1}{2}\cdot T(n/2) + n^2$

- Work ro divide the subproblems is not $\Theta(n^c)$ 
  - $T(n) = {2}\cdot T(n/2) + n$ $log$ $n$

###  Integer Addition

Addition - given two $n$ - bit integers $a,b$ compute $a+b$

Subtraction - given two $n$ - bit integers $a,b$ compute $a-b$

- Standard: $\Theta(n)$ bit operations

![image-20211019151441086](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019151441086.png)

### Integer Multiplication

- Given two $n$-bit numbers $a,b$ compute $a\cdot b$ 
- Typical algorithm - long multiplication:
  - $\Theta(n^2)$ 

![image-20211019151727324](C:\Users\leonc\AppData\Roaming\Typora\typora-user-images\image-20211019151727324.png)

#### Karatsuba’s divide-and-conquer Algorithm

- Divide $x,y$ into low and high order orbits
- Multiply four $1/2 n$ bit integers recursively
- Add and shift to obtain result:
  - $m=\lceil n/2 \rceil$ 
  - $a=\lfloor x/2^m \rfloor$ $b=x$ $mod$ $ 2^m$ 
  - $c=\lfloor y/2^m \rfloor$ $d=y$ $mod$ $ 2^m$ 

- $xy = (2^ma+b)(2^mc+d)=2^{2m}ac+2^m(bc+ad)+bd$

- Multiplication by two can be done via a bit shift



$Multiply(x,y,n)$

**If** $(n=1)$ 

​	**Return** $x\cross y$ 

**Else**

​	$m\leftarrow \lceil n/2 \rceil$                                   $\Longleftarrow \Theta(n)$

​	$a \leftarrow\lfloor x/2^m \rfloor;$ $b=x$ $mod$ $ 2^m$     $\Longleftarrow \Theta(n)$

​	$c\leftarrow\lfloor y/2^m \rfloor;$ $d=y$ $mod$ $ 2^m$     $\Longleftarrow \Theta(n)$

​	$e\leftarrow Multiply(a,c,m).$             $\Longleftarrow T(\lceil n/2\rceil)$

​	$f\leftarrow Multiply(b,d,m).$             $\Longleftarrow T(\lceil n/2\rceil)$

​	$g\leftarrow Multiply(b,c,m).$              $\Longleftarrow T(\lceil n/2\rceil)$

​	$h\leftarrow Multiply(a,d,m).             $             $\Longleftarrow T(\lceil n/2\rceil)$

​	**Return** $2^{2m}e + 2^m(g+h)+f$  $\Longleftarrow \Theta(n)$



##### Karatsuba Trick

- divide x and y into low and hih order bits
- Compute middle term $bc + ad$ use identity:
  - $bc+ad=ac+bd=(a-b)(c-d)$
- Multiply only three $1/2\cdot n$ bit integers recursively
  - $m=\lceil n/2 \rceil$ 
  - $a=\lfloor x/2^m \rfloor$ $b=x$ $mod$ $ 2^m$ 
  - $c=\lfloor y/2^m \rfloor$ $d=y$ $mod$ $ 2^m$ 

- $xy = (2^ma+b)(2^mc+d)=2^{2m}ac+2^m(bc+ad)+bd= 2^{2m}ac + 2^m(ac+bd-(a-b)(c-d))+bd$
  - This means that only three components need be calculated - the middle term can be calculated using the other parts



##### Algorithm



$KaratsubaMultiply(x,y,n)$

**If** $(n=1)$ 

​	**Return** $x\cross y$ 

**Else**

​	$m\leftarrow \lceil n/2 \rceil$                                   $\Longleftarrow \Theta(n)$

​	$a \leftarrow\lfloor x/2^m \rfloor;$ $b=x$ $mod$ $ 2^m$     $\Longleftarrow \Theta(n)$

​	$c\leftarrow\lfloor y/2^m \rfloor;$ $d=y$ $mod$ $ 2^m$      $\Longleftarrow \Theta(n)$

​	$e\leftarrow KaratsubaMultiply(a,c,m).$                                 $\Longleftarrow T(\lceil n/2\rceil)$

​	$f\leftarrow KaratsubaMultiply(b,d,m).$                                  $\Longleftarrow T(\lceil n/2\rceil)$

​	$g\leftarrow KaratsubaMultiply(|a-b|,|c-d|,m).$              $\Longleftarrow T(\lceil n/2\rceil)$

​	**Flip sign of $g$ if needed**

​	**Return** $2^{2m}e + 2^m(e+f-g)+f$

##### 

- Applying case 1 of the master theoram:

  $\Longrightarrow T(n)=\Theta(n^{log_23})=P(n^{1.585})$



#### Integer Arithmetic Reductions

| Arithmetic Problem     | Formula                          | Bit Complexity |
| ---------------------- | -------------------------------- | -------------- |
| Integer Multiplication | $a\cross b$                      | $M(n)$         |
| Integer Square         | $a^2$                            | $\Theta(M(n))$ |
| Integer Division       | $\lfloor a/b\rfloor,a$ $mod$ $b$ | $\Theta(M(n))$ |
| Integer Square Root    | $\lfloor \sqrt a \rfloor$        | $\Theta(M(n))$ |

















